[
["index.html", "Notas Curso de Estadística (Parte I) Capítulo 1 Introducción", " Notas Curso de Estadística (Parte I) Maikol Solís Actualizado el 23 November, 2020 Capítulo 1 Introducción "],
["inferencia-estadística.html", "Capítulo 2 Inferencia estadística 2.1 Ejemplo 2.2 Modelo estadístico 2.3 Estadístico", " Capítulo 2 Inferencia estadística Definición: Hacer afirmaciones probabilísticas respecto a (acerca de) cantidades desconocidas. 2.1 Ejemplo *Pregunta: ¿Será posible modelar cuánto dura un componente electrónico en fallar? Solución: Podemos responder esta pregunta dividiéndola en dos partes: Modelo probabilístico: Asuma que los tiempos de vida del componente son exponenciales (en años). Parámetro: Sea \\(\\theta &gt; 0\\) la tasa de fallo (unidades: 1/Tiempo(años)). Es decir, tenemos un modelo (exponencial) y estamos decretando que su información estará concentrada en el parámetro \\(\\theta\\). Nota: El parámetro \\(\\theta\\) contiene la información del modelo, pero ¿Cómo obtenemos esa información Muestra: Secuencia (sucesión) de variables aleatorias independientes \\(X_1,X_2,\\dots, X_n,\\dots\\). Tomemos una muestra \\(X_1,X_2,\\dots, X_n,\\dots \\stackrel{i.i.d}{\\sim} \\text{Exp}(\\theta)\\). Objetivos Estimar \\(X_m, X_{m+1}, \\dots\\) si se observa \\(X_1, X_{m-1}, \\dots\\) (Predicción). Estimar \\(\\theta\\) usando información. Datos: Realizaciones de variables aleatorias \\(X_1,\\dots,X_m\\) pertenecientes a la muestra. Estimación de \\(\\theta\\) Dado que \\(\\mathbb{E}(X) = \\dfrac{1}{\\theta}\\) con \\(X \\sim \\text{Exp}(\\theta)\\), por la ley de grandes números se tiene que \\[\\begin{equation*} \\underbrace{\\dfrac{1}{n} \\sum_{i=1}{X_i}}_{\\bar{X}_n} \\xrightarrow[n\\to \\infty]{\\mathbb{P}}\\mathbb{E}(X) = \\dfrac{1}{\\theta} \\end{equation*}\\] por propiedad de convergencia en probabilidad. Un posible candidato para estimar \\(\\theta\\) es \\(\\dfrac{1}{\\bar X_n}\\), bajo el supuesto por Ley de Grandes Números que \\(\\theta\\) es una constante (frecuentista). Realidad: \\(\\theta\\) no necesariamente es determinístico (factores externos, por la naturaleza del fenómeno). Asumimos un modelo probabilístico para \\(\\theta\\) (tasa siempre positiva): \\[\\begin{equation*} \\theta \\sim \\Gamma(\\alpha_0,\\beta_0) \\end{equation*}\\] Luego, según estudios previos la tasa esperada es 0.5/año \\[\\begin{equation*} \\mathbb{E}(\\theta) = \\dfrac{1}{2} = \\dfrac{\\alpha_0}{\\beta_0}. \\end{equation*}\\] Un primer indicio de que se podría establecer que \\(\\alpha_0 = 1\\) y de \\(\\beta_0 = 2\\). 2.2 Modelo estadístico Vamos a definir como típicamente se define un modelo estadístico. Variables aleatorias observables / hipotéticamente observables: \\[\\begin{equation*} \\underbrace{X_t}_{\\text{Observable}} = \\underbrace{Y_t}_{\\text{Hip. observable}} + \\underbrace{\\epsilon}_{\\text{Ruido}} \\end{equation*}\\] En otras palabras \\(Y_t\\) sería la el dato “verdadero” que pasó exactamente en el fenómeno analizado. Esta observación es afectada por muchos factores no observables (por ejemplo: errores de medición, cambio de las condiciones de la economía, etc.). La variable \\(\\epsilon\\) captura toda esa aleatoriedad que no es parte del fénomeno. Claramente ni \\(Y_t\\) ni \\(\\epsilon\\) se pueden medir y la mejor representación del nuestro es fenómeno es a partir de \\(X_t\\). Distribución conjunta de una muestra de variables observables. Es decir cuál es el supuesto general que estoy usando para describir mis observaciones. Parámetros que son hipotéticamente observables (desconocidos). ¿Cuál sería la mejor calibración de los componentes del modelo anterior de modo que mi modelo se ajuste a los datos? (Opcional) Distribución conjunta de los parámetros. En el caso de Bayes, los parámetro dejan de ser simple valores puntuales y se convierten en distribuciones completas. Inferencia estadística: procedimiento que genera afirmaciones probabilísticas de un modelo estadístico. Ejemplo de inferencias: Estimar \\(\\theta\\) a través de \\(\\dfrac{1}{\\bar X_n}\\). ¿Qué tan probable es que el promedio de las siguientes observaciones es al menos 2? \\[\\begin{equation*} \\dfrac{1}{10}\\sum_{i= m+1}^{m+10} X_i &gt; 2 \\end{equation*}\\] ¿Qué tan cierto es que \\(\\theta\\leq0.4\\) después de observar la muestra? Parámetro: característica (s) que determinan la distribución conjunta de las variables aleatorias de interés. Espacio paramétrico \\(\\Omega\\) (espacio de parámetros, puede ser de probabilidad) Ejemplos: \\(\\theta\\) &gt; 0 (ejemplo anterior); \\(\\Omega = (0,+\\infty)\\). \\(X_1,\\dots,X_n \\sim N(\\mu, \\sigma^2)\\), \\((\\mu,\\sigma^2)\\) parámetros; \\(\\Omega\\) = \\(\\mathbb{R}\\times[0,+\\infty)\\). Ejemplo: Clientes de un banco ¿Qué tan probable es que un cliente no pague su crédito hoy? Datos: \\(X_i = \\begin{cases}1&amp; \\text{el cliente } \\#i \\text{ no pagó}\\\\0 &amp; \\text{el cliente } \\#i \\text{ pagó}\\end{cases}\\). Muestra: \\(X_1,\\dots,X_{10000}\\) (realización al día de hoy). Modelos: \\(X_1,\\dots, X_{10000} \\stackrel{i.i.d}{\\sim} \\text{Ber}(p)\\) con \\(p\\in[0,1]\\). Parámetro: \\(p\\), \\(\\Omega = [0,1]\\). Inferencias: Estimar \\(p\\) (probabilidad de impago). Suponga que \\(L(X_i)\\) es el saldo en la cuenta del cliente \\(\\#i\\). \\[\\begin{equation*} \\mathbb{P}\\left(\\sum_{i=1}^{10000}L(X_i)&gt;u\\right)=\\text{Probabilidad de ruina} \\end{equation*}\\] 2.3 Estadístico Definición. Si \\(X_1,\\dots,X_n\\) es una muestra observable. Sea \\(r\\) una función real de \\(n\\) variables: \\[\\begin{equation*} T = r(X_1,\\dots,X_n) \\end{equation*}\\] es un estadístico. Nota: \\(T\\) también es aleatorio. Ejemplos: \\(\\hat p = \\dfrac{1}{10000}\\displaystyle\\sum_{i=1}^{10000}X_i = \\dfrac{\\#\\text{ no pagan}}{\\text{Total}} = r(X_1,\\dots,X_{10000})\\) \\(L_m = \\max L(X_i)\\) (saldo del cliente más riesgoso). \\(R_m = \\max L(X_i) - \\min L(X_i), 1\\leq i\\leq 10000\\) "],
["densidades-previas-conjugadas-y-estimadores-de-bayes.html", "Capítulo 3 Densidades previas conjugadas y estimadores de Bayes 3.1 Distribución previa (distribución a priori) 3.2 Densidad posterior 3.3 Proceso de modelación de parámetros. 3.4 Función de verosimilitud 3.5 Familias conjugadas 3.6 Densidades previas impropias 3.7 Funciones de pérdida 3.8 Efecto de muestras grandes 3.9 Consistencia 3.10 Laboratorio", " Capítulo 3 Densidades previas conjugadas y estimadores de Bayes 3.1 Distribución previa (distribución a priori) Suponga que tenemos un modelo estadístico con parámetro \\(\\theta\\). Su \\(\\theta\\) es aleatorio entonces su densidad (antes de observar cualquier muestra) se llama densidad previa: \\(\\pi\\). Ejemplo: \\(X_1,\\dots, X_n \\sim \\text{Exp}(\\theta)\\) y \\(\\theta\\) es aleatorio tal que \\(\\theta \\sim \\Gamma(\\stackrel{\\alpha}{1},\\stackrel{\\beta}{2})\\) entonces \\[ \\pi(\\theta) = \\dfrac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\theta^{\\alpha-1}e^{\\beta\\theta} = 2e^{-2\\theta}, \\quad \\theta &gt; 0\\] Ejemplo: Sea \\(\\theta\\) la probabilidad de obtener cara al tirar una moneda. En este caso antes de modelar exactamente el \\(\\theta\\), lo importante es modelar el tipo de moneda. Es decir, supongamos que tenemos dos opciones Moneda justa: \\(\\theta = \\dfrac{1}{2}\\) con probabilidad previa \\(0.8\\) (\\(\\pi(\\frac{1}{2}) = 0.8\\)). Moneda con solo una cara: \\(\\theta = 1\\) con probabilidad previa \\(0.2\\) (\\(\\pi(1) = 0.2\\)). En este ejemplo si tuviéramos 100 monedas con probabilidad previa \\(\\pi\\) entonces 20 tendrían solo una cara y 80 serían monedas normales. Notas: \\(\\pi\\) está definida en \\(\\Omega\\) (espacio paramétrico). \\(\\pi\\) es definida antes de obtener la muestra. Ejemplo (Componentes eléctricos) Supoga que se quiere conocer el tiempo de vida de cierto componente eléctrico. Sabemos que este tiempo se puede modelar con una distribución exponencial con parámetro \\(\\theta\\) desconocido. Este parámetro asumimos que tiene una distribución previa Gamma. Un experto en componentes eléctricos conoce mucho de su área y sabe que el parámetro \\(\\theta\\) tiene las siguientes características: \\[\\begin{equation*} \\mathbb{E}[\\theta] = 0.0002, \\quad \\sqrt{\\text{Var}(\\theta)} = 0.0001. \\end{equation*}\\] Como sabemos que la previa \\(\\pi\\) es Gamma, podemos deducir lo siguiente: \\[\\begin{equation*} \\mathbb{E}[\\theta] = \\dfrac{\\alpha}{\\beta}, \\text{Var}(\\theta) = \\dfrac{\\alpha}{\\beta^2} \\end{equation*}\\] \\[\\implies \\begin{cases}\\dfrac{\\alpha}{\\beta} = 2\\times 10^{-4}\\\\\\sqrt{\\dfrac{\\alpha}{\\beta^2}} = 1 \\times 10^{-4}\\end{cases} \\implies \\beta = 20000, \\alpha = 4\\] Notación: \\(X = (X_1,\\dots, X_n)\\): vector que contiene la muestra aleatoria. Densidad conjunta de \\(X\\): \\(f_\\theta(x)\\). Densidad de \\(X\\) condicional en \\(\\theta\\): \\(f_n(x|\\theta)\\). Supuesto: \\(X\\) viene de una muestra aleatoria si y solo si \\(X\\) es condicionalmente independiente dado \\(\\theta\\). Consecuencia: \\[f_n(X|\\theta) = f(X_1|\\theta)\\cdot f(X_2|\\theta)\\cdots f(X_n|\\theta)\\] Ejemplo Si \\(X = (X_1,\\dots, X_n)\\) es una muestra tal que \\(X_i\\sim \\text{Exp}(\\theta)\\), \\[\\begin{align*} f_n(X|\\theta) &amp;= \\begin{cases}\\prod_{i=1}^n \\theta e^{-\\theta X_i} &amp; \\text{si } X_i&gt;0\\\\ 0 &amp; \\text{si no} \\end{cases} \\\\ &amp;= \\begin{cases}\\theta^n e^{-\\theta\\sum_{i=1}^n X_i} &amp; X_i &gt; 0 \\\\ 0 &amp; \\text{si no}\\end{cases} \\end{align*}\\] 3.2 Densidad posterior Definición. Considere un modelo estadístico con parámetro \\(\\theta\\) y muestra aleatoria \\(X_1,\\dots, X_n\\). La densidad condicional de \\(\\theta\\) dado \\(X_1,\\dots,X_n\\) se llama densidad posterior: \\(\\pi(\\theta|X)\\) Teorema. Bajo las condiciones anteriores: \\[\\begin{equation*} \\pi(\\theta|X) = \\dfrac{f(X_1|\\theta)\\cdots f(X_n|\\theta)\\pi(\\theta)}{g_n(X)} \\end{equation*}\\] para \\(\\theta \\in \\Omega\\), donde \\(g_n\\) es una constante de normalización. Prueba: \\[\\begin{align*} \\pi(\\theta|X) &amp; = \\dfrac{\\pi(\\theta,X)}{\\text{marginal de X}} = \\dfrac{\\pi(\\theta,X)}{\\int \\pi(\\theta,X)\\;d\\theta}= \\dfrac{P(X|\\theta)\\cdot \\pi(\\theta)}{\\int \\pi(\\theta,X)\\;d\\theta}\\\\ &amp; \\dfrac{f_n(X|\\theta)\\cdot \\pi(\\theta)}{g_n(X)} = \\dfrac{f(X_1|\\theta)\\cdots f(X_n|\\theta)\\pi(\\theta)}{g_n(X)} \\end{align*}\\] Del ejemplo anterior, \\[f_n(X|\\theta) = \\theta^n e^{-\\theta y}, y = \\sum{X_i} \\text{ (estadístico})\\] Numerador: \\[f_n(X|\\theta)\\pi(\\theta) = \\underbrace{\\theta^n e^{-\\theta y}}_{f_n(X|\\theta)} \\cdot \\underbrace{\\dfrac{200000^4}{3!}\\theta^3e^{-20000\\cdot\\theta}}_{\\pi(\\theta)} = \\dfrac{20000^4}{3!}\\theta^{n+3}e^{(20000+y)\\theta}\\] Denominador: \\[g_n(x) = \\int_{0}^{+\\infty}\\theta^{n+3}e^{-(20000+y)\\theta}\\;d\\theta = \\dfrac{\\Gamma(n+4)}{(20000+y)^{n+4}}\\] Entonces la posterior corresponde a \\[\\pi(\\theta|X) = \\dfrac{\\theta^{n+3}e^{-(20000+y)\\theta}}{\\Gamma(n+4)} (20000+y)^{n+4}\\] que es una \\(\\Gamma(n+4,20000+y)\\). Con 5 observaciones (horas): 2911, 3403, 3237, 3509, 3118. \\[y = \\sum_{i=1}^{5}X_i = 16478, \\quad n= 5\\] por lo que \\(\\theta|X \\sim \\Gamma(9,36178)\\) Es sensible al tamaño de la muestra (una muestra grande implica un efecto de la previa menor). Hiperparámetros: parámetros de la previa o posterior. 3.3 Proceso de modelación de parámetros. De ahora en adelante vamos a entender un modelo como el conjunto de los datos \\(X_1, \\ldots, X_n\\), la función de densidad \\(f\\) y el parámetro de la densidad \\(\\theta\\). Estos dos últimos resumen el comportamiento de los datos. Ahora para identificar este modelo se hace por partes, La información previa \\(\\pi(\\theta)\\) es la información extra o basado en la experiencia que tengo del mdoelo. Los datos es la información observada. La función de densidad \\(f\\) filtra y mejora la información de la previa. La densidad posterior es la “mezcla” entre la información y los datos observados. Es una versión más informada de la distribución del parámetro. 3.4 Función de verosimilitud Bajo el modelo estadístico anterior a \\(f_n(X|\\theta)\\) se le llama verosimilitud o función de verosimilitud. Observación. En el caso de una función de verosimilitud, el argumento es \\(\\theta\\). Ejemplo. Sea \\(\\theta\\) la proporción de aparatos defectuosos, con \\(\\theta \\in [0,1]\\) \\[ X_i = \\begin{cases} 0 &amp; \\text{falló} \\\\ 1 &amp; \\text{no falló} \\end{cases}\\] \\(\\{X_i\\}_{i=1}^n\\) es una muestra aleatoria y \\(X_i \\sim Ber(\\theta)\\). Verosimilitud \\[ f_n(X|\\theta) = \\prod_{i=1}^n f(X_i|\\theta) = \\begin{cases}\\theta^{\\sum X_i}(1-\\theta)^{n-\\sum X_i} &amp; X_i = 0,1\\; \\forall i\\\\ 0 &amp; \\text{si no}\\end{cases}\\] Previa: \\[\\pi(\\theta) = 1_{\\{0\\leq\\theta\\leq 1\\}}\\] Posterior: Por el teorema de Bayes, \\[\\begin{align*} \\pi(\\theta|X) \\propto \\theta^y (1-\\theta)^{n-y}\\cdot 1 \\\\ &amp;= \\theta^{\\overbrace{y+1}^{\\alpha}-1}(1-\\theta)^{\\overbrace{n-y+1}^{\\beta}-1} &amp;\\implies \\theta|X \\sim \\text{Beta}(y+1,n-y+1) \\end{align*}\\] Predicción. Supuesto: los datos son secuenciales. Calculamos la distribución posterior secuencialmente: \\[\\begin{align*} \\pi(\\theta|X_1) &amp; \\propto \\pi(\\theta) f(X_1|\\theta)\\\\ \\pi(\\theta|X_1,X_2) &amp;\\propto \\pi(\\theta) f(X_1,X_2|\\theta) \\\\ &amp;= \\pi(\\theta) f(X_1|\\theta) f(X_2|\\theta) \\text{ (por independencia condicional)} \\\\ &amp; = \\pi(\\theta|X_1)f(X_2|\\theta)\\\\ \\vdots &amp; \\\\ \\pi(\\theta|X_1,\\dots,X_n) &amp; \\propto f(X_n|\\theta)\\pi(\\theta|X_1,\\dots, X_{n-1}) \\end{align*}\\] Bajo independencia condicional no hay diferencia en la posterior si los datos son secuenciales. Luego, \\[\\begin{align*} g_n(X) &amp; = \\int_{\\Omega} f(X_n|\\theta) \\pi(\\theta|X_1,\\dots, X_{n-1})\\;d\\theta\\\\ &amp; = P(X_n|X_1,\\dots,X_{n-1}) \\text{ (Predicción para }X_n) \\end{align*}\\] Continuando con el ejemplo de los artefactos, \\(P(X_6&gt;3000|X_1,X_2,X_3,X_4,X_5)\\). Se necesita calcular \\(f(X_6|X)\\). Dado que \\[ \\pi(\\theta|X) = 2.6\\times 10^{36}\\theta^8 e^{-36178\\theta}\\] se tiene \\[ f(X_6|X) = 2.6\\times 10^{36} \\int_{0}^1 \\underbrace{\\theta e^{-\\theta X_6}}_{\\text{Densidad de } X_6}\\theta^8 e^{-36178\\theta}\\;d\\theta = \\dfrac{9.55 \\times 10^{41}}{(X_6+36178)^{10}}\\] Entonces, \\[ P(X_6&gt;3000) = \\int_{3000}^{\\infty} \\dfrac{9.55\\times10^{41}}{(X_6+36178)^{10}}\\; dX_6 = 0.4882\\] La vida media se calcula como \\(\\dfrac{1}{2} = P(X_6&gt;u|X)\\). 3.5 Familias conjugadas Definición. Sea \\(X_1,\\dots, X_n\\) i.i.d. condicional dado \\(\\theta\\) con densidad \\(f(X|\\theta)\\). Sea \\(\\psi\\) la familia de posibles densidades previas sobre \\(\\Omega\\). Si, sin importar los datos, la posterior pertenece a \\(\\psi\\), entonces decimos que \\(\\psi\\) es una familia conjugada de previas. Ejemplos: La familia Beta es familia conjugada para muestras según una Bernoulli. La familia Gama es familia conjugada para muestras exponenciales. Para el caso Poisson, si \\(X_1,\\dots,X_n\\sim Poi(\\lambda)\\),entonces la familia Gamma es familia conjugada. La función de densidad de una Poisson es \\(P(X_i = k) = e^{-\\lambda}\\dfrac{\\lambda^k}{k!}\\). La verosimilitud corresponde a \\[ f_n(X|\\lambda) = \\prod_{i=1}^{n}e^{-\\lambda}\\dfrac{\\lambda^X_i}{X_i!} = \\dfrac{e^{-n\\lambda^y}}{\\prod_{i=1}^n X_i}.\\] La previa de \\(\\lambda\\) está definida por \\(\\pi(\\lambda)\\propto\\lambda^{\\alpha-1}e^{-\\beta\\lambda}\\). Por lo tanto, la posterior es \\[ \\pi(\\lambda|X) \\propto \\lambda^{y+\\alpha-1}e^{-(\\beta+n)\\lambda} \\implies \\lambda|X \\sim \\Gamma(y+\\alpha,\\beta+n)\\] En el caso normal, si \\(X_1,\\dots,X_n\\sim N(\\theta,\\sigma^2)\\),entonces la familia normal es conjugada si \\(\\sigma^2\\) es conocido. Si \\(\\theta \\sim N(\\mu_0,V_0^2) \\implies \\theta|X \\sim N(\\mu_1, V_1^2)\\) donde, \\[\\mu_1 = \\dfrac{\\sigma^2\\mu_0 + nV_0^2 \\bar X_n}{\\sigma^2 + nV_0^2} = \\dfrac{\\sigma^2}{\\sigma^2 + nV_0^2}\\mu_0 + \\dfrac{nV_0^2}{\\sigma^2 + nV_0^2}\\bar X_n\\] Combina de manera ponderada la previa y la de los datos. Ejemplo Considere una verosimilitud Poisson(\\(\\lambda\\)) y una previa \\[ \\pi(\\lambda) = \\begin{cases}2e^{-2\\lambda} &amp; \\lambda&gt; 0 \\\\ 0 &amp; \\lambda \\geq 0\\end{cases} \\quad \\lambda \\sim \\Gamma(1,2)\\] Supongamos que es una muestra aleatoria de tamaño \\(n\\). ¿Cuál es el número de observciones para reducir la varianza, a lo sumo, a 0.01? Por teorema de Bayes, la posterior \\(\\lambda|x \\sim \\Gamma(y+1,n+2)\\). Luego, la varianza de la Gamma es \\[\\dfrac{\\alpha}{\\beta^2} = \\dfrac{\\sum x_i + 1}{(n+2)^2} \\leq 0.01 \\implies \\dfrac{1}{(n+2)^2} \\leq \\dfrac{\\sum x_i + 1}{(n+2)^2} \\leq 0.01 \\implies 100 \\leq (n+2)^2 \\implies n\\geq 8\\] Teorema. Si \\(X_1,\\dots,X_n \\sim N(\\theta, \\sigma^2)\\) con \\(\\sigma^2\\) conocido y la previa es \\(\\theta \\sim N(\\mu_0,V_0^2)\\), entonces \\(\\theta|X\\sim N(\\mu_1,V_1^2)\\) donde \\[ \\mu_1 = \\dfrac{\\sigma^2\\mu_0 + nV_0^2 \\bar X_n}{\\sigma^2 + nV_0^2}, \\quad V_1^2 = \\dfrac{\\sigma^2V_0^2}{\\sigma^2 + nV_0^2}\\] Prueba: Verosimilitud: \\[ f_n(X|\\theta) \\propto \\exp\\left[- \\dfrac{1}{2\\sigma^2} \\sum_{i=1}^{n}(X_i\\theta)^2\\right]\\] Luego, \\[\\begin{align*} \\sum_{i=1}^n (X_i-\\theta)^2 &amp; = \\sum_{i=1}^n (X_i-\\bar X + \\bar X - \\theta)^2 \\\\ &amp; = n(\\bar X + \\theta)^2 + \\sum_{i=1}^n (X_i-\\bar X)^2 + \\underbrace{2 \\sum_{i=1}^n (X_i-\\bar X)(\\bar X - \\theta)}_{= 0 \\text{ pues } \\sum Xi = n\\bar X)} \\end{align*}\\] Entonces \\[ f_n(X|\\theta) \\propto \\exp\\left[-\\dfrac{n}{2\\sigma ^2}(\\bar X - \\theta )^2\\right].\\] Previa: \\[ \\pi(\\theta) \\propto \\exp\\left[-\\dfrac{1}{2V_0^2}(\\theta - \\mu_0)^2\\right].\\] Posterior: \\[ \\pi(\\theta|X) \\propto \\exp\\left[-\\dfrac{n}{2\\sigma ^2}(\\bar X - \\theta )^2-\\dfrac{1}{2V_0^2}(\\theta - \\mu_0)^2\\right].\\] Con \\(\\mu_1\\) y \\(V_1^2\\) definidos anteriormente, se puede comprobar la siguiente identidad: \\[-\\dfrac{n}{\\sigma ^2}(\\bar X - \\theta )^2-\\dfrac{1}{V_0^2}(\\theta - \\mu_0)^2= \\dfrac{1}{V_1^2}(\\theta-\\mu_1)^2 + \\underbrace{\\dfrac{n}{\\sigma^2 + nV_0^2}(\\bar X_n- \\mu_0)^2}_{\\text{Constante con respecto a }\\theta}\\] Por lo tanto, \\[\\pi(\\theta|X) \\propto \\exp\\left[-\\dfrac{n}{2V_1^2}(\\theta -\\mu_1)^2\\right]\\] Media posterior: \\[\\mu_1 = \\underbrace{\\dfrac{\\sigma^2}{\\sigma^2 + nV_0^2}}_{W_1}\\mu_0 + \\underbrace{\\dfrac{nV_0^2}{\\sigma^2 + nV_0^2}}_{W_2} \\bar X_n \\] Afirmaciones: Si \\(V_0^2\\) y \\(\\sigma^2\\) son fijos, entonces \\(W_1 \\xrightarrow[n\\to \\infty]{}0\\) (la importancia de la media empírica crece conforme aumenta \\(n\\)). Si \\(V_0^2\\) y \\(n\\) son fijos, entonces \\(W_2 \\xrightarrow[\\sigma^2\\to \\infty]{}0\\) (la importancia de la media empírica decrece conforme la muestra es menos precisa). Si \\(\\sigma^2\\) y \\(n\\) son fijos, entonces \\(W_2 \\xrightarrow[V_0^2\\to \\infty]{}1\\) (la importancia de la media empírica crece conforma la previa es menos precisa). Ejemplo (determinación de n) Sean \\(X_1,\\dots, X_n \\sim N(\\theta,1)\\) y \\(\\theta\\sim N(\\mu_0,4)\\). Sabemos que \\[V_1^2 = \\dfrac{\\sigma^2V_0^2}{\\sigma^2 + nV_0^2}. \\] Buscamos que \\(V_1\\leq 0.01\\), entonces \\[ \\dfrac{4}{4n+1}\\leq 0.01 \\implies n\\geq 99.75 \\text{ (al menos 100 observaciones)}\\] 3.6 Densidades previas impropias Definición. Sea \\(\\pi\\) una función positiva cuyo dominio está en \\(\\Omega\\). Suponga que \\(\\int\\pi(\\theta)\\;d\\theta = \\infty\\). Entonces decimos que \\(\\pi\\) es una densidad impropia. Ejemplo: \\(\\theta \\sim \\text{Unif}(\\mathbb{R})\\), \\(\\lambda \\sim \\text{Unif}(0,\\infty)\\). Una técnica para seleccionar distribuciones impropia es sustituir los hiperparámetros previos por 0. Ejemplo: Se presenta el número de soldados prusianos muertos por una patada de caballo (280 conteros, unidades de combate en 20 años). Unidades Ocurrencias 144 0 91 1 32 2 11 3 2 4 Muestra de Poisson: \\(X_1 = 0, X_2 = 1, X_3 = 1,\\dots, X_{280} = 0 \\sim \\text{Poi}(\\lambda)\\). Previa: \\(\\lambda \\sim \\Gamma(\\alpha, \\beta)\\). Posterior: \\(\\lambda|X \\sim \\Gamma(y+\\alpha, n+\\beta) = \\Gamma(196 + \\alpha, 280 + \\beta)\\). Sustituyendo, \\(\\alpha=\\beta = 0\\) \\[\\begin{align*} \\pi(\\lambda) &amp;= \\dfrac{1}{\\Gamma(\\alpha)}\\beta^\\alpha\\lambda^{\\alpha-1}e^{\\beta\\lambda} \\\\ \\\\ \\propto \\lambda^{\\alpha-1}e^{-\\lambda\\beta} \\\\ &amp;=\\dfrac{1}{\\lambda} \\end{align*}\\] donde \\(\\displaystyle\\int_{0}^{\\infty}\\dfrac{1}{\\lambda} d\\lambda = \\infty\\). Por teorema de Bayes, \\[\\theta|X \\sim \\Gamma(196,280)\\] 3.7 Funciones de pérdida Definición. Sean \\(X_1,\\dots, X_n\\) datos observables cuyo modelo está indexado por \\(\\theta\\in\\Omega\\). Un estimador de \\(\\theta\\) es cualquier estadístico \\(\\delta(X_1,\\dots, X_n)\\). Notación: Estimador \\(\\to \\delta(X_1,\\dots,X_n)\\). Estimación o estimado: \\(\\delta(X_1,\\dots,X_n)(\\omega) = \\delta(\\overbrace{x_1,\\dots,x_n}^{datos})\\) Definición. Una función de pérdida es una función de dos variables: \\[ L(\\theta,a), \\quad \\theta \\in\\Omega\\] con \\(a\\) un número real. Interpretación: es lo que pierde un analista cuando el parámetro es \\(\\theta\\) y el estimador es \\(a\\). Asuma que \\(\\theta\\) tiene una previa. La pérdida esperada es \\[ \\mathbb{E}[L(\\theta,a)] = \\int_{\\Omega}L(\\theta, a) \\pi(\\theta)\\;d\\theta\\] la cual es una función de \\(a\\), que a su vez es función de \\(X_1,\\dots,X_n\\). Asuma que \\(a\\) se selecciona el minimizar esta esperanza. A ese estimador \\(a = \\delta^*(X_1,\\dots, X_n)\\) se le llama estimador bayesiano, si ponderamos los parámetros con respecto a la posterior. \\[\\mathbb{E}[L(\\theta, \\delta^*)|X] = \\int_{\\Omega}L(\\theta, a) \\pi(\\theta)\\;d\\theta = \\min_a \\mathbb{E}[L(\\theta|a)X]. \\] 3.7.1 Función de pérdida cuadrática \\[ L(\\theta, a) = (\\theta-a)^2\\] En el caso en que \\(\\theta\\) es real y \\(\\mathbb{E}[\\theta|X]\\) es finita, entonces \\[ \\delta^*(X_1,\\dots, X_n) = \\mathbb{E}[\\theta|X] \\text{ cuando } L(\\theta,a) = (\\theta-a)^2. \\] Ejemplo: \\(X_1,\\dots, X_n \\sim \\text{Ber}(\\theta)\\), \\(\\theta \\sim \\text{Beta}(\\alpha,\\beta) \\implies \\theta|X \\sim \\text{Beta}(\\alpha+y,\\beta+n-y)\\). El estimador de \\(\\theta\\) es \\[ \\delta^*(X_1,\\dots, X_n) = \\dfrac{\\alpha+y}{\\alpha + \\beta + n} = \\overbrace{\\dfrac{\\alpha}{\\alpha + \\beta} }^{\\text{Esperanza previa}}\\cdot \\dfrac{\\alpha +\\beta}{\\alpha +\\beta + n} + \\overbrace{\\dfrac{y}{n}}^{\\bar X}\\cdot \\dfrac{n}{\\alpha +\\beta + n}. \\] 3.7.2 Función de pérdida absoluta \\[ L(\\theta,a) = |\\theta-a|\\] La pérdida esperada es \\[ f(a) = \\mathbb{E}[L(\\theta,a)|X] = \\int_{-\\infty}^{+\\infty}|\\theta-a|\\pi(\\theta|X)\\;d\\theta = \\int_{a}^{+\\infty}(\\theta-a)\\pi(\\theta|X)\\;d\\theta + \\int_{-\\infty}^{a}(a-\\theta)\\pi(\\theta|X)\\;d\\theta \\] Usando el teorema fundamental del cálculo, \\[F_{\\pi}(a|X) = \\int_{-\\infty}^{\\hat a}\\pi(\\theta|X)\\;d\\theta = \\dfrac12 \\Leftrightarrow \\hat a= \\operatorname*{argmin}_a f(a)\\] La mediana es el punto de \\(X_{0.5}\\) tal que \\(F(X_{0.5}) = \\dfrac{1}{2}\\). Corolario. Bajo la función de pérdida absoluta, el estimador bayesiano es la mediana posterior. Ejemplo: Bernoulli. \\[ \\dfrac{1}{\\text{Beta}(\\alpha+y, \\beta+n-y)}\\int_{-\\infty}^{X_{0.5}}\\theta^{\\alpha+y-1} (1-\\theta)^{\\beta+n-y-1}\\;d\\theta = \\dfrac12\\] Resuelva para \\(X_{0.5}\\). 3.7.3 Otras funciones de pérdida \\(L(\\theta,a) = |\\theta-a|^k\\), \\(k\\ne 1,2\\), \\(0&lt;k&lt;1\\). \\(L(\\theta,a) = \\lambda(\\theta)|\\theta-a|^2\\) (\\(\\lambda(\\theta)\\) penaliza la magnitud del parámetro). \\(L(\\theta,a)=\\begin{cases}3(\\theta-a)^2 &amp; \\theta\\leq a \\text{ (sobreestima)}\\\\ (\\theta-a)^2&amp;\\theta\\geq a \\text{ (subestima)} \\end{cases}\\) 3.8 Efecto de muestras grandes Ejemplo: ítemes malos (proporción: \\(\\theta\\)), \\(\\theta \\in [0,1]\\). Función de pérdida cuadrática. El tamaño de muestra son \\(n=100\\) ítemes, de los cuales \\(y=10\\) están malos. \\[ X_1,\\dots,X_n\\sim \\text{Ber}(\\theta)\\] Primer previa. \\(\\alpha = \\beta = 1\\) (Beta). El estimador bayesiano corresponde a \\[ \\mathbb{E}[\\theta|X] = \\dfrac{\\alpha+y}{\\alpha+\\beta+n} = \\dfrac{1+10}{2+100} = 0.108\\] Segunda previa. \\(\\alpha =1, \\beta=2 \\implies \\pi(\\theta) = 2e^{-2\\theta}, \\theta &gt;0\\). \\[ \\mathbb{E}[\\theta|X] = \\dfrac{1+10}{1+2+100} = \\dfrac{11}{103}=0.107\\] La media es \\(\\bar X_n = \\dfrac{10}{100} = 0.1\\). 3.9 Consistencia Definición. Un estimador de \\(\\theta\\) \\(\\delta(X_1,\\dots, X_n)\\) es consistente si \\[\\delta(X_1,\\dots, X_n)\\xrightarrow[n\\to \\infty]{\\mathbb{P}}\\theta.\\] Bajo pérdida cuadrática, \\(\\mathbb{E}[\\theta|X] = W_1\\mathbb{E}[\\theta] + X_2\\bar X_n = \\delta^*\\). Sabemos, por ley de grandes números, que \\(\\bar X_n \\xrightarrow[n\\to \\infty]{\\mathbb{P}}\\theta\\). Además, \\(W_1\\xrightarrow[n\\to \\infty]{}0\\) y \\(W_2\\xrightarrow[n\\to \\infty]{}1\\). En los ejemplos que hemos analizado \\[\\delta^* \\xrightarrow[n\\to \\infty]{\\mathbb{P}}\\theta \\] Teorema. Bajo condiciones generales, los estimadores bayesianos son consistentes. Estimador. Si \\(X_1,\\dots, X_n\\) es una muestra en un modelo indexado por \\(\\theta\\), \\(\\theta \\in \\Omega\\) (\\(k\\)-dimensiones), sea \\[h:\\Omega \\to H \\subset \\mathbb{R}^d.\\] Sea \\(\\psi = h(\\theta)\\). Un estimador de \\(\\psi\\) es un estadístico \\(\\delta^*(X_1,\\dots, X_n) \\in H\\). A \\(\\delta^*(X_1,\\dots, X_n)\\) estimador de \\(\\psi\\) se puede evaluar y construir estimadores nuevos. Ejemplo. \\(X_1,\\dots, X_n \\sim \\text{Exp}(\\theta)\\), \\(\\theta|X \\sim \\Gamma(\\alpha,\\beta) = \\Gamma (4,8.6)\\). La característica de interés es \\(\\psi = \\dfrac{1}\\theta\\), el valor esperado del tiempo de fallo. Es estimador se calcula de la siguiente manera: \\[\\begin{align*} \\delta^*(x) = \\mathbb{E}[\\psi|x] &amp; = \\int_{0}^\\infty \\dfrac{1}\\theta\\pi(\\theta|x)\\;d\\theta\\\\ &amp; = \\int_{0}^\\infty \\dfrac{1}\\theta \\dfrac{8.6^4}{\\Gamma(4)} \\theta^3e^{-8.6\\theta}\\;d\\theta\\\\ &amp;=\\dfrac{8.6^4}{6} \\underbrace{\\int_{0}^\\infty \\theta^2 e^{-8.6\\theta}\\;d\\theta}_{\\frac{\\Gamma(3)}{8.6^3}}\\\\ &amp; = \\dfrac{8.6^4}{6}\\dfrac{2}{8.6^3} = 2.867 \\text{ unidades de tiempo.} \\end{align*}\\] Por otro lado, vea que \\(\\mathbb{E}(\\theta|X) = \\dfrac{4}{8.6}\\). El estimador plug-in correspondería a \\[\\dfrac{1}{\\mathbb{E}(\\theta|X)} = \\dfrac{8.6}{4} = 2.15.\\] 3.10 Laboratorio Lo primero es cargar los paquetes necesarios que usaremos en todo el curso library(tidyverse) 3.10.1 Distribución previa En nuestro ejemplo se tenía que \\(\\mathbb E [\\theta] = 0.0002\\) y \\(\\mathrm{Var}(\\theta) = 0.001\\). Suponiendo que \\(\\theta\\) es gamma se puede resolver el sistema de ecuaciones obtenemos que \\(\\beta=20000\\) y \\(\\alpha=4\\). alpha_previa &lt;- 4 beta_previa &lt;- 20000 ggplot(data = data.frame(x = c(0, 1e6)), aes(x)) + stat_function(fun = dgamma, args = list(shape = alpha_previa, scale = beta_previa)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_minimal() 3.10.2 Distribución conjunta Asumiendo que tenemos algunos datos \\(X_1, ..., X_n\\), asumimos que estos son exponencial recordando que \\(\\mathbb E [X] = 1/\\theta\\), entonces una aproximación de esta densidad es x &lt;- c(2911, 3403, 3237, 3509, 3118) theta &lt;- 1 / mean(x) ggplot(data = data.frame(x = c(0, 1e5)), aes(x)) + stat_function(fun = dexp, args = list(rate = theta)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_minimal() 3.10.3 Distribución posterior Según los contenidos del curso, se puede estimar los parámetros de la densidad posterior de la forma (y &lt;- sum(x)) ## [1] 16178 (n &lt;- length(x)) ## [1] 5 (alpha_posterior &lt;- n + alpha_previa) ## [1] 9 (beta_posterior &lt;- beta_previa + y) ## [1] 36178 ggplot(data = data.frame(x = c(0, 7.5e5)), aes(x)) + stat_function(fun = dgamma, args = list(shape = alpha_previa, scale = beta_previa), aes(color = &quot;Previa&quot;)) + stat_function(fun = dgamma, args = list(shape = alpha_posterior, scale = beta_posterior), aes(color = &quot;Posterior&quot;)) + stat_function(fun = dexp, args = list(rate = theta), aes(color = &quot;Verosimilitud&quot;)) + ylim(0, 1.5e-5) + theme_minimal() 3.10.4 Agregando nuevos datos Si tenemos un 6to dato, y queremos ver cual es su distribución posterior. Lo primero es estimar la densidad posterior de este 6to dato, pero asumiendo que la previa es la densidad que obtuvimos en el caso anterior. Suponga que \\(X_6 = 3000\\) (alpha_previa &lt;- alpha_posterior) ## [1] 9 (beta_previa &lt;- beta_posterior) ## [1] 36178 (alpha_posterior &lt;- alpha_previa + 1) ## [1] 10 (beta_posterior &lt;- beta_previa + 3000) ## [1] 39178 ggplot(data = data.frame(x = c(0, 1e6)), aes(x)) + stat_function(fun = dgamma, args = list(shape = 4, scale = 20000), aes(color = &quot;Previa #1&quot;)) + stat_function(fun = dgamma, args = list(shape = alpha_previa, scale = beta_previa), aes(color = &quot;Previa #2&quot;)) + stat_function(fun = dgamma, args = list(shape = alpha_posterior, scale = beta_posterior), aes(color = &quot;Posterior&quot;)) + ylim(0, 1.5e-5) + theme_minimal() 3.10.5 Familias conjugadas normales Si tenemos pocos datos, la información previa es la que “prevalece”. x &lt;- rnorm(n = 3, mean = 10, sd = 1) (mu &lt;- mean(x)) ## [1] 9.952353 (sigma &lt;- sd(x)) ## [1] 1.506354 (n &lt;- length(x)) ## [1] 3 (mu_previa &lt;- 0) ## [1] 0 (sigma_previa &lt;- 1) ## [1] 1 (mu_posterior &lt;- ((sigma^2) / (sigma^2 + n * sigma_previa^2)) * mu_previa + ((n * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) * mu) ## [1] 5.66644 (sigma2_posterior &lt;- (sigma^2 * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) ## [1] 0.4306432 ggplot(data = data.frame(x = c(-5, 15)), aes(x)) + stat_function(fun = dnorm, args = list(mean = mu_previa, sd = sigma_previa), aes(color = &quot;Previa&quot;)) + stat_function(fun = dnorm, args = list(mean = mu_posterior, sd = sqrt(sigma2_posterior)), aes(color = &quot;Posterior&quot;)) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), aes(color = &quot;Verosimilitud&quot;)) + theme_minimal() Con más datos, la distribución se ajusta a esto y le quita importancia a la información previa. x &lt;- rnorm(n = 100, mean = 10, sd = 1) (mu &lt;- mean(x)) ## [1] 10.10844 (sigma &lt;- sd(x)) ## [1] 1.00343 (n &lt;- length(x)) ## [1] 100 (mu_previa &lt;- 0) ## [1] 0 (sigma_previa &lt;- 1) ## [1] 1 (mu_posterior &lt;- ((sigma^2) / (sigma^2 + n * sigma_previa^2)) * mu_previa + ((n * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) * mu) ## [1] 10.00768 (sigma2_posterior &lt;- (sigma^2 * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) ## [1] 0.009968347 ggplot(data = data.frame(x = c(-5, 15)), aes(x)) + stat_function(fun = dnorm, args = list(mean = mu_previa, sd = sigma_previa), aes(color = &quot;Previa&quot;)) + stat_function(fun = dnorm, args = list(mean = mu_posterior, sd = sqrt(sigma2_posterior)), aes(color = &quot;Posterior&quot;)) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), aes(color = &quot;Verosimilitud&quot;)) + theme_minimal() Si los datos por si solo son muy variable, la posterior tiende a parecerse a la distribución previa en lugar que a la verosimilitud. x &lt;- rnorm(n = 10, mean = 10, sd = 5) (mu &lt;- mean(x)) ## [1] 6.366894 (sigma &lt;- sd(x)) ## [1] 6.209978 (n &lt;- length(x)) ## [1] 10 (mu_previa &lt;- 0) ## [1] 0 (sigma_previa &lt;- 1) ## [1] 1 (mu_posterior &lt;- ((sigma^2) / (sigma^2 + n * sigma_previa^2)) * mu_previa + ((n * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) * mu) ## [1] 1.311036 (sigma2_posterior &lt;- (sigma^2 * sigma_previa^2) / (sigma^2 + n * sigma_previa^2)) ## [1] 0.7940854 ggplot(data = data.frame(x = c(-5, 15)), aes(x)) + stat_function(fun = dnorm, args = list(mean = mu_previa, sd = sigma_previa), aes(color = &quot;Previa&quot;)) + stat_function(fun = dnorm, args = list(mean = mu_posterior, sd = sqrt(sigma2_posterior)), aes(color = &quot;Posterior&quot;)) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), aes(color = &quot;Verosimilitud&quot;)) + theme_minimal() 3.10.6 Funciones de pérdida Lo más importante acá es que dependiendo de la función de pérdida podemos construir una estimador para \\(\\theta\\). En el caso de los componentes electrónicos recordemos que la posterior nos daba alpha &lt;- 9 beta &lt;- 36178 Pérdida cuadrática: Recoremos que la media de una gamma es \\(\\alpha/\\beta\\) entonces (theta &lt;- alpha / beta) ## [1] 0.00024877 Y por lo tanto el tiempo promedio del componente electrónico es \\(1/\\theta\\)=4019.7777778. Pérdidad absoluta: La distribución Gamma no tiene una forma cerrada para la mediana, por que se puede aproximar así, m &lt;- rgamma(n = 1000, scale = beta, shape = alpha) (theta &lt;- median (m)) ## [1] 311778.2 Y por lo tanto el tiempo promedio del componente electrónico es \\(1/\\theta\\)=3.2074086^{-6}. OJO: En este caso la pérdida cuadrática ajusta mejor ya que la distribución que la pérdida absoluta ya que la distribución NO es simétrica. En el caso simétrico los resultados serían muy similares. 3.10.7 Caso concreto Suponga que se que quiere averiguar si los estudiantes de cierto colegio duermen más de 8 horas o menos de 8 horas. Para esto primero cargaremos el siguiente paquete, library(LearnBayes) Suponga que se hace una encuesta a 27 estudiantes y se encuentra que 11 dicen que duermen más de 8 horas diarias y el resto no. Nuestro objetivo es encontrar inferencias sobre la proporción \\(p\\) de estudiantes que duermen al menos 8 horas diarias. El modelo más adecuado es \\[ f(x \\vert p) \\propto p^s (1-p)^f \\] donde \\(s\\) es la cantidad de estudiantes que duermen más de 8 horas y \\(f\\) los que duermen menos de 8 horas. Una primera aproximación para la previa es usar una distribución discreta. En este caso, el investigador asigna una probabilidad a cierta cantidad de horas de sueño, según su experiencia. Así, por ejemplo: p &lt;- seq(0.05, 0.95, by = 0.1) prior &lt;- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior &lt;- prior / sum(prior) plot(p, prior, type = &quot;h&quot;, ylab = &quot;Probabilidad Previa&quot;) El paquete LearnBayes tiene la función pdisc que estima la distribución posterior para una previa discreta binomial. Recuerde que el valor 11 representa la cantidad de estudiantes con más de 8 horas de sueño y 16 lo que no duermen esa cantidad. data &lt;- c(11, 16) post &lt;- pdisc(p, prior, data) round(cbind(p, prior, post), 2) ## p prior post ## [1,] 0.05 0.03 0.00 ## [2,] 0.15 0.18 0.00 ## [3,] 0.25 0.28 0.13 ## [4,] 0.35 0.25 0.48 ## [5,] 0.45 0.16 0.33 ## [6,] 0.55 0.07 0.06 ## [7,] 0.65 0.02 0.00 ## [8,] 0.75 0.00 0.00 ## [9,] 0.85 0.00 0.00 ## [10,] 0.95 0.00 0.00 Y podemos ver la diferencia entre la previa (negro) y la posterior (roja), plot(p, post, type = &quot;h&quot;, col = &quot;red&quot;) lines(p + 0.01, prior, type = &quot;h&quot;) ¿Qué se puede deducir de estos resultados? Ejercicio: Suponga que se tiene la base de datos studentdata. Realice los cálculos anteriores con esos datos, data(&quot;studentdata&quot;) horas_sueno &lt;- studentdata$WakeUp - studentdata$ToSleep horas_sueno &lt;- na.omit(horas_sueno) summary(horas_sueno) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.500 6.500 7.500 7.385 8.500 12.500 hist(horas_sueno, main = &quot;&quot;) Ahora supongamos que se tiene quiere ajustar una previa continua a este modelo. Para esto usaremos una distribución Beta con parámetros \\(\\alpha\\) y \\(\\beta\\), de la forma \\[ pi(p\\vert \\alpha, \\beta) \\propto p^{1-\\alpha} (1-p)^{1-\\beta}. \\] El ajuste de los paramétros de la Beta depende mucho de la información previa que se tenga del modelo. Una forma fácil de estimarlo es a través de cuantiles con los cuales se puede reescribir estos parámetros. Para una explicación detallada revisar https://stats.stackexchange.com/a/237849 En particular, suponga que se cree que el \\(50\\%\\) de las observaciones la proporción será menor que 0.3 y que el \\(90\\%\\) será menor que 0.5. Para esto ajustaremos los siguientes parámetros quantile2 &lt;- list(p = .9, x = .5) quantile1 &lt;- list(p = .5, x = .3) (ab &lt;- beta.select(quantile1, quantile2)) ## [1] 3.26 7.19 a &lt;- ab[1] b &lt;- ab[2] s &lt;- 11 f &lt;- 16 En este caso se obtendra la distribución posterior Beta con paramétros \\(\\alpha + s\\) y \\(\\beta + f\\), curve(dbeta(x, a + s, b + f), from = 0, to = 1, xlab = &quot;p&quot;, ylab = &quot;Densidad&quot;, lty = 1, lwd = 4) curve(dbeta(x, s + 1, f + 1), add = TRUE, lty = 2, lwd = 4) curve(dbeta(x, a, b), add = TRUE, lty = 3, lwd = 4) legend(.7, 4, c(&quot;Previa&quot;, &quot;Verosimilitud&quot;, &quot;Posterior&quot;), lty = c(3, 2, 1), lwd = c(3, 3, 3)) "],
["estimación-por-máxima-verosimilitud.html", "Capítulo 4 Estimación por máxima verosimilitud 4.1 Propiedades del MLE 4.2 Cálculo numérico 4.3 Laboratorio", " Capítulo 4 Estimación por máxima verosimilitud ¿Será posible estimar sin una densidad previa? Se debería ajustar la noción de muestra a independencia dado el valor de un parámetro. Recuerde que, para \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} f(X|\\theta)\\) con \\(\\theta\\) fijo, la función de verosimilitud se define como \\[ f_n(X|\\theta) = \\pi(X_i|\\theta) = G(\\theta|X).\\] Si \\(\\theta_1,\\theta_2\\in \\Omega\\), \\(\\theta\\) es el valor real del parámetro. Si la muestra es fija, evaluamos, para \\(\\theta_1\\), \\(f_n(X|\\theta_1) = L(\\theta_1|X)\\) y, de igual forma para \\(\\theta_2\\), \\(f_n(X|\\theta_2) = L(\\theta_2|X)\\). Supongamos que \\[\\begin{equation*} f_n(X|\\theta_1) &gt;f_n(X|\\theta_2) \\implies L(\\theta_1|X)&gt;L(\\theta_2|X) \\text{ (principio de verosimilitud)} \\end{equation*}\\] Interpretación. Es más verosímil (realista) que el verdadero parámetro sea \\(\\theta_1\\) que \\(\\theta_2\\) dada la muestra. Definición. Para cada \\(x\\in \\mathcal{X}\\) (espacio muestral), sea \\(\\delta(x) \\in \\delta\\) estimador de \\(\\theta\\) tal que \\(f_n(x|\\theta)\\) es máximo. A \\(\\delta(x)\\) se le llama MLE (estimador de máxima verosimilitud). Ejemplo. Si \\(X_1,\\dots, X_n \\sim \\text{Exp}(\\theta)\\), estime \\(\\theta\\). Determinamos la función de verosimilitud, \\[\\begin{equation*} f_n(X|\\theta) = \\prod_{i=1}^n \\dfrac{1}\\theta e^{-X_i/\\theta} = \\dfrac1{\\theta^n} \\exp\\left(\\dfrac{1}\\theta \\sum_{i=1}^nX_i\\right) = \\theta^{-n}e^{-y/\\theta}. \\end{equation*}\\] Considere la log-verosimilitud \\[\\begin{equation*} \\ell(\\theta|X) = \\ln f_n(X|\\theta) = -n\\ln \\theta - \\dfrac{y}{\\theta} \\end{equation*}\\] Como es una transformación monótona creciente, la función de verosimilitud se maximiza si la log-verosimilitud es máxima. Entonces, \\[\\begin{align*} \\dfrac{\\partial}{\\partial\\theta} \\ell(\\theta|X) &amp; = \\dfrac{-n}{\\theta}+\\dfrac{y}{\\theta^2} = 0 \\\\ \\implies \\dfrac{1}{\\theta}\\left(-n+\\dfrac{y}\\theta\\right) &amp; =0 \\\\ \\implies \\hat\\theta = \\dfrac{y}{n} &amp; = \\bar X_n. \\end{align*}\\] Para verificar que es un máximo: \\[\\dfrac{\\partial^2 \\ell}{\\partial\\theta^2} = \\left. \\dfrac{n}{\\theta^2} -\\dfrac{2y}{\\theta^3}\\right\\vert_{\\theta = \\frac{y}{n}} = \\dfrac{1}{\\hat\\theta^2} \\bigg[n-\\dfrac{2y}{\\frac yn}\\bigg] = \\dfrac{-n}{\\hat\\theta^2} &lt; 0.\\] Entonces \\(\\hat\\theta = \\bar X_n\\) es el MLE de \\(\\theta\\). Laboratorio: Suponga que se tiene 100 valores con distribución exponencial con parámetro \\(\\theta=1\\). x &lt;- rexp(n = 100, rate = 1) n &lt;- length(x) y &lt;- sum(x) theta &lt;- seq(0.5, 1.5, length.out = 1000) L &lt;- theta^(-n) * exp(-y / theta) plot(theta, L) l &lt;- -n * log(theta) - y / theta plot(theta, l) Ejemplo. En una prueba sobre alguna enfermedad, en un \\(90\\%\\) da la verdadera condición (enfermo) y en un \\(10\\%\\) la prueba se equivoca (que diga que la persona esté enferma cuando está sana). Considere una variable aleatoria \\(\\text{Bernoulli}(\\theta)\\),\\(\\theta \\in \\{0.9,0.1\\}\\) Una muestra sería \\[x = \\begin{cases}1 &amp; \\text{si la prueba es positiva}\\\\0&amp; \\text{si no}\\end{cases}\\] Si \\(x=0\\), entonces \\(f(0|\\theta) = \\begin{cases}0.9 &amp; \\text{si }\\theta = 0.1\\\\0.1&amp; \\text{si }\\theta = 0.9\\end{cases}\\). Si \\(x=1\\), entonces \\(f(1|\\theta) = \\begin{cases}0.1 &amp; \\text{si }\\theta = 0.1\\\\0.9&amp; \\text{si }\\theta = 0.9\\end{cases}\\). El MLE corresponde a \\[\\hat\\theta = \\begin{cases}0.1 &amp; \\text{si }x= 0\\\\0.9&amp; \\text{si }x= 1\\end{cases}\\] Ejemplo. Para el caso normal, \\(X_1,\\dots, X_n \\sim N(\\mu,\\sigma^2)\\), \\(\\sigma^2\\) conocida, estime \\(\\mu\\). \\[f_n(x|\\mu) = \\prod_{i=1}^n \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\dfrac{(x_i-\\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2}\\exp\\left(-\\dfrac1{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\right).\\] La log-verosimilitud es de la forma \\[ \\ell(\\mu|x) = \\dfrac{-n}{2}\\ln(2\\pi\\sigma^2)-\\dfrac1{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2.\\] Basta con minimizar \\(Q(\\mu) = \\sum_{i=1}^n(x_i-\\mu)^2\\). \\[ \\dfrac{\\partial Q}{\\partial\\mu} = -2\\sum_{i=1}^n(x_i-\\mu) \\implies n\\mu = \\sum_{i=1}^nx_i \\implies \\hat\\mu = \\bar x_n.\\] No hace falta verificar la condición de segundo orden, pues \\(Q\\) es una función cuadrática de \\(\\mu\\) y tiene un único máximo. \\[ \\hat\\mu_{MLE} = \\bar x_n \\quad (*)\\] Ahora, si \\(X_1,\\dots, X_n \\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu,\\sigma^2)\\) desconocido, por \\((*)\\), \\[ \\ell(\\sigma^2|X_1,\\dots, X_n) = \\dfrac n2 \\ln(2\\pi\\sigma^2)--\\dfrac1{2\\sigma^2}\\sum_{i=1}^n(x_i-\\bar x_n)^2 \\] \\[ \\dfrac{\\partial L}{\\partial\\sigma^2} = -\\dfrac n2 \\dfrac1{2\\pi\\sigma^2} + \\dfrac1{2(\\sigma^2)^2} \\sum_{i=1}^n(x_i-\\bar x_n)^2= 0 \\] Entonces \\[ \\sigma^2 = \\dfrac 1n \\sum_{i=1}^n(x_i-\\mu)^2 \\text{ (varianza muestral)}\\] Las condiciones de segundo orden quedan como ejercicio. Nota. Si \\(\\theta_{MLE}\\) de \\(\\theta\\), entonces \\(h(\\theta_{MLE})\\) es el MLE de \\(h(\\theta)\\). Sea \\(h(x,y) = \\sqrt{y}\\) (es inyectiva). \\(h(\\bar x_n, \\hat\\sigma^2) = \\sqrt{\\hat\\sigma^2} = \\hat\\sigma\\). El MLE de \\(\\dfrac{\\sigma}{\\mu} = \\dfrac{\\hat \\sigma}{\\bar x_n}\\). Laboratorio: library(scatterplot3d) x &lt;- rnorm(100) n &lt;- length(x) mu &lt;- seq(-0.5, 0.5, length.out = 50) sigma &lt;- seq(0.5, 1.5, length.out = 50) ms &lt;- expand.grid(sigma, mu) l &lt;- -(n / 2) * log(2 * pi / ms[, 1]^2) - (1 / (2 * ms[, 1]^2) * sum((x - ms[, 2])^2)) scatterplot3d(ms[, 1], ms[, 2], l, angle = 45) Ejemplo. \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} \\text{Unif}(0, \\theta)\\). Estime \\(\\theta\\) \\((\\theta &gt; 0)\\). Suponga que \\(x_i&gt;0 \\forall i\\). \\[f(X|\\theta) = \\dfrac 1\\theta \\cdot 1_{[0,\\theta]}(x)\\] La verosimilitud es \\[f_n(x|\\theta) = \\prod_{i=1}^{n} f(x_i|\\theta) = \\dfrac 1{\\theta^n} \\prod_{i=1}^n 1_{\\{0\\leq X_i\\leq \\theta\\}} \\quad 0\\leq X_i \\leq \\theta \\;\\forall i\\] Vea que \\(f_n(x|\\theta)\\) es válido si y solo si \\(0\\leq X_{(n)}\\leq \\theta\\). El valor de la muestra \\(\\{X_1,\\dots, X_n\\}\\) en la \\(i\\)-ésima posición cuando los datos se ordenan de menor a mayor se denota \\(X_{(i)}\\) (estadístico de orden). En este caso, \\(X_{(n)} = \\max\\{X_1,\\dots, X_n\\}\\). Entonces \\(\\hat\\theta_{MLE} = x_{(n)}\\). Laboratorio: x &lt;- runif(100, 0, 2) n &lt;- length(x) theta &lt;- seq(1.5, 2.5, length.out = 1000) L &lt;- numeric(1000) for (k in 1:1000) { L[k] &lt;- 1 / theta[k]^n * prod(x &lt; theta[k]) } plot(theta, L) 4.1 Propiedades del MLE 4.1.1 Propiedad de invarianza Teorema. Si \\(\\hat\\theta\\) es el MLE de \\(\\hat\\theta\\) y si \\(g\\) es biyectiva, entonces \\(g(\\theta)\\) es el MLE de \\(g(\\theta)\\). Prueba: Sea \\(\\Gamma\\) el espacio paramétrico \\(g(\\Omega)\\). Como \\(g\\) es biyectiva entonces defina \\(h\\) la inversa de \\(g\\colon \\theta = h(\\psi), \\psi \\in \\Gamma\\). Reparametrizando la verosimilitud, \\[f_n(x|\\theta) = f_n(x|h(\\psi)). \\] El MLE de \\(\\psi:\\hat\\psi\\) satisface que \\(f_n(x|h(\\hat\\psi))\\) es máximo. Como \\(f_n(x|\\theta)\\) se maximiza cuando \\(\\theta = \\hat \\theta\\), entonces \\(f_n(x|h(\\psi))\\) se maximiza cuando \\(\\hat \\theta = h(\\psi)\\) para algún \\(\\psi\\). Se concluye que \\(\\hat\\theta = h(\\hat\\psi) \\implies \\hat\\psi = g(\\hat \\theta)\\). Ejemplo: \\(g(\\theta) = \\dfrac 1\\theta\\) es biyectiva si \\(\\theta &gt; 0\\). Así, \\[\\begin{equation*} \\frac{1}{\\hat{\\theta}} = \\frac{1}{\\frac{1}{\\bar{X}_n}} = \\bar{X}_n \\text{ es parámetro de la tasa.} \\end{equation*}\\] ¿Qué pasa si \\(h\\) no es biyectiva? Definicion (Generalización del MLE). Si \\(g\\) es una función de \\(\\theta\\) y \\(G\\) la imagen de \\(\\Omega\\) bajo \\(g\\). Para cada \\(t\\in G\\) defina \\[ G_t = \\{\\theta: g(\\theta) = t\\}\\] Defina \\(L^*(t) = \\displaystyle\\max_{\\theta\\in G_t} \\ln f_n(x|\\theta)\\). El MLE de \\(g(\\theta) (=\\hat t)\\) satisface \\(L^*(\\hat t) = \\displaystyle\\max_{t \\in G} L^*(t)\\). Teorema. Si \\(\\hat \\theta\\) es el MLE de \\(\\theta\\) entonces \\(g(\\hat\\theta)\\) es el MLE de \\(g(\\theta)\\) (\\(g\\) es arbitraria). Prueba. Basta probar \\(L^*(\\hat t) = \\ln f_n(x|\\hat \\theta)\\). Se cumple que \\(\\hat\\theta\\in G_{\\hat t}\\). Como \\(\\hat \\theta\\) maximiza \\(f_n(x|\\theta)\\) \\(\\forall \\theta\\), también lo hace si \\(\\theta \\in G_{\\hat t}\\). Entonces \\(\\hat t = g(\\hat \\theta)\\) (no pueden existir 2 máximos en un conjunto con la misma imagen). Ejemplos.\\(X_1,\\dots, X_n \\sim N(\\mu, \\sigma^2)\\). Si \\(h(\\mu, \\sigma^2) = \\sigma\\) (no es biyectiva) \\(\\implies h(\\hat X_n,\\hat\\sigma^2) = \\sqrt{\\hat\\sigma^2}\\) es el MLE de \\(\\sigma\\). \\(h(\\mu,\\sigma^2) = \\dfrac{\\sigma^*}{\\mu}\\) (coeficiente de variación). \\(\\dfrac{\\hat{\\sigma}}{\\bar X_n}\\) es el MLE de CV. \\(h(\\mu, \\sigma^2) = \\mu^2 + \\sigma^2\\). \\(\\mathbb{E}[X^2] - \\mu^2 = \\sigma^2 \\implies \\mathbb{E}[X^2] = \\mu^2 + \\sigma^2\\). El MLE de \\(\\mathbb{E}[X^2]\\) es \\(\\bar X_n^2 + \\hat \\sigma ^2\\). 4.1.2 Consistencia Los estimadores bayesianos son de la forma \\[EB = W_1\\mathbb{E}[\\text{Previa}] + W_2\\hat X_n.\\] El estimador bayesiano “combina” la esperanza de la previa y el \\(\\hat\\theta_{MLE}\\). El \\(\\hat\\theta_{MLE}\\) “hereda la consistencia del estimador bayesiano”. \\[EB = W_1\\mathbb{E}[\\text{Previa}] + W_2 \\hat\\theta_{MLE}.\\] Afirmación. Bajo “condiciones usuales”, \\[\\hat\\theta_{MLE} \\xrightarrow[n\\to \\infty]{\\mathbb P}\\theta.\\] 4.2 Cálculo numérico 4.2.1 Método de los momentos Ejemplo. \\(X_1,\\dots, X_n \\sim \\Gamma(\\alpha,1)\\). Estime \\(\\alpha\\). \\[f_n(x|\\alpha) = \\dfrac{1}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-x}.\\] Verosimilitud: \\(f_n(x|\\alpha) = \\dfrac 1 {\\Gamma(\\alpha)^n}(\\prod x_i)e^{\\sum x_i}\\). \\[\\begin{align*} \\dfrac{\\partial}{\\partial \\alpha} L(\\alpha|x) &amp; = \\dfrac{\\partial}{\\partial \\alpha} \\bigg[ -n\\ln \\Gamma(\\alpha) + (\\alpha-1)\\ln(\\pi x_i) - \\sum x_i\\bigg]\\\\ &amp; = -n\\dfrac{1}{\\Gamma(\\alpha)} \\dfrac d{d\\alpha}\\Gamma(\\alpha) + \\ln (\\prod x_i) = 0 \\end{align*}\\] Definición. Asumimos que \\(X_1,\\dots, X_n \\sim F\\) indexada con un parámetro \\(\\theta \\in \\mathbb{R}^k\\) y que al menos tiene \\(k\\) momentos finitos. Para \\(j = 1,\\dots, k\\) sea \\(\\mu_j(\\theta) = \\mathbb{E}[X_1^j|\\theta]\\). Suponga que \\(\\mu(\\theta) = (\\mu_1(\\theta),\\dots,\\mu_2(\\theta))\\) es biyectiva. Sea \\(M\\) la inversa de \\(\\mu\\), \\[ M(\\mu(\\theta)) = \\theta =M(\\mu_1(\\theta),\\dots,\\mu_2(\\theta)) \\] y defina los momentos empíricos \\[ m_j = \\dfrac 1n \\sum_{i=1}^n X_i^j, \\quad j=1,\\dots, k.\\] El estimador según el método de los momentos es \\[\\hat\\theta = M(m_1,\\dots,m_k).\\] Del ejemplo anterior, \\(\\mu_1(\\alpha) = \\mathbb{E}[x_1|\\alpha] = \\alpha\\).Dado que \\(m_1 = \\hat x_n\\), el sistema por resolver es \\[ \\mu_1(\\alpha) = m_1 \\Longleftrightarrow \\alpha = \\bar x_n\\] El estimador por método de momentos es \\(\\hat \\alpha = \\bar X_n\\). Ejemplo. \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} \\Gamma(\\alpha, \\beta)\\). La varianza de \\(X\\) es \\[ \\dfrac{\\alpha}{\\beta^2} = \\text{Var}X = \\mathbb{E}[X^2]-\\mathbb{E}[X]^2 = \\mathbb{E}[X^2] - \\dfrac{\\alpha^2}{\\beta^2}.\\] Se debe resolver el sistema \\[ \\begin{cases}\\mu_1(\\theta) = \\dfrac{\\alpha}\\beta = \\bar X_n = m_1&amp; (1)\\\\\\mu_2(\\theta) = \\dfrac{\\alpha(\\alpha+1)}{\\beta^2}=m_2 &amp; (2)\\end{cases}\\] De \\((1)\\), \\(\\alpha = m_1\\beta\\). Sustituyendo en \\((2)\\), \\[m_2 = \\dfrac{m_1\\beta(m_1\\beta+1)}{\\beta^2} = m_1^2+\\dfrac{m_1}\\beta = m_2\\implies m_2-m_1^2 = \\dfrac{m_1}{\\beta}.\\] De esta manera, \\[ \\hat\\beta = \\dfrac{m_1}{m_2-m_1^2},\\quad \\hat\\alpha = \\dfrac{m_1^2}{m_2-m_1^2}\\] Teorema. Si \\(X_1,X_2,\\dots\\) i.i.d con distribución indexada por \\(\\theta \\in \\mathbb{R}^k\\). Suponga que los \\(k\\) momoentos teóricos son finitos \\(\\forall \\theta\\) y suponga que \\(M\\) es continua. Entonces el estimador por el método de momentos es consistente. ¿Cuál es el comportamiento en la distribución de \\(\\hat\\theta\\) cuando la muestra es grande? Del teorema del límite central, \\[ \\dfrac{\\bar X_n-\\theta}{\\dfrac{\\sigma}{\\sqrt{n}}} = \\dfrac{\\sqrt{n}(\\bar X_n-\\theta)}{\\sigma} \\xrightarrow{d} N(0,1)\\] \\[\\text{Var}(\\bar X_n) = \\dfrac 1{n^2}\\sum \\text{Var}(X_1) = \\dfrac{\\sigma^2}n\\] Implica que se debe multiplicar la media muestral por una constante para hacer la desviación visibile y, con ello, hacer inferencia del parámetro. Caso general. Si \\(f(X|\\theta)\\) es “suficientemente suave” como función de \\(\\theta\\), es puede comprobar que la verosimilitud tiende a una normal conforme \\(n \\to \\infty\\). Es decir, \\[ f(X|\\theta)\\propto \\exp\\Bigg[\\dfrac{-1}{2\\frac{V_n(\\theta)}{n}}(\\theta-\\hat\\theta)\\Bigg], \\quad n \\to \\infty\\quad(*) \\] donde \\(\\hat\\theta\\) es el MLE de \\(\\theta\\). \\[V_n(\\theta)\\xrightarrow[n\\to \\infty]{}V_{\\infty}(\\theta)&lt;\\infty\\] Notas: Si \\(n \\to \\infty\\) la normal en \\((*)\\) tiene muchísima precisión y es concentrada alrededor de \\(\\hat\\theta\\). En el caso bayesiano, ninguna previa en \\(\\theta\\) puede anular el efecto en la verosimilitud cuando \\(n\\to \\infty\\). Por \\((*)\\) el MLE se distribute asintóticamente como \\[N\\left(\\theta, \\dfrac{V_\\infty(\\theta)}{n}\\right),\\] \\(Var(X_n)\\xrightarrow[n\\to \\infty]{} 0\\) y \\(\\mathbb{E}[X_n] = X \\implies X_n \\xrightarrow[n\\to \\infty]{\\mathbb P} X\\) (confirma que el MLE es consistente). 4.2.2 Método Delta Si \\(Y_1,Y_2,\\dots\\) es una sucesión de variables aleatorias y sea \\(F^*\\) su c.d.f. continua. Sea \\(\\theta\\in \\mathbb R\\) y \\(\\{a_n\\}\\) sucesión de números positivos tal que \\(a_n \\nearrow\\infty\\). Suponga que \\(a_n(Y_n-\\theta) \\xrightarrow{d} F^*\\). Si \\(\\alpha\\) es una función tal que \\(\\alpha&#39;(\\theta)\\ne 0\\), entonces \\[\\dfrac{a_n}{\\alpha&#39;(\\theta)}[\\alpha(Y_n)-\\alpha(\\theta)] \\xrightarrow{d} F^*\\] Ejemplo. \\(X_1,X_2,\\dots\\) i.i.d. de variables con media \\(\\mu\\) y varianza \\(\\sigma^2\\). Sea \\(\\alpha\\) una función tal que \\(\\alpha&#39;(\\mu)\\neq 0\\). Por el T.L.C, \\[ \\dfrac{\\sqrt{n}}{\\sigma}(X_n-\\mu)\\xrightarrow{d}N(0,1)\\] Entonces por el método Delta \\[ \\dfrac{\\sqrt{n}}{\\sigma\\alpha&#39;(\\mu)}[\\alpha(\\bar X_n)-\\alpha(\\mu)]\\xrightarrow{d}N(0,1) \\] Si \\(\\alpha(\\mu) = \\dfrac 1\\mu\\) \\((\\mu\\neq 0) \\implies -\\dfrac{1}{\\mu^2} = \\alpha&#39;(\\mu)\\). Entonces por el método Delta \\[ \\dfrac{\\sqrt{n}}{\\sigma}\\mu^2\\bigg[\\dfrac 1{\\bar X_n}-\\dfrac 1\\mu\\bigg]\\xrightarrow{d}N(0,1) \\] Ejemplo Si \\(X_1,X_2\\dots \\stackrel{i.i.d}{\\sim} \\text{Exp}(\\theta)\\). Sea \\(T_n = \\sum X_i \\implies \\hat\\theta = \\dfrac 1{\\bar X_n} = \\dfrac n{T_n}\\). Note que \\(\\dfrac{1}{\\hat{\\theta}} = \\bar X_n\\) y \\[ \\dfrac{\\sqrt{n}}{\\sigma}\\bigg[\\bar X_n-\\dfrac 1\\theta\\bigg]\\xrightarrow[n\\to\\infty]{d}N(0,1) .\\] La varianza de una exponencial es \\(\\sigma^2 = \\text{Var}(X_1) = \\dfrac1{\\theta^2}\\), entonces \\[ \\theta\\sqrt{n}\\bigg[\\bar X_n-\\dfrac 1\\theta\\bigg]\\xrightarrow[n\\to\\infty]{d}N(0,1) .\\] El método Delta nos dice, con \\(\\alpha(\\mu) = \\dfrac 1\\mu\\), \\(\\alpha&#39;(\\mu) = -\\dfrac 1{\\mu^2}\\), el comportamiento asintótico de MLE: \\[\\begin{align*} \\dfrac{\\theta\\sqrt{n}}{\\alpha&#39;(1/\\theta)}\\bigg[\\bar \\alpha(X_n)-\\alpha\\left(\\dfrac 1\\theta\\right)\\bigg] &amp; = \\dfrac{\\theta\\sqrt{n}}{\\dfrac{1}{1/\\theta}}\\bigg[ \\dfrac 1{\\bar X_n} -\\theta\\bigg]\\xrightarrow[n\\to\\infty]{d}N(0,1) \\\\ &amp; = \\dfrac{\\sqrt{n}}{\\theta}\\bigg[\\dfrac 1{\\bar X_n} -\\theta\\bigg]\\xrightarrow[n\\to\\infty]{d}N(0,1) \\end{align*}\\] El MLE \\(\\hat\\theta = \\dfrac 1{\\bar X_n}\\) es asintóticamente normal con media \\(\\theta\\) y varianza \\(\\dfrac{V_n(\\theta)}{n} = \\dfrac{\\theta^2}n\\). Caso bayesiano. Tome una previa conjugada \\(\\theta \\sim \\Gamma(\\alpha,\\beta)\\), posterior \\(\\theta \\sim \\Gamma(\\alpha+n,\\beta+y)\\), \\(y = \\sum X_i\\). Supongamos que es entero positivo. \\[\\Gamma(\\alpha+n,\\beta+y) \\sim \\sum_{i=1}^{\\alpha+n}e^{\\beta+y}\\] Por el T.L.C., la distribución posterior \\(\\theta|X\\) se distribuye como una normal con media \\(\\dfrac{\\alpha+n}{\\beta+y}\\) y varianza \\(\\dfrac{\\alpha+n}{(\\beta+y)^2}\\). Tomando una previa poco informativa, (\\(\\alpha, \\beta\\) son pequeños), la media es \\[\\dfrac ny = \\dfrac 1{\\bar X_1} = \\hat\\theta_{MLE}\\] y la varianza \\[\\dfrac 1{y^2/n} = \\dfrac{\\theta^2}n = \\dfrac{V_n(\\hat\\theta)}{n}.\\] 4.3 Laboratorio Suponga que tenemos una tabla con los siguientes datos, los cuales representan la cantidad de giros hacia la derecha en cierta intersección. (X &lt;- c(rep(0, 14), rep(1, 30), rep(2, 36), rep(3, 68), rep(4, 43), rep(5, 43), rep(6, 30), rep(7, 14), rep(8, 10), rep(9, 6), rep(10, 4), rep(11, 1), rep(12, 1))) ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 ## [26] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 ## [51] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [76] 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [101] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [126] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 ## [151] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ## [176] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 ## [201] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 ## [226] 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [251] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 ## [276] 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 10 10 10 10 11 12 Queremos ajustar esta tabla a una distribución Poisson con función de densidad \\[ \\mathbb{P}(X=x) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\] Se puede probar que teórico de máxima verosimilitud para \\(\\lambda\\) es \\(\\overline{X}\\) (Tarea). Queremos estimar este parámetro alternativamente maximizando la función de verosimilitud. Primero veamos la forma de los datos, hist(X, main = &quot;histograma del número de giros a la derecha&quot;, right = FALSE, prob = TRUE) Definamos la función correspondiente a \\(-\\log(\\mathbb{P}(X=x))\\) n &lt;- length(X) negloglike &lt;- function(lambda) { n * lambda - sum(X) * log(lambda) + sum(log(factorial(X))) } Para encontrar el parámetro deseado, basta minimizar la función negloglike usando el la instrucción de optimización no lineal nlm. lambda.hat &lt;- nlm(negloglike, p = c(0.5), hessian = TRUE) Aquí el valor p = c(0.5) representa un valor inicial de búsqueda y hessian = TRUE determina el cálculo explícito de la segunda derivada. Compare el resultado de lambda.hat$estimate con mean(X). lambda.hat$estimate ## [1] 3.893331 mean(X) ## [1] 3.893333 "],
["estadísticos-suficientes-y-criterio-de-factorización.html", "Capítulo 5 Estadísticos Suficientes y Criterio de Factorización 5.1 Estadísticos suficientes 5.2 Teorema de Factorización de Fisher 5.3 Estadístico suficiente multivariado. 5.4 Estadísticos minimales 5.5 Mejorando estimadores", " Capítulo 5 Estadísticos Suficientes y Criterio de Factorización 5.1 Estadísticos suficientes Una función de verosimilitud se va a describir a través de un número. El objetivo es buscar un estadístico \\(T=r(X_1,\\dots,X_n)\\) que resuma de manera óptima la información de \\(X_1,\\dots,X_n\\) Definición. Sea \\(X_1,\\dots,X_n\\) una muestra indexada por \\(\\theta\\). Sea \\(T\\) un estadístico, suponga que para cada \\(\\theta \\in \\Omega\\) y para cada \\(t\\) en la imagen de \\(T\\), \\(X_1\\cdots X_n|T=t\\) depende solamente de \\(t\\) y no de \\(\\theta\\). Entonces \\(T\\) es suficiente. 5.2 Teorema de Factorización de Fisher Teorema. Si \\(X_1,\\dots,X_n\\) es una muestra aleatoria de \\(f(X|\\theta)\\), el parámetro \\(\\theta\\) es desconocido. Un estadístico \\(T=r(X_1,\\dots,X_n)\\) es suficiente si y solo si \\[f_n(x|\\theta) = u(x)v(r(x),\\theta)\\;\\forall x\\in \\mathbb R, \\; \\forall \\theta \\in \\mathbb R.\\] Prueba (Discreta). \\(f_n(x|\\theta) = \\mathbb P(X=x|\\theta)\\) “\\(\\Leftarrow\\)” Sea \\(A(t) = \\{x\\in \\mathbb R| r(x) =t\\}\\). Para \\(\\theta \\in \\mathbb R\\), \\(x\\in A(t)\\), \\[\\begin{align*} \\mathbb P(X=x|T=t) &amp; = \\dfrac{\\mathbb P(X=x \\cap T=t)}{\\mathbb P (T=t)} \\\\ &amp;= \\dfrac{f_n(x|\\theta, T=t)}{\\displaystyle\\sum_{y \\in A(t)}f_n(y|\\theta)} \\\\ &amp; = \\dfrac{u(x)v(r(x),\\theta)}{\\displaystyle\\sum_{y \\in A(t)} u(y)v(r(y),\\theta)} \\\\ &amp; = \\dfrac{u(x)v(t,\\theta)}{\\displaystyle v(t,\\theta)\\sum_{y \\in A(t)} u(y)} \\text{(Como \\(y\\in A(t)\\) entonces \\(r(y) = t\\) que es constante.)}\\\\ &amp;= \\dfrac{u(x)}{\\displaystyle\\sum_{y \\in A(t)}u(y)} \\end{align*}\\] no depende de \\(\\theta\\). Si \\(x\\notin A(t) \\implies \\mathbb P(X=x|T=t) = 0\\) no depende de \\(\\theta\\). “\\(\\Rightarrow\\)” Si \\(T\\) es un estadístico suficiente, \\(u(x) = \\mathbb P(X=x|T=t)\\) no depende de \\(\\theta\\). Sea \\(v(t,\\theta) = \\mathbb P_{\\theta}(T=t)\\). Entonces \\[ f_n(x|\\theta) = \\mathbb P (X=x|\\theta) = \\dfrac{\\mathbb P(X=x|\\theta)}{\\mathbb P(T=t)}\\mathbb P(T=t) = u(x)v(t,\\theta).\\] Consecuencia: \\(f_n(x|\\theta) \\propto v(r(x),\\theta)\\) (\\(u(x)\\) es una constante con respecto a \\(\\theta\\)). Aplicando el teorema de Bayes, \\[ \\pi(\\theta|x) \\propto \\pi(\\theta)v(r(x),\\theta).\\] Corolario. Un estadístico \\(r(x)\\) es suficiente si y solo si no importa cuál previa de \\(\\theta\\) se use, la posterior depende solamente de \\(r(x)\\) a través de los datos. Ejemplo. \\(X_1,\\dots, X_n \\sim \\text{Poi}(\\lambda)\\), \\[f_n(x|\\theta) = \\prod_{i=1}^n \\dfrac{e^{-\\lambda}}{x_i!} = \\dfrac{e^{-\\lambda n} \\lambda ^{\\sum x_i (\\;= r(x))}}{\\prod x_i!} = \\underbrace{\\dfrac{1}{\\prod_{i=1}^n x_i!}}_{u(x)} \\underbrace{e^{-\\lambda n}\\lambda^{r(x)}}_{v(r(x),\\lambda)}\\] Si \\(x_i &lt; 0\\) para al menos un \\(i\\), entonces \\(f_n(x|\\theta) = 0\\). Tome \\(u(x) = 0\\). Por el teorema de factorización, \\(r(x) = \\sum x_i\\) es un estadístico suficiente para \\(\\lambda\\). Ejemplo. \\(X_1,\\dots, X_n \\sim f(x|\\theta)\\) \\[ f(x|\\theta) = \\begin{cases}\\theta x^{\\theta-1} &amp; 0&lt;x&lt; 1\\\\ 0 &amp; \\text{otro caso}\\end{cases}\\] Verosimilitud: (\\(0&lt;x_i&lt;1\\) \\(\\forall i\\)) \\[ f_n(x|\\theta) = \\theta^n\\bigg[\\underbrace{\\prod(x_i)}_{r(x)}\\bigg]^{\\theta-1} = \\underbrace{\\theta^n(r(x))^{\\theta-1}}_{v(r(x),\\theta)}\\cdot \\underbrace{1}_{u(x)}\\] Por el teorema de factorización \\(r(x) = \\prod x_i\\) es un estadístico suficiente., Ejemplo. \\(X_1,\\dots, X_n \\sim N(\\mu, \\sigma^2)\\) (\\(\\sigma^2\\) conocido). \\[\\begin{align*} f_n(x|\\theta) &amp; = (2\\pi\\sigma^2)^{-n/2} \\exp\\bigg[-\\dfrac{1}{2\\sigma^2}\\sum_{i=1}^n(X_i-\\mu)^2\\bigg] \\\\ &amp; = (2\\pi\\sigma^2)^{-n/2} \\exp\\bigg[-\\dfrac{1}{2\\sigma^2}\\underbrace{\\sum_{i=1}^n X_i^2}_{r_2(x)}+ \\dfrac{\\mu}{\\sigma^2}\\underbrace{\\sum_{i=1}^n X_i}_{r_1(x)} - \\dfrac{\\mu^2 n}{2\\sigma^2} \\bigg] \\end{align*}\\] Tome \\[u(x) = (2\\pi\\sigma^2)^{-n/2}\\exp\\bigg[-\\dfrac{1}{2\\sigma^2} \\displaystyle\\sum_{i=1}^n X_i^2\\bigg],\\] \\[ v(r_{1}(x),\\mu) = \\exp\\bigg[\\dfrac{\\mu}{\\sigma^2}r_{1}(x) - \\dfrac{n\\mu^2}{2\\sigma^2}\\bigg]. \\] Por teorema de factorización, \\(r_{1}(x)=\\sum X_i\\) es un estadístico suficiente para \\(\\mu\\). Con \\(\\sigma^2\\) desconocido, \\(\\theta = (\\mu,\\sigma^2)\\), tome \\(u(x) = 1\\), \\[ v(r_1(x),r_2(x),\\theta) = (2\\pi\\sigma^2)^{-n/2}\\exp\\bigg[\\dfrac{-r_2(x)}{2\\sigma^2} + \\dfrac{\\mu r_1(x)}{\\sigma^2}- \\dfrac{n\\mu^2}{2\\sigma^2}\\bigg] \\] Entonces \\[ (r_1(x),r_2(x)) = \\left(\\sum{x_i},\\sum x_i ^2\\right) \\] es un estadístico suficiente para \\((\\mu, \\sigma^2)\\). Ejemplo. \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim}\\text{Unif}(0,\\theta)\\), \\(\\theta&gt;0\\), \\(f(x|\\theta) = 1_{[0,\\theta]}(x)\\dfrac 1\\theta\\). \\[f_n(x|\\theta) = \\prod_{i=1}^n 1_{[0,\\theta]}(x_i)\\left(\\dfrac 1\\theta \\right) \\] Nota: si al menos uno de los \\(x_i&lt;0\\) o \\(x_i&gt;\\theta\\), \\(u(x) = 0\\) \\((f(x|\\theta) = 0)\\) (Trivial). Si \\(0&lt;x_i&lt;\\theta\\) \\(\\forall i \\implies f_n(x|\\theta) = 1_{[0,\\theta]}(\\max\\{x_i\\})\\left(\\dfrac 1\\theta \\right)^n.\\) Si \\(T = r(x) = X_{(n)} \\implies f_n(x|\\theta) = u(x)v(r(x),\\theta)\\), \\(u(x) = 1\\). Por teorema de factorización, \\(r(x) = x_{(n)}\\) es un estadístico suficiente para \\(\\theta\\). 5.3 Estadístico suficiente multivariado. Si \\(\\theta \\in \\mathbb R^k\\), \\(k\\geq 1\\) se necesita al menos \\(k\\) estadísticos \\((T_1,\\dots,T_k)\\) para cada \\(i=1,\\dots,k\\), \\(T_i = r_i(X_1,\\dots, X_n)\\). Definición. Suponga que para cada \\(\\theta\\in \\Omega\\) y \\((t_1,\\dots, t_k) \\in \\mathbb R^k\\) valor del estadístico \\((T_1,\\dots,T_k)\\), la distribución condicional de \\(X_1,\\dots, X_n\\) dado \\((T_1,\\dots,T_k) = (t_1,\\dots, t_k)\\) no depende de \\(\\theta\\), entonces \\((T_1,\\dots,T_k)\\) es un estadístico suficiente para \\(\\theta\\). Criterio de factorización: \\[ f_n(x|\\theta) = u(x)v(r_1(x),\\dots,r_k(x),\\theta) \\Leftrightarrow T = (r_1(x),\\dots,r_k(x)) \\text{ es suficiente}\\] Si \\((T_1,\\dots,T_k)\\) es suficiente para \\(\\theta\\) y si \\((T_1&#39;,\\dots,T_k&#39;) = g(T_1,\\dots,T_k)\\) donde \\(g\\) es biyectiva, entonces \\((T_1&#39;,\\dots,T_k&#39;)\\) es suficiente para \\(\\theta\\). \\[ u(x)v(r(x)|\\theta) = u(x)v(g^{-1}(g(r(x))),\\theta).\\] Ejemplo. Considere los siguiente \\[\\begin{align*} T_1 &amp;= \\sum_{i=1}^{n} X_i \\\\ T_2 &amp;= \\sum_{i=1}^{n} X_i^2 \\\\ T_1&#39; &amp; = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\\\ T_2&#39; &amp;= \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X}_n) ^{2} \\end{align*}\\] Entonces defina la siguiente función \\[ (T_1&#39;,T_2&#39;) = g(T_1,T_2) = \\left(\\dfrac{1}{n}T_1,\\dfrac{1}{n}T_2 - \\dfrac{1}{n^2}T_1^2\\right). \\] De la primera entrada, \\[ T_1&#39; = \\dfrac 1n T_1 \\implies T_1 = nT_1&#39;.\\] De la segunda, \\[\\begin{align*} T_2&#39; = \\dfrac 1n T_2 - \\dfrac 1{n^2} &amp; = \\dfrac 1n \\sum X_i^2 - \\left(\\dfrac 1n \\sum X_i\\right)^2\\\\ &amp; = \\dfrac 1n \\sum X_i^2 - 2X_i\\bar X_n^2 + \\bar X_n \\\\ &amp; = \\dfrac 1n \\sum(X_i-\\bar X_n)^2 = \\hat\\sigma_n^2 \\end{align*}\\] Como \\(g\\) es biyectiva entonces \\((\\bar X_n, \\sigma_n^2)\\) es un estadístico suficiente para \\((\\mu,\\sigma^2)\\). Ejemplo. \\(X_1,\\dots, X_n \\sim \\text{Unif}(a,b)\\), \\(a&lt;b\\). Encuentre un estadístico suficiente. Si \\(x_i \\leq a\\) o \\(x_i&gt;b\\), tome \\(u(x) = 0\\). Si \\(a&lt; x_i &lt;b\\) \\(\\forall i\\), \\(x_i &gt; a\\) \\(\\forall i \\Leftrightarrow x_{(1)}&gt;a\\). \\(x_i &lt; b\\) \\(\\forall i \\Leftrightarrow x_{(n)}&lt;b\\). La verosimilitud es de la forma \\[f_n(x|(a,b)) = \\prod_{i=1}^n1_{[a,b]}(x_i) = \\underbrace{\\dfrac 1{(b-a)^n} 1_{\\{(z,w): z&gt;a, w&lt;b\\}}(X_{(1)},X_{(n)})}_{v(r_1(x),r_2(x),(a,b))}\\cdot \\underbrace{1}_{u(x)}\\] Por teorema de factorización \\((r_{1}(x), r_{2}(x)) = (X_{(1)},X_{(n)})\\) es un estadístico suficiente para \\((a,b)\\). 5.4 Estadísticos minimales Idea: un estadístico suficiente que garantice una partición de \\(\\mathcal X\\) (espacio muestral) de la manera más simple posible. Definición (Estadístico de orden). Sean \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} f\\). Al ordenar los datos \\[(Y_1,\\dots,Y_n) = (X_{(1)},\\dots,X_{(n)}) \\text { tal que } Y_1&lt;\\dots&lt;Y_n\\] Nota: \\((X_{(1)},\\dots,X_{(n)})\\) es un estadístico suficiente de \\(\\theta\\). Ejemplo. \\(X_1,\\dots, X_n \\sim \\text{Cauchy}(\\alpha)\\). \\[ f(x) = \\dfrac1\\pi[1+(x-\\alpha)^2]^{-1}, x\\in\\mathbb R\\] Busque un estimador suficiente para \\(\\alpha \\in \\mathbb R\\). \\[ f_n(x|\\alpha) = \\prod(x|\\alpha) = \\dfrac 1\\pi [1+(x_i-\\alpha)^2]^{-1} = \\underbrace{\\dfrac 1{\\pi^n}}_{u(x)}\\underbrace{\\prod_{i=1}^n[1+(x_i-\\alpha)^2]^{-1} }_{v(y,\\alpha)} \\] donde \\(y = (X_{(1)},\\dots,X_{(n)})\\) es suficiente para \\(\\alpha\\). Ejercicio: estime \\(\\alpha\\) usando R o usando método de momentos. Definición. Un estadístico \\(T\\) es suficiente minimal si \\(T\\) es suficiente y es función de cualquier otro estadístico suficiente. Teorema. Si \\(T = r(X_1,\\dots, X_n)\\) es un estadístico suficiente para \\(\\theta\\), entonces el MLE \\(\\hat\\theta\\) de \\(\\theta\\) depende de \\(X_1,\\dots, X_n\\) solamente a través de \\(T\\). Además, si \\(\\hat \\theta\\) es suficiente entonces \\(\\hat \\theta\\) es minimal. Prueba. Por teorema de factorización, \\(f_n(x|\\theta) = u(x)v(r(x),\\theta)\\) de \\(T =r(x)\\) es suficiente y \\[\\hat\\theta = \\operatorname*{argmax}_\\theta f_n(x|\\theta) = \\operatorname*{argmax}_\\theta v(r(x),\\theta), \\quad (\\Delta)\\] Como \\(\\hat\\theta = g(T)\\) para cualquier \\(T\\) estadístico suficiente, entonces \\(\\hat\\theta\\) es minimal. Teorema. Si \\(T = r(X_1,\\dots, X_n)\\) es un estadístico suficiente para \\(\\theta\\) entonces el estimador bayesiano (bajo una escogencia de \\(L\\)) depende de \\(X_1,\\dots, X_n\\) solamente a través de \\(T\\) (el estimador bayesiano es minimal). Prueba. Sustituya \\((\\Delta)\\) por \\(\\pi(\\theta|x) \\propto v(r(x),\\theta)\\cdot\\pi(\\theta)\\). Como cualquier estimador bayesiano depende de \\(\\pi(\\theta|x)\\), cualquier estimador bayesiano depende e los datos a través de \\(r(x)\\). 5.5 Mejorando estimadores Idea: ¿Será posible mejorar un estimar que no es suficiente? ¿Existirá otra medida de comparación entre estimadores? Considere una función de riesgo o pérdida \\[ R(\\theta,\\delta) = \\mathbb E[(\\delta(x)-\\theta) ^2]\\] Si \\(\\delta(x)\\) estima una característica de \\(F\\): \\[ R(\\theta,\\delta) = \\mathbb E[(\\delta(x)-h(\\theta))^2]\\quad (\\Delta\\Delta)\\] donde \\(h\\) es la característica. Nota: la función de riesgo puede ser calculada con una posterior \\(\\pi(\\theta|X)\\). Definición. Decimos que \\(\\delta\\) es inadmisible si \\(\\exists \\delta_0\\) (otro estimador) tal que \\(R(\\theta, \\delta_{0}) \\leq R(\\theta,\\delta)\\) \\(\\forall \\theta \\in \\Omega\\). deltadelta Decimos que \\(\\delta_0\\) “domina” a \\(\\delta\\) en el caso anterior. Decimos que \\(\\delta_0\\) es admisible si no existe otro estimador que domine a \\(\\delta_0\\). A \\((\\Delta \\Delta)\\) se le llama MSE o error cuadrático medio. Teorema (Rao-Blackwell). Sea \\(\\delta(X)\\) un estimador y \\(T\\) un estadístico suficiente para \\(\\theta\\) y sea \\(\\delta_0 = \\mathbb E[\\delta(X)|T]\\). Entonces \\[ R(\\theta,\\delta_0) \\leq R(\\theta,\\delta) \\; \\forall \\theta \\in \\Omega\\] Prueba. Por la desigualdad de Jensen, \\[ \\mathbb E_\\theta[(\\delta(x)-\\theta)^2] \\geq (E_\\theta[(\\delta(x)-\\theta)])^2. \\] También, \\[\\mathbb E[(\\delta(x)-\\theta)^2|T] \\geq (E[(\\delta(x)|T)]-\\theta)^2 = (\\delta_0(T)-\\theta)^2.\\] Entonces, \\[ \\mathbb E[(\\delta(x)-\\theta)^2] \\leq \\mathbb E[\\mathbb E[(\\delta(x)-\\theta)^2|T]] = \\mathbb E[(\\delta(x)-\\theta)^2] = R(\\theta,\\delta).\\] Nota. Si cambiamos a \\(R(\\theta,\\delta) = \\mathbb E[|\\delta(x)-\\theta|]\\) (error medio absoluto), el resultado anterior es cierto. Ejemplo. Sean \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} \\text{Poisson}(\\theta)\\) donde \\(\\theta\\) es la tasa de “visitas” de clientes por hora. Numericamente podemos hacer el ejemplo con \\(\\theta = 2\\) y una muestra de \\(n = 10000\\), X &lt;- rpois(n = 10000, lambda = 2) head(X, 20) ## [1] 3 5 2 4 1 1 3 3 3 2 0 2 3 2 4 1 2 6 1 3 hist(X) A partir de la verosimilitud, \\[f_n(X|\\theta) = \\dfrac{e^{-\\theta n} \\theta^{\\sum X_i}}{\\prod X_i!} \\] se tiene que \\(T=\\sum X_i\\) es un estadístico suficiente para \\(\\theta\\). Sea \\(Y_i = \\begin{cases} 1 &amp; \\text{si } X_i = 1\\\\ 0 &amp; \\text{si } X_i \\ne 1\\end{cases}\\). Esta \\(Y\\) se calcula de la forma Y &lt;- X == 1 head(Y, 10) ## [1] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE El objetivo es estimar \\(p\\) donde \\(p\\) es la probabilidad de que \\(X_i =1\\) (solo llegue un cliente por hora). Un estimador de \\(p\\) (MLE) es \\[\\delta(x) = \\dfrac{\\sum Y_i}{n}\\] (delta &lt;- mean(Y)) ## [1] 0.2618 ¿Es el óptimo? Calculamos \\[\\mathbb E[\\delta(x)|T] = \\dfrac 1n \\sum_{i=1}^n \\mathbb E (Y_i|T)\\] Vea que \\[\\begin{align*} \\mathbb E[Y_i|T = t] = \\mathbb P(X_i = 1 | T = t) &amp; = \\dfrac{\\mathbb P(X_i = 1, T=t)}{\\mathbb P(T=t)}\\\\ &amp; = \\dfrac{\\mathbb P(X_i = 1, \\sum_{j\\ne i} X_j = t-1)}{\\mathbb P(T=t)}\\\\ &amp; = \\dfrac{\\mathbb P(X_i = 1) \\mathbb P(\\sum_{j\\ne i} X_j = t-1)}{\\mathbb P(T=t)} = \\Delta \\end{align*}\\] \\(\\mathbb P(X_i = 1) = \\theta e^{-\\theta}\\) \\(\\mathbb P(\\sum_{j\\ne i}X_j = t-1) = e^{-(n-1)\\theta}\\dfrac{((n-1)\\theta)^{t-1}}{(t-1)!}\\) \\(\\mathbb P(T=t) = e^{-n\\theta}\\dfrac{(n\\theta)^t}{t!}\\) Entonces, \\[\\begin{align*} \\Delta = \\dfrac{\\theta e^{-n\\theta}\\dfrac{((n-1)\\theta)^{t-1}}{(t-1)!}}{e^{-n\\theta}\\dfrac{(n\\theta)^t}{t!}} = \\dfrac tn \\left(1-\\dfrac 1n\\right)^{t-1} = G\\left(\\dfrac tn\\right) \\end{align*}\\] es el estadístico con MSE mínimo. T &lt;- sum(X) n &lt;- length(X) (delta_0 &lt;- (T / n) * (1 - 1 / n)^(T - 1)) ## [1] 0.268275 En este caso \\(\\delta_0\\) es mejor que \\(\\delta\\) bajo una pérdida cuadrática. "],
["distribución-muestral-de-un-estadístico.html", "Capítulo 6 Distribución muestral de un estadístico 6.1 Distribución muestral 6.2 Distribución \\(\\chi^2\\) 6.3 Distribución \\(t\\)", " Capítulo 6 Distribución muestral de un estadístico 6.1 Distribución muestral Definición. Suponga que \\(X_1,\\dots,X_n\\) es una muestra con parámetro \\(\\theta\\) con parámetro \\(\\theta\\) (desconocido). Sea \\(T=r(X_1,\\dots,X_n,\\theta)\\). La distribución de \\(T\\) dado \\(\\theta\\) se llama distribución muestral. Ejemplo. Si \\(X_1,\\dots,X_n \\sim N(\\mu,\\sigma^2)\\). El MLE de \\(\\mu\\) es \\[\\begin{equation*} \\bar X_n =\\dfrac 1n \\sum_{i=1}^n X_i. \\end{equation*}\\] La distribución muestral del estadístico \\(\\bar X_n\\) es \\[ \\bar X_n \\sim N\\left(\\mu, \\dfrac{\\sigma^2}n \\right) \\] \\(\\mathbb E[\\bar X_n] = \\dfrac 1n\\displaystyle\\sum_{i=1}^n\\mathbb E[X_i] = \\dfrac 1n\\cdot n \\mathbb E[X_1] = \\mu\\). \\(\\text{Var}(\\bar X_n) = \\text{Var}\\left(\\dfrac 1n \\displaystyle\\sum_{i=1}^n X_i\\right) = \\dfrac{1}{n^2}\\cdot n\\cdot \\text{Var}(X_1) = \\dfrac{\\sigma^2}n\\). Ejemplo. \\(X_i:\\) tiempo de vida de un aparato. \\(X_1,\\dots,X_n \\stackrel{i.i.d}{\\sim} \\text{Exp}(\\theta)\\). La previa de \\(\\theta\\) es \\(\\Gamma(1,2)\\). Solamente observamos \\(n=3\\). La posterior sería \\[ \\theta|X \\sim \\Gamma(1+3,2+\\sum_{i=1}^3 X_i). \\] El estimador bayesiano, bajo pérdida cuadrática, es \\[ \\mathbb E[\\theta|X] = \\dfrac 4{2+\\sum X_i} = \\hat\\theta \\] Problema: estimar \\(\\mathbb P(|\\hat\\theta-\\theta|&lt;0.1)\\). Note que \\[\\begin{align*} \\mathbb P(|\\hat\\theta-\\theta|&lt;0.1) &amp;= \\mathbb E [1_{|\\hat\\theta-\\theta|&lt;0.1|\\theta)}] \\\\ &amp;= \\mathbb E[\\mathbb E [1_{|\\hat\\theta-\\theta|&lt;0.1|\\theta)}\\vert \\theta]] \\\\ &amp;= \\mathbb E[\\mathbb P(|\\hat\\theta-\\theta|&lt;0.1|\\theta)] \\end{align*}\\] Debemos definir primero cuál es la función de distribución de \\(\\hat{\\theta}\\). \\[\\begin{align*} F_{\\hat{\\theta}}(t|\\theta) = \\mathbb P(\\hat\\theta\\leq t|\\theta)&amp;= \\mathbb P\\left( \\dfrac 4{2+T}\\leq t\\bigg|\\theta\\right) \\\\ &amp; = \\mathbb P\\left( 2+T \\geq \\dfrac 4t\\bigg|\\theta\\right)\\\\ &amp; = \\mathbb P\\left( T \\geq \\dfrac 4t-2\\bigg|\\theta\\right) \\end{align*}\\] Nota: Recuerde que sumas de exponenciales es una gamma. (Ver teorema 5.7.7) Entonces \\(T=\\sum_{i=1}^{3}X_{i}\\sim \\Gamma(3,\\theta)\\), por lo que \\(F(t|\\theta) = 1-G_{\\Gamma(3,0)}\\left( \\dfrac 4t-2\\right)\\). Aqui denotamos como \\(G\\) a la distribución de \\(T\\). De esta manera, \\[\\begin{align*} \\mathbb P[|\\hat\\theta-\\theta|&lt;0.1|\\theta] &amp; = \\mathbb P [-0.1+\\theta &lt; \\hat\\theta &lt; 0.1 +\\theta|\\theta]\\\\ &amp; = G_{\\Gamma(3,\\theta)}\\left(\\dfrac 4{-0.1+\\theta} - 2\\right)-G_{\\Gamma(3,\\theta)}\\left(\\dfrac 4{0.1+\\theta} - 2\\right) \\end{align*}\\] y se toma la esperanza para estimar la esperanza. Este valor no se puede estimar de forma cerrada, sino que se podría aproximar mediante una simulación Otra solución es estimar \\(\\theta\\) usando el MLE \\(\\hat{\\theta} = \\frac{3}{T}\\). Se podría construir esa probabilidad de forma que no dependa de \\(\\theta\\). \\[ \\mathbb P \\left(\\bigg| \\underbrace{\\dfrac{\\hat\\theta_{MLE}}\\theta-1}_{\\text{Cambio relativo}} \\bigg| &lt; 0.1\\bigg|\\theta \\right) = \\mathbb P \\left( \\bigg| \\dfrac{3}{\\theta T}-1 \\bigg| &lt; 0.1 \\bigg| \\theta \\right) = \\Delta \\] Si \\(T\\sim\\Gamma(3,\\theta) \\implies \\theta T \\sim \\Gamma(3,1)\\). Por lo tanto, \\[ \\Delta = \\mathbb P \\left(0.9&lt;\\dfrac 3{\\theta T}&lt;1.1\\bigg|\\theta\\right) = \\mathbb P \\left(\\dfrac 3{1.1}&lt;\\theta T&lt;\\dfrac 3{0.9}\\right) = 13,4\\% \\] 6.2 Distribución \\(\\chi^2\\) Definición. Para \\(m&gt;0\\) definimos \\[ \\chi^2_m \\sim \\Gamma\\left(\\dfrac m2, \\dfrac 12 \\right) \\] la distribución chi-cuadrado con \\(m\\) grados de libertad. Propiedades: \\(\\mathbb E[X] = m\\). \\(\\text{Var} (X) = 2m\\). Para \\(X_i \\sim \\chi^2_{m_i}\\), \\(i = 1,\\dots, k\\), independientes, entonces \\[\\sum_{i=1}^k X_i \\sim \\chi^2_{\\sum m_i}\\] Si \\(X\\sim N(0,1) \\implies Y = X^2\\sim \\chi^2_1\\). Si \\(X_i \\stackrel{i.i.d}{\\sim} N(0,1) \\implies \\sum_{i=1}^m X_i^2 = \\chi^2_m\\). Ejemplo. Si \\(X_1,\\dots,X_n \\sim N(\\mu,\\sigma^2) \\implies Z = \\dfrac{X_i-\\mu}{\\sigma} \\sim N(0,1)\\) \\(\\forall i\\). Entonces \\[\\sum Z_i^2 \\sim \\chi^2_n \\implies \\sum \\dfrac{(X_i-\\mu)^2}{\\sigma^2}\\sim \\chi^2_n \\quad (\\*) \\] Además, si \\(\\mu\\) es conocido y \\(\\sigma^2\\) desconocido, entonces el MLE de \\(\\sigma ^{2}\\) es \\[\\hat\\sigma_0^2=\\dfrac{1}n \\sum_{i=1}^n(X_i-\\mu)^2\\] De esta manera, observe que, de \\((*)\\), \\[\\dfrac{n}{\\sigma^2} \\dfrac{1}n \\sum_{i=1}^n(X_i-\\mu)^2 = n\\dfrac{\\hat \\sigma_{0}^2}{\\sigma^2} \\sim \\chi^2_n \\] La principal limitación es que \\(\\mu\\) es conocida. Asuma que también es desconocida. ¿Cuál es la distribución muestral de \\((\\bar X_n,\\hat\\sigma^2)\\)? Teorema. Bajo las condiciones anteriores, \\(\\bar X_n\\) y \\(\\hat \\sigma_n\\) son independientes aunque \\(\\hat \\sigma_n\\) es función de \\(\\bar X_n\\). La distribución muestral de \\(\\bar X_n\\) es \\(N\\left(\\mu,\\dfrac{\\sigma^2}{n}\\right)\\). \\(n\\dfrac{\\hat \\sigma_{0}^2}{\\sigma^2} =\\sum_{i=1}^n \\dfrac{(X_i-\\mu)^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\). Nota: De álgebra lineal, recuerde que una matriz \\(A_{n\\times n}\\) es ortogonal si cumple que \\(A^{-1} = A\\), \\(\\det(A) = 1\\). Si \\(X, Y\\in \\mathbb R ^{n}\\), \\(AX =Y\\), \\(A\\) ortogonal, entonces \\[ \\|Y\\|_2^2 = \\|X\\|_2^2 \\quad (\\Delta\\Delta)\\] Teorema. Si \\(X_1,\\dots,X_n \\sim N(0,1)\\), \\(A\\) es ortogonal \\(n\\times n\\) y \\(Y=AX\\) donde \\(X = (X_1,\\dots,X_n)^T\\) entonces \\(Y_1,\\dots,Y_n \\sim N(0,1)\\). Prueba. Ver 8.3.1. Si \\(X_1,\\dots,X_n \\sim N(0,1)\\), use Gram-Schmidt con vector inicial \\[\\begin{equation*} u = \\left[ \\frac{1}{\\sqrt{n}}, \\cdots, \\frac{1}{\\sqrt{n}}\\right] \\end{equation*}\\] Generamos \\(A = \\begin{bmatrix}u\\\\\\vdots\\end{bmatrix}\\). Defina \\(Y =AX\\). Entonces \\[ Y_1 = uX = \\dfrac 1{\\sqrt{n}}\\sum_{i=1}^n X_i = \\sqrt{n} \\bar X_n.\\] Por la propiedad \\((\\Delta \\Delta)\\), \\(\\displaystyle\\sum_{i=1}^n Y_i^2 = \\displaystyle\\sum_{i=1}^n X_i^2\\). Entonces, \\[ \\sum_{i=2}^nY_i^2 = \\sum_{i=1}^nY_i^2 - Y_1^2 = \\sum_{i=1}^nX_i^2-n\\bar X_n^2\\sum_{i=1}^n(X_i-\\bar X_n)^2. \\] Como \\(Y_1^2\\) y \\(\\sum_{i=2}^nY_i^2\\) son independientes, entonces \\(\\bar X_n\\) y \\(\\dfrac{1}n \\sum_{i=1}^n(X_i-\\bar X_n)^2\\) son independientes. Note que \\(\\sum_{i=2}^n Y_i^2 \\sim \\chi^2_{n-1}\\) ya que \\(Y_i \\stackrel{i.i.d}{\\sim} N(0,1)\\). Si \\(X_1,\\dots,X_n \\sim N(\\mu, \\sigma^2)\\), tome \\(Z_i = \\dfrac{X_i-\\mu}\\sigma\\) y repita todo lo anterior. Ejemplo. \\(X_1,\\dots,X_n\\sim N(\\mu,\\sigma^2)\\) (\\(\\mu,\\sigma\\) desconocidos). Los MLE son \\[\\hat \\mu = \\bar X_n,\\quad \\hat\\sigma = \\bigg[\\dfrac{1}{n}\\sum_{i=1}^n(X_i-\\bar X_n)^2 \\bigg]^{\\frac 12}.\\] Encuentre \\(n\\) tal que \\[\\begin{equation*} p = \\mathbb P \\bigg[|\\hat\\mu-\\mu|&lt;\\dfrac {\\sigma}{5}, |\\hat\\sigma-\\sigma|&lt;\\dfrac \\sigma 5\\bigg] \\geq \\dfrac 12. \\end{equation*}\\] Por independencia de \\(\\bar X_n\\) y \\(\\hat\\sigma^2_n\\), \\[p= \\mathbb P \\bigg[|\\hat\\mu-\\mu|&lt;\\dfrac \\sigma5\\bigg] \\mathbb P \\bigg[|\\hat\\sigma-\\sigma|&lt;\\dfrac \\sigma5\\bigg]\\] Por un lado, \\[\\mathbb P \\bigg[|\\hat\\mu-\\mu|&lt;\\dfrac \\sigma5\\bigg] = \\mathbb P \\bigg[-\\dfrac{\\sqrt n}5\\leq \\underbrace{\\dfrac{\\sqrt{n}(\\hat\\mu-\\mu)}\\sigma}_{N(0,1)} &lt;\\dfrac {\\sqrt n}{5}\\bigg] = \\Phi\\left(\\dfrac{\\sqrt n}{5}\\right)-\\Phi\\left(-\\dfrac{\\sqrt n}{5}\\right).\\] Además, \\[\\begin{align*} \\mathbb P \\bigg[|\\hat\\sigma-\\sigma|&lt;\\dfrac \\sigma5\\bigg] =&amp;\\mathbb P \\bigg[-\\dfrac \\sigma 5 &lt; \\hat\\sigma-\\sigma&lt;\\dfrac \\sigma5\\bigg] \\\\ =&amp;\\mathbb P \\bigg[-\\dfrac{\\sigma}{5} +\\sigma &lt; \\hat\\sigma&lt;\\dfrac \\sigma5 +\\sigma\\bigg] \\\\ =&amp;\\mathbb P \\bigg[-\\dfrac 45 \\sigma &lt; \\hat\\sigma&lt;\\dfrac 65\\sigma\\bigg] \\\\ =&amp;\\mathbb P \\bigg[-\\dfrac 45 &lt; \\dfrac{\\hat\\sigma}{\\sigma}&lt;\\dfrac 65\\bigg] \\\\ =&amp;\\mathbb P \\bigg[\\left(-\\dfrac 45\\right)^2 &lt; \\dfrac{\\hat{\\sigma}^2}{\\sigma^2}&lt;\\left(\\dfrac 65\\right)^2\\bigg] \\\\ =&amp;\\mathbb P \\bigg[0.64n &lt; \\dfrac{\\hat{n\\sigma}^2}{\\sigma^2} &lt;1.44n\\bigg] \\\\ =&amp; F_{\\chi^2_{n-1}}(1.44n)-F_{\\chi^2_{n-1}}(0.64n). \\end{align*}\\] Estime \\(n\\) de manera que \\[\\bigg[1-2\\Phi\\left(-\\dfrac{\\sqrt n}{5}\\right)\\bigg][F_{\\chi^2_{n-1}}(1.44n)-F_{\\chi^2_{n-1}}(0.64n)] \\geq \\dfrac 12.\\] Se resuelve numéricamente, y si \\(n=21\\) se cumple. ggplot(data = data.frame(x = seq(0, 40, length.out = 1000)), aes(x)) + stat_function(fun = dchisq, args = list(df = 5), aes(color = &quot;05 grados de libertad&quot;)) + stat_function(fun = dchisq, args = list(df = 10), aes(color = &quot;10 grados de libertad&quot;)) + stat_function(fun = dchisq, args = list(df = 20), aes(color = &quot;20 grados de libertad&quot;)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_minimal() 6.3 Distribución \\(t\\) Definición. Sea \\(Y\\) y \\(Z\\) dos variables independientes tal que \\(Y\\sim \\chi^2_m\\) y \\(Z\\sim N(0,1)\\). Si \\[X := \\dfrac Z{\\sqrt{\\dfrac Ym}},\\] tiene una distribución \\(t\\) de Student con \\(m\\) grados de libertad. Tiene como densidad \\[f_X(x) = \\dfrac{\\Gamma\\left(\\dfrac{m+1}2\\right)}{\\sqrt{m\\pi}\\Gamma\\left(\\dfrac m2 \\right)}\\left(1+\\dfrac{x^2}m\\right)^{-\\frac{m+1}2}, \\quad x\\in \\mathbb R.\\] Propiedades: \\(f_X\\) es simétrica. La media de \\(X\\) no existe si \\(m\\leq 1\\). Si la media existe, es 0. Las colas de una \\(t\\) de Student son más pesadas que una \\(N(0,1)\\). Si \\(m\\) es entero, los primeros \\(m-1\\) momentos de \\(X\\) existen y no hay momentos de orden superior. Si \\(m&gt;2\\), \\(\\text{Var}\\left(X \\right)=\\dfrac m{m-2}\\). Si \\(m=1\\), \\(X\\sim \\text{Cauchy}\\). Ejercicio: \\(f_x(x)\\xrightarrow[m\\to \\infty]{}\\Phi(x)\\) (sirve como aproximación). La discrepancia de ambas está en la cola y se disipa cuando \\(m\\) es grande. Recuerde que, por el teorema 8.3.1, \\(\\bar X_n\\) y \\(Y=\\dfrac{n\\hat\\sigma^2}{\\sigma}\\) son independientes, con \\(\\bar X_n \\sim N\\left(\\mu, \\dfrac{\\sigma^2}{n}\\right)\\) y \\(Y\\sim \\chi^2_{n-1}\\). Además, \\[Z = \\sqrt n\\dfrac{\\bar X_n-\\mu}{\\sigma} \\sim N(0,1).\\] Sea \\[T = \\dfrac Z{\\sqrt{\\dfrac Y{n-1}}} = \\dfrac{\\sqrt n \\dfrac{\\bar X_n-\\mu}{\\sigma}} {\\sqrt{\\dfrac{\\dfrac{n\\hat\\sigma^2}{\\sigma^2}}{n-1}}} = \\dfrac{\\bar X_n-\\mu}{\\sqrt{\\dfrac{\\hat\\sigma}{n-1}}}\\] el cual no depende de \\(\\sigma\\). Teorema. Si \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} N(\\mu,\\sigma^2)\\), defina \\[\\sigma&#39; = \\bigg[\\dfrac 1{n-1}\\sum_{i=1}^n(X_i-\\bar X_n)^2\\bigg]^\\frac 12.\\] Entonces \\[\\dfrac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma&#39;} \\sim t_{n-1}\\] Nota. \\(\\sigma&#39; = \\left(\\dfrac n{n-1}\\right)^\\frac 12 \\hat\\sigma\\) (si \\(n\\) es grande, \\(\\sigma&#39; = \\hat\\sigma\\)). Prueba. Sean \\[S_n^2=\\sum_{i=1}^n(X_i-\\bar X_n)^2, \\quad Z = \\sqrt n \\dfrac{\\bar X_n-\\mu}{\\sigma}. \\] Dado que \\(Y = \\dfrac{S_n^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\), entonces \\[\\begin{align*} U = \\dfrac{Z}{\\sqrt{\\dfrac Y{n-1}}} &amp; = \\dfrac{\\dfrac{\\sqrt n}\\sigma (\\bar X_n-\\mu)}{\\sqrt{\\dfrac{S_n^2}{\\sigma^2(n-1)}}} \\\\ &amp; = \\dfrac{\\sqrt n (\\bar X_n-\\mu)}{\\sqrt{\\dfrac{S_n^2}{n-1}}}\\\\&amp; = \\dfrac{\\sqrt n (\\bar X_n-\\mu)}{\\sigma&#39;} \\sim t_{n-1}. \\end{align*}\\] ggplot(data = data.frame(x = seq(-5, 5, length.out = 1000)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;Normal(0,1)&quot;)) + stat_function(fun = dt, args = list(df = 1), aes(color = &quot; t con 01 grados de libertad&quot;)) + stat_function(fun = dt, args = list(df = 5), aes(color = &quot; t con 05 grados de libertad&quot;)) + stat_function(fun = dt, args = list(df = 10), aes(color = &quot; t con 10 grados de libertad&quot;)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_minimal() "],
["intervalos-de-confianza.html", "Capítulo 7 Intervalos de confianza 7.1 Intervalos de confianza para la media de una distribución normal 7.2 Caso normal. 7.3 Intervalos de confianza abiertos 7.4 Intervalos de confianza en otros casos", " Capítulo 7 Intervalos de confianza 7.1 Intervalos de confianza para la media de una distribución normal Dado \\(\\theta\\) un parámetro en \\(\\mathbb{R}\\) hemos estudiado procedimientos para encontrar estadísticos \\(T\\in \\mathbb R\\) para estimarlo. La limitación que tenemos acá es que no sabemos que tan aleatorio es \\(T\\). Entonces podemos sustituir este estadístico \\(T\\) con otros dos estadísticos \\(T_1\\) y \\(T_2\\) de modo que sepamos que \\[\\begin{equation*} T_1 \\leq \\theta \\leq T_2 \\end{equation*}\\] En caso que \\(\\theta \\in \\mathbb{R} ^{k}\\) se puede construir un conjunto de estadísticos \\(T_1, \\ldots, T_{k^\\prime}\\) con \\(k^\\prime = 2k\\) tal que \\[\\begin{equation*} \\theta \\in [T_1, T_2] \\times \\cdots \\times [T_{k^\\prime-1}, T_{k^\\prime}] \\end{equation*}\\] 7.2 Caso normal. En el caso normal, \\(\\bar X_n\\) es un estimador puntual de \\(\\mu\\). ¿Será posible encontrar un estimador por intervalos? Para efectos didácticos, vamos a empezar al revés de lo que usualmente se acostumbra. Defina \\(U = \\dfrac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma&#39;} \\sim t_{n-1}\\). Si \\(c&gt;0\\), \\[\\begin{align*} \\mathbb P[-c&lt;U&lt;c] &amp; = \\mathbb P \\bigg[ -c&lt;\\dfrac{\\sqrt{n}(\\bar X_n-\\mu)}{\\sigma&#39;} &lt;c\\bigg]\\\\ &amp; = \\mathbb P \\bigg[-\\dfrac{c\\sigma&#39;}{\\sqrt n} &lt; \\bar X_n - \\mu &lt;\\dfrac{c\\sigma&#39;}{\\sqrt n}\\bigg] \\\\ &amp; = \\mathbb P \\bigg[ \\bar X_n -\\dfrac{c\\sigma&#39;}{\\sqrt n} &lt; \\mu &lt; \\bar X_n + \\dfrac{c\\sigma&#39;}{\\sqrt n}\\bigg] \\end{align*}\\] El intervalo \\[\\begin{equation*} T = \\bigg[\\bar X_n - \\dfrac{c\\sigma&#39;}{\\sqrt n},\\bar X_n + \\dfrac{c\\sigma&#39;}{\\sqrt n}\\bigg] \\end{equation*}\\] es un intervalo aleatorio que “contiene” a \\(\\mu\\). Si queremos restringir la probabilidad anterior, tome \\(\\gamma \\in (0,1)\\): \\[ \\mathbb P(\\mu\\in T) = \\gamma. \\] Para que se cumpla lo anterior, seleccione \\(c\\) tal que \\[\\begin{align*} \\gamma = \\mathbb P( \\mu \\in T) &amp; = F_{t_{n-1}}(c)-F_{t_{n-1}}(-c) \\\\ &amp; = F_{t_{n-1}}(c) - [1-F_{t_{n-1}}(c)]\\\\ &amp; = 2F_{t_{n-1}} - 1 \\end{align*}\\] Entonces \\[ \\dfrac{\\gamma+1}2 = F_{t_{n-1}}(c) \\implies c = F_{t_{n-1}}^{-1}\\left(\\dfrac{\\gamma+1}2 \\right). \\] Definición. Si \\(X\\) es una variable aleatoria continua con distribución \\(F\\) (monótona creciente), entonces \\(x=F^{-1}(p)\\) es el cuantil de orden \\(p\\) de \\(F\\) (\\(p\\)-cuantil). El intervalo aleatorio \\[\\begin{equation*} \\bigg[\\bar X_n - F_{t_{n-1}}^{-1}\\left(\\dfrac{\\gamma+1}2 \\right)\\dfrac{\\sigma&#39;}{\\sqrt n},\\bar X_n + F_{t_{n-1}}^{-1}\\left(\\dfrac{\\gamma+1}2 \\right)\\dfrac{\\sigma&#39;}{\\sqrt n}\\bigg] \\end{equation*}\\] contiene a \\(\\mu\\) con probabilidad \\(\\gamma\\). *Definición. Sea \\(X = (X_1,\\dots,X_n)\\) una muestra con parámetro \\(\\theta\\). Sea \\(g(\\theta)\\) una característica de la distribución que genera la muestra. Sea \\(A &lt; B\\) dos estadísticos que cumplen (\\(\\forall \\theta\\)): \\[ \\mathbb P [A&lt;g(\\theta)&lt;B]\\geq \\gamma.\\quad (*) \\] Al intervalo \\((A,B)\\) le llamamos intervalo de confianza con coeficiente \\(\\gamma\\) para \\(g(\\theta)\\) (intervalo de confianza al \\(100\\gamma\\) para \\(g(\\theta)\\)). En el caso que \\((*)\\) tenga una igualdad, el intervalo es exacto. Nota. Si observamos \\(X\\), calculamos \\(A=a\\), \\(B=b\\). Entonces \\((a,b)\\) es el valor observado de un intervalo de confianza. Ejemplo. Se mide la lluvia con nubes inyectadas con “sulfato de plata” con \\(n=26\\) observaciones. Se desea hacer inferencia sobre \\(\\mu\\), la cantidad de lluvia media (escala logarítmica). Para \\(\\gamma = 0.95\\), se calcula \\[ c=F^{-1}_{t_{25}}\\left(\\dfrac{1+\\gamma}2\\right) =F^{-1}_{t_{25}}(0.975) = 2.060 \\] Note que \\(\\frac{1+\\gamma}{2}=\\) \\(0.975\\) y el segundo valor se obtiene de una tabla de valor de la \\(t\\)-student o de la expresión qt(p = 0.975, df = 26-1) = \\(2.06\\) El intervalo de confianza para \\(\\mu\\) al \\(95\\%\\) es \\[\\bar X_n \\pm \\underbrace{0.404}_{\\frac{2.060}{\\sqrt{26}}}\\sigma&#39;\\] Si \\(\\bar X_n = 5.134\\) y \\(\\sigma&#39; = 1.6\\) el valor observado del intervalo de confianza al \\(95\\%\\) para \\(\\mu\\) corresponde a \\[[5.134-0.404\\cdot1.6, 5.134+0.404\\cdot1.6]= [4.47,5.78]\\] Interpretación. El intervalo observado \\([4.48,5.78]\\) contiene a \\(\\mu\\) con un nivel de confianza del \\(95%\\). Usualmente a \\(\\dfrac{c\\sigma&#39;}{\\sqrt{n}}\\) se le llama margen de error (MOE). Interpretación gráfica. El proceso de construir un intervalo de confianza, quiere decir que si usted repitiera ese experimento muchas veces, el \\(100\\gamma\\%\\) (e.g, 95% o 99%) de la veces, el intervalo escogido tendría el parámetro real de la población \\(\\theta\\). Ejemplo interactivo sobre intervalos de confianza. Tomado de (R Psycologist)[https://rpsychologist.com/d3/ci/] 7.3 Intervalos de confianza abiertos Si \\(\\gamma\\) es el nivel de confianza dado, sea \\(\\gamma_1&lt;\\gamma_2\\) tal que \\(\\gamma_2 -\\gamma_1 = \\gamma\\). Sea \\(U = \\dfrac{\\sqrt n}{\\sigma&#39;}(\\bar X_n-\\mu)\\). Si \\[ A= \\bar X_n - T_{n-1}^{-1}(\\gamma_1)\\dfrac{\\sigma&#39;}{\\sqrt n} \\text{ y } B= \\bar X_n + T_{n-1}^{-1}(\\gamma_2)\\dfrac{\\sigma&#39;}{\\sqrt n}, \\] se cumple que \\((A,B)\\) es un intervalo de confianza al \\(100\\gamma\\) ya que \\[ \\mathbb P[\\mu \\in (A,B)] = \\mathbb P[T_{n-1}^{-1}(\\gamma_1)&lt;U&lt;T_{n-1}^{-1}(\\gamma_2)] = \\gamma_2-\\gamma_1 = \\gamma. \\] Definición (Intervalos de confianza abiertos). Bajo las condiciones anteriores, si \\(A\\) es un estadístico que satisface \\(\\forall \\theta\\): \\[\\mathbb P [A&lt;g(\\theta)]\\geq \\gamma,\\] A \\(A\\) se le llama límite inferior de confianza al \\(100\\gamma\\) y al intervalo \\((A,\\infty)\\) es el intervalo de confianza inferior al \\(100\\gamma\\). De forma análoga, si \\(B\\) satisface: \\[\\mathbb P [g(\\theta)&lt;B]\\geq \\gamma,\\] a \\((-\\infty,B)\\) se le llama intervalo de confianza superior para \\(g(\\theta)\\), con nivel \\(\\gamma\\). Si hay igualdad, el intervalo es exacto. Ejemplo. En el caso normal, encuentra \\(B\\) tal que \\(\\mathbb P(\\mu&lt;B) = \\gamma\\). Se sabe que \\[F_{t_{n-1}}(c) = \\mathbb P(U&gt;-c) = \\mathbb P \\left(\\dfrac{\\sqrt n(\\mu - \\bar X_n)}{\\sigma&#39;}&lt;c\\right).\\] Entonces \\[\\gamma = \\mathbb P\\left(\\mu &lt; \\bar X_n\\dfrac{\\sigma&#39;}{\\sqrt n}c\\right).\\] Tome \\(c\\) tal que \\[F_{t_{n-1}}(-c) = \\gamma \\implies c = -F_{t_{n-1}}(\\gamma)\\] Por lo tanto \\[B = \\bar X_n - \\dfrac{\\sigma&#39;}{\\sqrt{n}}F^{-1}_{t_{n-1}}(\\gamma).\\] 7.4 Intervalos de confianza en otros casos Ejemplo. Tiempos de vida, \\(n=3\\), \\(X_i\\sim \\text{Exp}(\\theta)\\). Si \\(T = \\sum_{i=1}^3X_i\\), \\(\\theta T\\sim \\Gamma(3,1)\\). Queremos calcular un intervalo de confianza superior para \\(\\theta\\) al \\(100\\delta\\) (exacto): \\(\\mathbb P[\\theta&lt;B] = \\gamma\\). Si \\(G\\) es la función de distribución de la gamma, sabemos que \\[\\begin{equation*} \\gamma = \\mathbb{P}[\\theta T&lt;G^{-1}(\\gamma)] = \\mathbb{P}\\bigg[\\theta&lt;\\dfrac{G^{-1}(\\gamma)}{T}\\bigg]. \\end{equation*}\\] El límite superior es \\(\\dfrac{G^{-1}(\\gamma)}{T}\\). theta &lt;- 2 X &lt;- rexp(3, rate = theta) T &lt;- sum(X) G_inv &lt;- qgamma(p = 0.95, shape = 3, rate = 1) Entonces el intervalo de confianza para este caso es c(0, G_inv / T) ## [1] 0.000000 2.574318 Definición. Sea \\(X = (X_1,\\dots,X_n)\\) una muestra de una distribución \\(F_\\theta\\). Sea \\(V(X,\\theta)\\) una variable aleatoria cuya distribución no depende de \\(\\theta\\). Decimos que \\(V\\) es una cantidad pivotal. Los intervalos de confianza se determinan a partir de un proceso de inversión de la cantidad pivotal. Encuentre \\(r(v,x)\\) tal que \\[r(V(X,\\theta)) = g(\\theta) \\quad (*)\\] y \\(g\\) es una función cualquiera. Del ejemplo anterior, \\(V(X,\\theta) = \\theta T\\), \\[r(V(X,\\theta),X) = \\dfrac{V(X,\\theta)}T = g(\\theta) = \\theta.\\] Teorema. Bajo las condiciones anteriores, si \\(V\\) existe sea \\(G\\) su c.d.f. y asume que \\(G\\) es continua. Asuma que \\(r\\) en es cierta y asuma que \\(r(v,x) \\nearrow\\) en \\(v\\) para cada \\(x\\). Sea \\(0&lt;\\gamma&lt;1\\) y \\(\\gamma_2&gt;\\gamma_1\\) tal que \\(\\gamma_2-\\gamma_1 = \\gamma\\). Entonces los extremos del intervalo de confianza para \\(g(\\theta)\\) al \\(100\\gamma\\) son \\[A=r(G^{-1}(\\gamma_1),X), \\quad B=r(G^{-1}(\\gamma_2),X).\\] Ejemplo. \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim} N(\\mu,\\sigma^2)\\). Encuentra A, B tales que \\(\\mathbb P[A&lt;\\sigma^2&lt;B] = \\gamma\\). Se sabe que \\[\\dfrac{n\\hat\\sigma^2}{\\sigma^2}\\sim \\chi^2_{n-1}.\\] Tome \\(V(X,\\sigma^2) = \\dfrac{n\\hat\\sigma^2}{\\sigma^2}\\). Entonces \\[\\gamma = \\mathbb P[\\chi^2_{n-1,\\gamma_1}&lt;V(X,\\sigma^2)&lt;\\chi^2_{n-1,\\gamma_2}]\\] donde \\(\\gamma = \\gamma_2-\\gamma_1\\). Tome \\[r(v,X) =\\dfrac{\\sum(X_i -\\bar X_n) ^2}{v} = \\dfrac{n\\hat\\sigma}{v}.\\] Invirtiendo el intervalo, \\[\\gamma = \\mathbb P \\bigg[ \\underbrace{\\dfrac{\\sum(X_i -\\bar X_n) ^2}{\\chi^2_{n-1,\\gamma_2}}} _{A} &lt;\\sigma^2&lt;\\underbrace{\\dfrac{\\sum(X_i -\\bar X_n) ^2}{\\chi^2_{n-1,\\gamma_1}}}_B\\bigg]\\] El IC para \\(\\sigma^2\\) al \\(100\\delta\\) es \\[ \\Bigg[ \\dfrac{\\sum(X_i -\\bar X_n) ^2}{\\chi^2_{n-1,\\gamma_2}}, \\dfrac{\\sum(X_i -\\bar X_n) ^2}{\\chi^2_{n-1,\\gamma_1}}\\Bigg].\\] Por ejemplo X &lt;- rnorm(n = 1000, 0, 2) gamma1 &lt;- 0.025 gamma2 &lt;- 0.975 gamma2 - gamma1 ## [1] 0.95 (chi2_gamma1 &lt;- qchisq(p = gamma1, df = 1000 - 1)) ## [1] 913.301 (chi2_gamma2 &lt;- qchisq(p = gamma2, df = 1000 - 1)) ## [1] 1088.487 (diferencias &lt;- sum((X - mean(X))^2)) ## [1] 4034.648 Finalmente el intervalo es c(diferencias / chi2_gamma2, diferencias / chi2_gamma1) ## [1] 3.706657 4.417654 NOTA: Las cantidades pivotales no siempre existen. Esto ocurre principalemente con las distribuciones discretas 7.4.1 Intervalos de confianza aproximados. Sean \\(X_1,\\dots, X_n \\stackrel{i.i.d}{\\sim}F_{\\mu}\\) donde \\(\\mathbb{E}[X_i] = \\mu\\) y \\(\\text{Var}(X_i) = \\sigma^2\\) (conocida). Note que \\[D = \\mathbb P[A&lt;\\mu&lt;B] = \\mathbb P \\bigg[-z_{\\frac{1+\\gamma}2}&lt;\\dfrac{\\sqrt n(\\bar X_n-\\mu)}{\\sigma} &lt;z_{\\frac{1+\\gamma}2}\\bigg] \\stackrel{TLC}{\\approx} \\gamma.\\] Así, \\[D \\underset{n\\to\\infty}{\\approx} \\Phi\\left(z_{\\frac{1+\\gamma}2}\\right)-\\Phi\\left(-z_{\\frac{1+\\gamma}2}\\right) = \\gamma.\\] Ejercicio. El intervalo de confianza correspondiente para \\(\\mu\\) es \\[ \\bar X_n \\pm z_{\\frac{1+\\gamma}2}\\dfrac{\\sigma}{\\sqrt n}. \\] Considere \\(U = \\dfrac{\\bar X_n - \\mu}{\\sigma&#39;/\\sqrt n}\\). \\(U\\) es pivotal, pero no necesariamente una \\(t_{n-1}\\). Considere que \\((\\sigma&#39;)^2 = \\dfrac{n}{n-1}\\hat \\sigma^2\\) y además \\(\\hat\\sigma^2\\) es el MLE de \\(\\sigma^2\\) y por lo tanto consistente. \\[\\begin{align*} \\hat{\\sigma}^2 &amp;\\xrightarrow[]{\\mathbb{P}}\\sigma^2 \\\\ ((\\sigma&#39;)^2 &amp;\\xrightarrow[]{\\mathbb{P}}\\sigma^2). \\end{align*}\\] Recuerde que si \\(X_n\\xrightarrow[]{d}Z\\) y \\(Y_n\\xrightarrow[]{\\mathbb{P}}a\\), entonces \\(X_nY_n \\xrightarrow[]{d}aZ\\). Por lo tanto, \\[ \\underbrace{\\dfrac{\\bar X_n-\\mu}{\\sigma/\\sqrt n}}_{\\xrightarrow[]{d} N(0,1)} \\cdot \\underbrace{\\dfrac{\\sigma/\\sqrt n}{\\sigma&#39;/\\sqrt n}}_{\\xrightarrow[]{\\mathbb P}1} \\xrightarrow[]{d}N(0,1)\\] Entonces \\(U\\xrightarrow[]{d}N(0,1)\\). Como consecuencia \\[ \\mathbb P \\bigg[-z_{\\frac{1+\\gamma}2}&lt;\\dfrac{\\bar X_n-\\mu}{\\sigma&#39;/\\sqrt n} &lt;z_{\\frac{1+\\gamma}2}\\bigg] \\stackrel{TLC}{\\approx} \\gamma. \\] y el IC aproximado para \\(\\mu\\) al \\(100\\gamma\\) \\[\\bar X_n \\pm z_{\\frac{1+\\gamma}2}\\dfrac{\\sigma&#39;}{\\sqrt n}.\\] Ejemplo. Si \\(X_1,\\dots, X_n\\sim \\text{Poi}(\\theta)\\), \\(\\mu =\\sigma^2 = \\theta\\). Por TLC, \\[\\sqrt n\\dfrac{\\bar X_n-\\theta}{\\sqrt{\\theta}}\\xrightarrow[]{d}N(0,1).\\] Entonces \\[ \\mathbb P[|\\bar X_n-\\theta|&lt;c] = \\mathbb P\\bigg[\\dfrac{\\sqrt n|\\bar X_n-\\theta|}{\\sqrt \\theta}&lt;\\dfrac{c\\sqrt n}{\\sqrt \\theta}\\bigg] \\approx 2\\Phi\\left(\\dfrac{c\\sqrt n}{\\sqrt \\theta}\\right)-1. \\] Explicación del fenómeno: En este caso recuerden que \\(\\bar{X}_n\\) es una variable aleatoria. Lo que dice el teorema del límite central es que conforme \\(n\\) es grande, la distribución de \\(\\bar{X}_n\\) (centrada y escalada apropiadamente) converge a una normal estándar. Xbar &lt;- data.frame(n = numeric(), Z = numeric()) idx &lt;- rep(x = c(10, 2000), times = 1000) for (k in 1:length(idx)) { muestra &lt;- rpois(n = idx[k], lambda = 5) Xbar[k, &quot;Z&quot;] &lt;- sqrt(idx[k]) * (mean(muestra) - 5) / sqrt(5) Xbar[k, &quot;n&quot;] &lt;- idx[k] } ggplot(Xbar) + geom_histogram(mapping = aes(x = Z, y = ..density..), color = &quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = &quot;red&quot;) + facet_wrap(. ~ n, scales = &quot;free&quot;) 7.4.2 Transformaciones estabilizadoras de la varianza [Ver página 365 del libro de texto. } ¿Cómo transformar \\(X_n\\) para que tenga varianza constante? Note que en el caso anterior se necesitó saber explícitamente el valor exacto de \\(\\theta\\) para hacer el ejercicio. Por el método Delta, la varianza “aproximada” de \\(\\alpha(\\bar X_n)\\) es \\[\\left( \\dfrac{\\alpha&#39;(\\mu)}{a_n}\\right)^2 =\\left( \\dfrac{\\alpha&#39;(\\mu)\\sigma}{\\sqrt n}\\right)^2 = \\dfrac{\\alpha&#39;(\\mu)^2\\sigma^2(\\mu)}{n}.\\] Si se desea que la varianza sea constante con respecto a \\(\\mu\\), \\[\\begin{align*} \\alpha&#39;(u)^2\\sigma^2(\\mu) &amp;= 1 \\\\ \\implies \\alpha&#39;(\\mu) &amp; = \\dfrac{1}{\\sigma(\\mu)} \\quad (\\sigma(\\mu)&gt;0)\\\\ \\implies \\alpha(\\mu) &amp;= \\int_{a}^{\\mu} \\dfrac{dx}{\\sigma(x)}dx \\end{align*}\\] donde \\(a\\) es una constante arbitraria que hace la integral finita (y fácil de calcular). Del ejemplo anterior (Poisson), recuerde que \\(\\sigma ^{2} = \\theta = \\mu\\), entonces se podría tomar que \\(\\sigma(\\mu) = \\sqrt{\\mu}\\) y por lo tanto definimos \\[ \\alpha(\\mu) = \\int_{0}^\\mu\\dfrac{dx}{\\sqrt x} = 2\\sqrt \\mu \\] Por el método Delta, \\[ 2\\bar X_n^{\\frac12} \\underset{n \\text{ grande}}{\\sim} N\\left(2\\theta^{\\frac 12},\\dfrac1n\\right) \\] De esta manera \\[\\mathbb P[|2\\bar X_n^{\\frac12}-2\\theta^{\\frac12}|&lt;c] =\\mathbb P\\Bigg[\\dfrac{|2\\bar X_n^{\\frac12}-2\\theta^{\\frac12}|}{\\sqrt{1/n}}&lt;\\sqrt nc\\Bigg] \\approx 2\\Phi(\\sqrt nc)-1 \\] Desarrollando, \\[\\mathbb P[-c+2\\bar X_n^{\\frac12}&lt;2\\theta^{\\frac 12}&lt;c+2\\bar X_n^{\\frac12}]\\approx 2\\Phi(\\sqrt nc)-1 \\] Se despeja \\(c\\) tal que \\[\\Phi(\\sqrt n c) = \\dfrac{1+\\gamma}2\\implies c = \\dfrac 1{\\sqrt n} z_{\\frac{1+\\gamma}2}.\\] El intervalo para \\(2\\theta^{\\frac 12}\\) es \\[\\bigg[2\\bar X_n^{\\frac 12} -\\dfrac 1{\\sqrt n} z_{\\frac{1+\\gamma}2},2\\bar X_n^{\\frac 12} +\\dfrac 1{\\sqrt n} z_{\\frac{1+\\gamma}2}\\bigg]\\] set.seed(42) X &lt;- rpois(n = 1000, lambda = 5) Xbar &lt;- mean(X) z &lt;- qnorm(p = 0.975) c(2 * sqrt(Xbar) - 1 / sqrt(1000) * z, 2 * sqrt(Xbar) + 1 / sqrt(1000) * z) ## [1] 4.371529 4.495488 Para estimar el IC para \\(\\theta\\), vea que si \\(y=2x^{\\frac12} \\implies x = \\dfrac{y^2}{4}\\). Aplicando esta transformación al intervalo anterior, se obtiene \\[\\bigg[\\dfrac 14 \\left(2\\bar X_n^{\\frac 12} -\\dfrac 1{\\sqrt n} z_{\\frac{1+\\gamma}2}\\right)^2,\\dfrac 14 \\left(2\\bar X_n^{\\frac 12} +\\dfrac 1{\\sqrt n} z_{\\frac{1+\\gamma}2}\\right)^2\\bigg].\\] c((1 / 4) * (2 * sqrt(Xbar) - 1 / sqrt(1000) * z)^2, (1 / 4) * (2 * sqrt(Xbar) + 1 / sqrt(1000) * z)^2) ## [1] 4.777567 5.052354 "],
["estimación-bayesiana-bajo-normalidad.html", "Capítulo 8 Estimación Bayesiana bajo normalidad 8.1 Precisión de una distribución normal 8.2 Distribución marginal de \\(\\mu\\) 8.3 Intervalos de credibilidad. 8.4 Efecto de previas no informativas (Opcional)", " Capítulo 8 Estimación Bayesiana bajo normalidad 8.1 Precisión de una distribución normal Definición. La precisión \\(\\tau\\) de una normal se define como \\(\\tau = \\dfrac 1{\\sigma^2}\\). Sean \\(X_1,\\dots,X_n\\sim N(\\mu,\\sigma^2) = N(\\mu,\\tau ^{-1})\\). Su densidad corresponde a \\[\\begin{align*} f(x|\\mu,\\sigma^2) &amp;= \\left(\\dfrac 1{2\\pi\\sigma^2}\\right)^{\\frac12}\\exp\\bigg[-\\dfrac1{2\\sigma^2}(x-\\mu)^2\\bigg] \\\\ &amp;= \\left(\\dfrac \\tau{2\\pi}\\right)^{\\frac12}\\exp\\bigg[-\\dfrac\\tau{2}(x-\\mu)^2\\bigg]=f(x|\\mu,\\tau). \\end{align*}\\] La verosimilitud es \\[f_n(x|\\mu,\\tau) = \\left(\\dfrac\\tau{2\\pi}\\right)^{\\frac n2}\\exp\\bigg[-\\dfrac\\tau2\\sum_{i=1}^n(x_i-\\mu)^2 \\bigg].\\] La previa de la densidad conjunta es \\([\\mu,\\tau]\\propto [\\mu|\\tau]\\cdot [\\tau]\\) y la posterior \\([\\mu,\\tau|x] \\propto [\\mu|\\tau,x]\\cdot[\\tau|x]\\). Las previas por seleccionar son \\([\\mu|\\tau]\\sim \\text{Normal}\\) y \\([\\tau]\\sim\\text{Gamma}\\). Recuerde: La distribución Gamma tiene forma \\[\\begin{equation*}f(x \\mid \\alpha, \\beta)=\\left\\{\\begin{array}{ll} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x} &amp; \\text { for } x&gt;0 \\\\ 0 &amp; \\text { for } x \\leq 0 \\end{array}\\right.\\end{equation*}\\] Y la verosimilitud es \\[\\begin{equation*} f_n(x \\mid \\alpha, \\beta)=\\left\\{\\begin{array}{ll} \\displaystyle \\frac{\\beta^{n\\alpha}}{\\Gamma(\\alpha)^n} \\left(\\prod_{i=1}^n x_i\\right) ^{\\alpha-1} e^{-\\beta \\sum_{i=1}^n x_i} &amp; \\text { for } x&gt;0 \\\\ 0 &amp; \\text { for } x \\leq 0 \\end{array}\\right.\\end{equation*}\\] Teorema. Si \\(X_1,\\dots,X_n \\overset{i.i.d}{\\sim} N(\\mu,\\tau ^{-1})\\), \\(\\mu \\in \\mathbb R\\), \\(\\tau&gt;0\\) (precisión) y \\(\\mu\\sim N(\\mu_0,\\lambda_0\\tau)\\), \\(\\mu\\in\\mathbb R\\), \\(\\lambda_0&gt;0\\), \\(\\tau\\sim\\Gamma(\\alpha_0,\\beta_0)\\), \\(\\alpha_0,\\beta_0&gt;0\\). Entonces \\[[\\mu,\\tau|x] \\propto [\\mu|\\tau,x]\\cdot[\\tau|x]\\] donde \\([\\mu|\\tau,x] \\sim N(\\mu_1,\\lambda_1\\tau)\\) con \\[\\lambda_1 = \\lambda_0+n, \\quad \\mu_1 = \\dfrac{\\lambda_0\\mu_0 + n\\bar x_n}{\\lambda_0+n},\\] y \\([\\tau] \\sim \\Gamma(\\alpha_1,\\beta_1)\\), \\[\\alpha_1 = \\alpha_0+\\dfrac n2, \\quad \\beta_1 = \\beta_0 + \\dfrac 12s_n^2 + \\dfrac{n\\lambda_0(\\bar X_n-\\mu_0)^2}{2(\\lambda_0+n)}.\\] Prueba. Previa: \\[\\begin{align*} [\\mu,\\tau] &amp; \\propto [\\mu|\\tau] \\cdot[\\tau] \\\\ &amp; = \\tau^{\\frac 12}\\exp\\bigg[-\\dfrac{\\lambda_0\\tau}2(\\mu-\\mu_0)\\cdot \\tau^{\\alpha_0-1}e^{-\\beta_0\\tau}\\bigg]\\\\ &amp; = \\tau^{\\alpha_0-\\frac 12}\\exp\\bigg[-\\dfrac{\\lambda_0\\tau}{2}(\\mu-\\mu_0)^2 - \\beta_0\\tau\\bigg] \\end{align*}\\] Por Bayes: \\[\\begin{align*} [\\mu,\\tau|x] &amp; \\propto [\\mu,\\tau]\\cdot[x|\\mu,\\tau]\\\\ &amp; \\propto [\\mu,\\tau]\\cdot\\tau^{\\frac 12} \\exp\\bigg[-\\dfrac\\tau 2\\sum_{i=1}^n(x_i-\\mu)^2\\bigg]\\\\ &amp; \\propto \\tau^{\\alpha_0+\\frac{n+1}2-1}\\exp\\bigg[-\\dfrac\\tau 2(\\lambda_0[\\mu-\\mu_0]^2 + \\sum(x_i-\\mu)^2-\\beta_0\\tau\\bigg] \\end{align*}\\] Además \\[\\sum_{i=1}^n (x_i-\\mu)^2 = \\sum_{i=1}^n (x_i-\\bar x_n + \\bar x_n -\\mu)^2 = s_n^2 + n(\\bar x_n-\\mu)^2.\\] Completando cuadrados (queda como ejercicio) se obtiene \\[n(\\bar x_n -\\mu)^2 + \\lambda_0(\\mu-\\mu_0)^2 = (\\lambda_0+n)(\\mu-\\mu_1)^2 + \\dfrac{n\\lambda_0(\\bar x_n-\\mu_0)}{\\lambda_0+n}.\\] Entonces \\[\\sum_{i=1}^{n}(x_i-\\mu)^2 +\\lambda_0(\\mu-\\mu_0)^2 = (\\underbrace{\\lambda_0+n}_{\\lambda_1})(\\mu-\\mu_1) + \\underbrace{s_n^2+\\dfrac{n\\lambda_0(\\bar x_n-\\mu_0)}{\\lambda_0+n}}_{\\beta_1}\\] Entonces \\[[\\mu,\\tau|x] \\propto \\underbrace{\\tau^{\\overbrace{\\alpha_0+\\frac n2 -1}^{\\alpha_1}}\\exp[-\\beta_1\\tau]}_{[\\tau|x]} \\cdot \\underbrace{\\tau^{\\frac 12} \\exp\\bigg[-\\dfrac{\\lambda_1\\tau}{2}(\\mu-\\mu_1)^2\\bigg]}_{[\\mu|\\tau,x]}\\] Por lo que \\([\\tau|x]\\sim \\Gamma(\\alpha_1,\\beta_1)\\) y \\([\\mu|\\tau,x] \\sim N(\\mu_1,\\lambda_1\\tau)\\). Definición Sean \\(\\mu,\\tau\\) dos variables aleatorias. Si \\(\\mu|\\tau \\sim N(\\mu_0,\\lambda_0\\tau)\\), \\(\\tau\\sim\\Gamma(\\alpha_0,\\beta_0)\\); decimos que \\[[\\mu, \\tau]\\sim \\text{Normal - Gamma}(\\mu_0,\\lambda_0,\\alpha_0,\\beta_0).\\] Conclusión: la Normal-Gamma conjuga con una verosimilitud normal. Limitación: \\(\\mu\\) y \\(\\tau\\) son independientes. Al combinar con la verosimilitud, cualquier tipo de independencia a nivel de previas se pierde. 8.2 Distribución marginal de \\(\\mu\\) Teorema. Suponga que \\([\\mu,\\tau]\\sim \\text{Normal-Gamma}(\\mu_0,\\lambda_0,\\alpha_0,\\beta_0)\\). Entonces \\[\\left(\\dfrac{\\lambda_0\\alpha_0}{\\beta}\\right)^{\\frac 12}(\\mu-\\mu_0)\\sim t_{2\\alpha_0}.\\] Prueba. Vea que \\(\\mu|\\tau \\sim N(\\mu_0,\\lambda_0\\tau)\\). Se despeja la desviación estándar, \\[\\lambda_0\\tau = \\dfrac 1{\\sigma^2} \\implies \\sigma = (\\lambda_0\\tau)^{-\\frac 12}.\\] Entonces \\[Z = \\dfrac{\\mu-\\mu_0}{\\sqrt{\\lambda_0\\tau}}\\Bigg|\\tau \\sim N(0,1).\\] La densidad conjunta de \\((Z,\\tau)\\) es \\[f(z,\\tau) = \\pi_2(\\tau)\\cdot\\pi_1(z|\\tau)\\] Si \\(g_1(\\mu|\\tau)\\) es la densidad de \\(\\mu|\\tau\\), por teorema de cambio de variable \\[\\begin{align*} f(z,\\tau) &amp; = \\pi_2(\\tau)\\cdot \\underbrace{g_1((\\lambda_0\\tau)^{-\\frac 12}z+\\mu_0|\\tau)(\\lambda_0\\tau)^{-\\frac 12}}_{\\phi(z)} = \\pi_2\\phi(z) \\end{align*}\\] Entonces \\(Z\\) y \\(\\tau\\) son independientes y \\(Z\\sim N(0,1)\\). Sea \\(Y = 2\\beta_0\\tau\\) y \\(\\tau\\sim \\Gamma(\\alpha_0,\\beta_0)\\), entonces \\[Y\\sim \\Gamma\\left(\\dfrac{2\\alpha_0}{2},\\dfrac12\\right) \\implies Y\\sim \\chi^2_{2\\alpha_0}\\] y \\(Y\\) es independiente de \\(Z\\). Por lo tanto, \\[U = \\dfrac Z{\\left( \\dfrac Y{2\\alpha_0}\\right)^{\\frac 12}}\\sim t_{2\\alpha_0}.\\] Observe que \\[U = \\dfrac{(\\lambda_0\\tau)^{\\frac 12}(\\mu-\\mu_0)}{\\left( \\dfrac {2\\beta_0\\tau}{2\\alpha_0}\\right)^{\\frac 12}} =\\left(\\dfrac{\\lambda_0\\alpha_0}{\\beta_0}\\right)^{\\frac 12}(\\mu-\\mu_0). \\] Consecuencia: \\[\\mu =\\left(\\dfrac{\\beta_0}{\\lambda_0\\alpha_0}\\right)^{\\frac 12} U+\\mu_0,\\quad U\\sim t_{2\\alpha_0}.\\] Propiedades: \\(\\mathbb E(\\mu) = \\mu_0 + 0 = \\mu_0\\). \\(\\text{Var}(\\mu) = \\dfrac{\\beta_0}{\\alpha_0\\lambda_0}\\cdot \\dfrac{\\alpha_0}{\\alpha_0-1} = \\dfrac{\\beta_0}{\\lambda_0(\\alpha_0-1)}\\). Ejemplo. Se hizo un experimento para determinar la relación del sabor del queso con respecto a su composición química. Vamos a cargar la base de datos que corresponde a este estudio. cheese &lt;- read.table(&quot;./data/cheese_data.txt&quot;, header = TRUE) head(cheese) ## Case taste Acetic H2S Lactic ## 1 1 12.3 4.543 3.135 0.86 ## 2 2 20.9 5.159 5.043 1.53 ## 3 3 39.0 5.366 5.438 1.57 ## 4 4 47.9 5.759 7.496 1.81 ## 5 5 5.6 4.663 3.807 0.99 ## 6 6 25.9 5.697 7.601 1.09 El químico más importante en este estudio es el ácido láctico (Lactic). hist(cheese$Lactic) Intervalo \\(t\\)-student Queremos construir un intervalo de confianza al 90% para la media de esta variable. De acuerdo al histograma podemos asumir que las oncentraciones de ácido en queso \\(X_1,\\dots, X_n\\sim N(\\mu,\\sigma^2)\\) Primero empecemos usando la técnica que aprendimos en el capítulo pasado y haremos un intervalo de confianza con cuantiles \\(t\\)-student. (Xbar &lt;- mean(cheese$Lactic)) ## [1] 1.442 (s &lt;- sd(cheese$Lactic)) ## [1] 0.30349 (s2 &lt;- var(cheese$Lactic)) ## [1] 0.09210621 (n &lt;- length(cheese$Lactic)) ## [1] 30 (gamma &lt;- 0.9) ## [1] 0.9 (level &lt;- (gamma + 1) / 2) ## [1] 0.95 (tquantile &lt;- qt(p = level, df = n - 1)) ## [1] 1.699127 c(Xbar - tquantile * s / sqrt(n), Xbar + tquantile * s / sqrt(n)) ## [1] 1.347852 1.536148 Intervalo Gamma-Normal Ahora supongamos que \\(X_1,\\dots, X_n\\) es normal con media \\(\\mu\\) y precisión \\(\\tau\\). Entonces \\[\\mu,\\tau \\sim \\text{Normal-Gamma}(\\mu_0 = 1, \\lambda_0 = 1,\\alpha_0 = 1/2, \\beta_0 = 1/2)\\] Los hiperparámetros \\(\\mu_0,\\lambda_0, \\alpha_0, \\beta_0\\) son escogidos de basados en la experiencia previa (que puede ser ninguna). mu_0 &lt;- lambda_0 &lt;- 1 alpha_0 &lt;- beta_0 &lt;- 1 / 2 Los datos de este experimento son \\(n = 30\\), \\(\\bar x_n = 1.442\\), \\(s_n^2 = 0.0921062\\). Aplicando las fórmulas del teorema anterior: (mu_1 &lt;- (lambda_0 * mu_0 + n * Xbar) / (lambda_0 + n)) ## [1] 1.427742 (lambda_1 &lt;- lambda_0 + n) ## [1] 31 (alpha_1 &lt;- alpha_0 + n / 2) ## [1] 15.5 (beta_1 &lt;- beta_0 + 0.5 * (n - 1) * s^2 + n * lambda_0 * (Xbar - mu_0) / (2 * (lambda_0 + n))) ## [1] 2.049411 \\(\\mu_1 = 1.4277419\\). \\(\\lambda_1 = 31\\). \\(\\alpha_1 = 15.5\\). \\(\\beta_1 = 2.049411\\) La posterior es \\[[\\mu, \\tau]\\sim \\text{Normal - Gamma}(\\mu_1,\\lambda_1,\\alpha_1,\\beta_1).\\] library(NormalGamma) previa &lt;- dnormgam(par = c(mu_0, sqrt(s2 / lambda_0), alpha_0, 1 / beta_0), plot = FALSE) posterior &lt;- dnormgam(par = c(mu_1, sqrt(s2 / lambda_1), alpha_1, 1 / beta_1), plot = FALSE) df &lt;- rbind(data.frame(distribucion = &quot;Previa&quot;, x = previa$xout, y = previa$dout), data.frame(distribucion = &quot;Posterior&quot;, x = posterior$xout, y = posterior$dout)) ggplot(df, aes(x, y, color = distribucion)) + geom_point() + theme_minimal() Podemos calcular inferencias sobre el \\(\\sigma\\) usando el hecho que \\(\\tau\\) es Gamma. \\[\\begin{align*} \\mathbb P[\\sigma&gt;0.3|x] &amp; = \\mathbb P\\bigg[\\sqrt{\\dfrac 1\\tau} &gt;0.3\\bigg|x\\bigg]\\\\ &amp; = \\mathbb P\\bigg[\\dfrac 1\\tau &gt;0.3^2\\bigg|x\\bigg]\\\\ &amp; = \\mathbb P\\bigg[\\tau &lt;\\dfrac 1{0.3^2}\\bigg|x\\bigg] \\\\ &amp; = \\mathbb P\\bigg[\\tau &lt;11.11\\bigg|x\\bigg] \\\\ &amp;=0.9554296 \\end{align*}\\] dado que \\([\\tau|x] \\sim \\Gamma(\\alpha_1,\\beta_1) = \\Gamma(15.5,2.049411)\\). En este caso observe que el cálculo es directo usando la función pgamma pgamma(q = 0.3^(-2), shape = alpha_1, rate = beta_1) ## [1] 0.9554296 Lo más importante es que basados en el teorema de las marginales podemos construir un intervalo de confianza para \\(\\mu\\). Note que si que tenemos la posterior, entonces sabemos que \\[U = \\left(\\dfrac{\\lambda_1\\alpha_1}{\\beta_1}\\right)^{\\frac 12}(\\mu-\\mu_1) \\sim t_{2\\alpha_1} \\] entonces \\[\\begin{equation*} \\mathbb P \\left(t_{2\\alpha_1, \\gamma_1} \\leq U \\leq t_{2\\alpha_1, \\gamma_2}\\right) = \\gamma \\end{equation*}\\] Solo basta despejar \\(U\\) con respecto a \\(\\mu\\). Como los cuantiles de la t son simétricos, la solución es \\[\\begin{equation*} \\mathbb P \\left(\\mu_1 - \\left(\\dfrac{\\beta_1}{\\lambda_1\\alpha_1}\\right)^{1/2} t_{2\\alpha_1, \\tfrac{\\gamma+1}{2}} \\leq U \\leq \\mu_1 + \\left(\\dfrac{\\beta_1}{\\lambda_1\\alpha_1}\\right)^{1/2} t_{2\\alpha_1, \\tfrac{\\gamma+1}{2}}\\right) \\end{equation*}\\] tquantile2alpha &lt;- qt(p = level, df = 2 * alpha_1) c( mu_1 - tquantile2alpha * (beta_1 / (lambda_1 * alpha_1))^(1 / 2), mu_1 + tquantile2alpha * (beta_1 / (lambda_1 * alpha_1))^(1 / 2) ) ## [1] 1.317011 1.538473 Noten que para este caso, encontramos un intervalo más pequeño que antes. Ejemplo. Suponga que \\(X_1,\\dots,X_{n}\\) son los días de hospitalización en 18 centros de salud. (ver ejemplo 8.6.3, pág 500). \\[[\\mu,\\tau]\\sim \\text{Normal-Gamma}(\\mu_0=200,\\lambda_0=2,\\alpha_0=2,\\beta_0=6300).\\] Encuentre un intervalo que contenga \\(\\mu_1\\) centrado en \\(\\mu_0\\) tal que la probabilidad que eso pase sea \\(0.95\\). \\[\\left(\\dfrac{\\alpha_0\\lambda_0}{\\beta_0}\\right)^{\\frac 12}(\\mu-\\mu_0) = 0.025(\\mu - 200)\\sim t_{2\\cdot2} = t_4.\\] Entonces \\[0.95 = \\mathbb P[l&lt;0.025(\\mu-200)&lt;u] = 2F_{t_4}(u)-1 \\implies u = t_{4,0.975} = 2.776.\\] Así, \\[\\mathbb P[-2.776&lt;0.025(\\mu-200)&lt;2.776]=0.95\\] y el intervalo es \\([89,311]\\). Con datos: \\(\\bar X_n = 182.17\\) y \\(s_n^2 = 88678.5\\). Los hiperparámetros posteriores son \\(\\mu_1 = 183.95\\), \\(\\lambda_1 = 20\\), \\(\\alpha_1 = 11\\), \\(\\beta_1 = 50925.37\\). Resolvemos el mismo problema: \\[\\left(\\dfrac{\\alpha_1\\lambda_1}{\\beta_1}\\right)^{\\frac 12}(\\mu-\\mu_0) = 0.0657(\\mu - 183.95)\\sim t_{2\\alpha_1=22}.\\] Se busca \\(u\\): \\[F_{t_{22}}(u|x) = \\dfrac{0.95+1}{2} \\implies u = t_{22,0.975}=2.074\\] y \\[0.95 = \\mathbb P[-2.074&lt;0.0657(\\mu-183.95)&lt;2.074|x].\\] El intervalo de credibilidad o predicción es \\([152.38,215.52]\\). Si \\(X_1,\\dots,X_{18}\\sim N(\\mu,\\sigma^2)\\), \\(\\mu,\\sigma^2\\) fijos y desconocidos. \\[\\bar X_n+t_{17,0.975}\\dfrac{\\sigma&#39;}{\\sqrt {18}} \\text{ al }95\\%.\\] El intervalo de confianza es \\([146.25,218.09]\\). Ejercicio Usando los datos de la variable InPatientDays de la siguiente base de datos. load(&quot;./data/Nursing.rda&quot;) head(Nursing$InPatientDays) ## [1] 128 155 281 291 238 180 Repita los cálculos númericos del ejercicio igual que el ejemplo pasado. 8.3 Intervalos de credibilidad. El intervalo de credibilidad de una distribución posterior se define como los valores \\(A\\) y \\(B\\) tal que \\[\\begin{equation*} \\mathbb P (A\\leq \\pi(\\theta\\vert x) \\leq B) = \\gamma \\end{equation*}\\] para algún \\(\\gamma&gt;0\\). Ejemplo: Supongamos que tenemos los tiempos de vida unos aparatos \\(X_1, X_2, X_3 \\sim \\mathrm{Exp}(\\theta)\\). La previa de \\(\\theta\\) es \\(\\Gamma(1,2)\\). Sabemos desde antes que \\[\\begin{equation*} \\theta \\vert X \\sim \\Gamma(4, 2+ \\sum_{i=1}^3 X_i). \\end{equation*}\\] Y la medía del estimador es \\[\\begin{equation*} \\mathbb E [\\theta\\vert X] = \\hat \\theta = \\dfrac{4}{2+ \\sum_{i=1}^3 X_i } \\end{equation*}\\] # Valores dados con paramétro desconocido X &lt;- rexp(n = 3, rate = 2) gamma &lt;- 0.95 level &lt;- (gamma + 1) / 2 alpha &lt;- 4 beta &lt;- 2 + sum(X) (theta_hat &lt;- alpha / beta) ## [1] 1.656052 A &lt;- qgamma(p = 0.025, shape = alpha, rate = beta) B &lt;- qgamma(p = 0.975, shape = alpha, rate = beta) c(A, B) ## [1] 0.4512184 3.6297644 ggplot(data = data.frame(x = c(0, 4)), aes(x)) + stat_function(fun = dgamma, args = list(shape = alpha, rate = beta)) + geom_vline(xintercept = A, color = &quot;red&quot;) + geom_vline(xintercept = B, color = &quot;red&quot;) + geom_vline(xintercept = theta_hat, color = &quot;blue&quot;) + theme_minimal() Ejercicio Para hacer este ejercicio sin usar bayes, se debe resolver usando una función estabilizadora de la varianza. Encuentre esa función y aplique el procedimiento que vimos el capítulo anterior. 8.4 Efecto de previas no informativas (Opcional) Considere una previa no informativa: \\([\\mu,\\tau] \\propto [\\mu]\\cdot[\\tau]\\) (supuesto de independencia), con \\([\\mu] \\propto 1\\), \\(\\tau = \\dfrac1{\\sigma^2}\\) y \\([\\sigma] \\propto \\dfrac{1}{\\sigma}\\). Dado que \\(\\sigma = (\\tau)^{-\\frac{1}2}\\), usando el teorema de cambio de variables, \\[\\dfrac{d\\sigma}{d\\tau} = -\\dfrac12\\tau^{-\\frac32} \\implies \\bigg|\\dfrac12\\tau^{-\\frac32}\\bigg|f_\\sigma\\left(\\dfrac 1{\\tau^{\\frac12}}\\right) = \\dfrac 12 \\tau^{-1}.\\] Entonces \\([\\mu,\\tau]\\propto\\tau^{-1}\\). Ejercicio. Verifique que \\([\\mu,\\tau]\\sim \\text{Normal-Gamma}(\\mu_0=0,\\lambda_0=0,\\alpha_0=-1/2,\\beta_0=0)\\). Usando Bayes, \\(X_1,\\dots,X_n \\sim N(\\mu, \\tau)\\). \\[\\begin{align*} \\pi(\\mu,\\tau|x) \\propto [\\mu,\\tau]\\cdot[x|\\mu, \\tau] \\\\ &amp; = \\tau^{-1} (2\\pi\\sigma^2)^{-n/2}\\exp\\bigg[-\\dfrac 1{2\\sigma^2}\\sum (X_i-\\mu)^2\\bigg]\\\\ &amp; \\propto \\tau^{-1} \\tau^{n/2} \\exp\\bigg[-\\dfrac \\tau 2 s_n^2 - \\dfrac{n\\tau}{2}(\\mu-\\bar X_n)^2\\bigg]\\\\ &amp; = \\tau^{1/2} \\exp\\bigg[-\\dfrac{n\\tau}2 (\\mu-\\bar X_n)^2\\bigg]\\cdot \\tau^{\\frac{n-1}{2}-1}\\exp\\bigg[-\\dfrac{s_n^2}{2}\\tau \\bigg] \\end{align*}\\] Entonces \\[\\mu|\\tau \\sim N(\\bar X_n,n\\tau)\\] \\[\\tau|x\\sim \\Gamma\\left(\\dfrac{n-1}2, \\dfrac{s_n^2}{2}\\right)\\]. Por lo tanto, \\[\\mu,\\tau|x \\sim \\text{Normal-Gamma}(\\mu_1 = \\bar X_n,\\lambda_1=n,\\alpha_1=(n-1)/2,\\beta_0=s_n^2/2).\\] Ejemplo. Tomando \\(\\bar X_n = 5.134\\), \\(s_n^2 = 63.96\\) con una previa no informativa para \\(\\mu,\\tau\\). Entonces la posterior es Normal-Gamma con hiperparámetros: \\(\\mu_1 = 5.134\\), \\(\\lambda_1 = 26\\), \\(\\alpha = \\dfrac{25}2\\), \\(\\beta_1 = 31.98\\). Queremos hacer inferencia sobre \\(\\mu\\): \\[\\begin{align*} 0.95 &amp; = \\mathbb P[-t_{25,0.975}&lt;U&lt;t_{25,0.975}]\\\\ &amp; = \\mathbb P\\bigg[-t_{25,0.975}&lt;\\left(\\dfrac{26\\cdot 12.5}{31.98}\\right)^{\\frac 12}(\\mu-5.134) &lt;t_{25,0.975}\\bigg] \\end{align*}\\] El intervalo es \\([4.488,5.78]\\). Calculemos \\(\\mathbb P[\\mu&gt;4|x]\\). Sea \\(w =\\left(\\dfrac{\\alpha_1\\lambda_1}{\\beta_1}\\right)^{\\frac 12} = 3.188\\). \\[ \\mathbb P[\\mu&gt;4|x] = P[w(\\mu-\\bar X_n)&gt;w(4-\\bar X_n)]=1-T_{t_{25}}(-3.615) = 0.9993. \\] Generalizando: \\[ w = \\left(\\dfrac{n(n-1)/2}{s_n^2/2}\\right)^{\\frac 12} = \\left(\\dfrac{n(n-1)}{s_n^2}\\right)^{\\frac 12} = \\left(\\dfrac{n}{(\\sigma&#39;)^2}\\right)^{\\frac 12}. \\] Entonces \\[\\begin{align*} \\gamma &amp; = \\mathbb P\\bigg[-t_{n-1,\\frac{1+\\gamma}{2}} &lt; \\left(\\dfrac{n}{(\\sigma&#39;)^2}\\right)^{\\frac 12}(\\mu-\\bar X_n)&lt;t_{n-1,\\frac{1+\\gamma}{2}}\\bigg] \\\\ &amp; = \\mathbb P\\bigg[\\bar X_n-t_{n-1,\\frac{1+\\gamma}{2}} \\dfrac{\\sigma&#39;}{\\sqrt n} &lt; \\mu &lt; \\bar X_n+t_{n-1,\\frac{1+\\gamma}{2}} \\dfrac{\\sigma&#39;}{\\sqrt n} \\bigg]. \\end{align*}\\] "],
["estimación-insesgada.html", "Capítulo 9 Estimación insesgada 9.1 Estimadores insesgados 9.2 Estimador insesgado de la varianza 9.3 Información de Fisher 9.4 Desigualdad de Cramer-Rao 9.5 Estimadores eficientes 9.6 Comportamiento asintótico del MLE", " Capítulo 9 Estimación insesgada 9.1 Estimadores insesgados Definición. Un estimador \\(\\delta(x)\\) es un estimador insesgado de \\(g(\\theta)\\) si \\(\\mathbb E_{\\theta}[\\delta(x)] = g(\\theta)\\), \\(\\forall \\theta\\). A \\(\\mathbb E_{\\theta}[\\delta(x)] - g(\\theta)\\) se le denomina sesgo. Ejemplo. Si \\(X_1,\\dots, X_n \\overset{i.i.d}{\\sim} F_\\theta\\), \\(\\mu = \\mathbb E[X_1]\\), entonces \\[\\mathbb E[\\bar X_n] = \\dfrac 1n \\sum_{i=1}^n\\mathbb E(X_i) = \\mu\\] \\(\\bar X_n\\) es estimador insesgado de \\(\\mu\\). Ejemplo. \\(X_1,X_2,X_3 \\overset{i.i.d}{\\sim} \\text{Exp}(\\theta)\\). El MLE de \\(\\theta\\) es \\[\\hat\\theta = \\dfrac 3T = \\dfrac 3{\\sum_{i=1}^{3}X_i}\\] ¿Será \\(\\hat\\theta\\) un estimador insesgado? theta_real &lt;- 5 X &lt;- matrix(rexp(n = 1000 * 3, rate = theta_real), ncol = 3) T &lt;- apply(X = X, MARGIN = 1, FUN = sum) theta_hat &lt;- 3 / T hist(theta_hat - theta_real, breaks = 100) Teoricamente podemos ver que \\[\\mathbb E[\\hat\\theta] = \\mathbb E\\bigg[\\dfrac 3T\\bigg]= 3\\mathbb E\\bigg[\\dfrac 1T\\bigg], \\quad T\\sim \\Gamma(3,\\theta)\\] Como \\(\\dfrac 1T \\sim \\text{Gamma Inversa}(3,\\theta)\\)1, se tiene que \\[\\mathbb E\\bigg[\\dfrac 1T\\bigg] = \\dfrac{\\theta}2 \\implies \\mathbb E[\\hat \\theta] =\\dfrac{3\\theta}2 \\neq \\theta\\] Por lo que \\(\\hat \\theta\\) es un estimador sesgado, con sesgo \\[\\text{sesgo}(\\hat\\theta) = \\dfrac{3\\theta}{2} -\\theta = \\dfrac \\theta 2.\\] Si por ejemplo \\(\\theta=5\\), entonces la diferencia debería ser aproximadamente \\(\\dfrac 5 2\\). mean(theta_hat - theta_real) ## [1] 2.515532 Si \\(U = \\dfrac {2\\hat\\theta}{3} = \\dfrac 23 \\cdot \\dfrac{3}{T} = \\dfrac 2T\\), \\[\\mathbb E[U] = \\dfrac 23 \\mathbb E(\\hat\\theta) =\\dfrac 23 \\cdot \\dfrac 32 \\theta.\\] Entonces \\(U\\) es un estimador insesgado. U &lt;- 2 / T mean(U - theta_real) ## [1] 0.01035435 Importante: El caso ideal es encontrar estimadores en donde \\(\\text{Var}(\\delta(x))\\to 0\\) y además que sean insesgados. ¿Cómo controlar sesgo y varianza? Defina la siguiente cantidad \\[\\begin{align*} \\text{Sesgo}^2(\\delta(x))+\\text{Var}(\\delta(x)) &amp; = (\\mathbb E_\\theta[\\delta(x)]-\\theta)^2 + \\mathbb E[[\\delta(x)-\\mathbb E[\\delta(x)]]^2]\\\\ &amp; =\\mathbb E[ \\underbrace{(\\mathbb E_\\theta[\\delta(x)]-\\theta)^2}_{A^2} + \\underbrace{[\\delta(x)-\\mathbb E[\\delta(x)]]^2}_{B^2}]\\\\ &amp; = \\mathbb E[A^2+B^2 - 2(\\underset{=0}{\\mathbb E[\\delta(x)]-\\theta)(\\delta(x)-\\mathbb E[\\delta(x)]})]\\\\ &amp; = \\mathbb E[(\\mathbb E[\\delta(x)]-\\theta - \\mathbb E[\\delta(x)] + \\delta(x))^2]\\\\ &amp; = \\mathbb E[(\\delta(x)-\\theta)^2] = MSE(\\delta(x)) * [ ] \\end{align*}\\] Si \\(\\delta\\) tiene varianza finita, entonces definimos el error cuadrático medio (MSE) de \\(\\delta(x)\\) como, \\[MSE_{\\theta}(\\delta(x)) =\\text{Sesgo}^2(\\delta(x)) + \\text{Var}(\\delta(x)).\\] Ejemplo. Comparar \\(\\hat\\theta\\) y \\(U =\\dfrac 2T\\) en términos del MSE. Dado que \\(\\text{Var}\\left(\\dfrac 1T\\right) = \\dfrac{\\theta^2}4\\),2 se tiene \\(\\mathrm{MSE}(U) = \\text{Var}\\left(\\dfrac 2T\\right) = 4\\dfrac{\\theta^2}4 = \\theta^2\\). var(U) + mean(U - theta_real)^2 ## [1] 21.90015 \\(\\mathrm{MSE}(\\hat\\theta) = (\\text{Sesgo}(\\hat\\theta))^2+\\text{Var}\\left(\\dfrac 3T\\right) = \\dfrac{\\theta^2}{4}+\\dfrac{9\\theta^2}{4} = \\dfrac{5\\theta^2}{2}\\). var(theta_hat) + mean(theta_hat - theta_real)^2 ## [1] 55.603 \\(U\\) es mejor estimador en términos de MSE que el \\(\\hat\\theta\\). OJO: El estimado bayesiano es \\(\\theta_{Bayes} = \\dfrac{4}{2+T}\\) y este es un poco más eficiente que los otros dos. theta_bayes &lt;- 4 / (2 + T) var(theta_bayes) + mean(theta_bayes - theta_real)^2 ## [1] 11.83203 9.2 Estimador insesgado de la varianza Teorema. Si \\(X_1,\\dots, X_n \\sim F_{\\theta}\\) con varianza finita y \\(g(\\theta) = \\text{Var}(X_1)\\) entonces \\[\\hat\\sigma_1^2 = \\dfrac{1}{n-1}\\sum(X_i-\\bar X_n)^2\\] es un estimador insesgado de \\(\\sigma^2\\). Prueba. Considere que \\[\\begin{equation} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}=\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}+n\\left(\\bar{X}_{n}-\\mu\\right)^{2} \\end{equation}\\] Entonces si \\(\\sigma_0 ^{2} = \\dfrac 1n \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}\\) \\[\\mathbb E[\\hat\\sigma_0^2] = \\mathbb E \\bigg[ \\dfrac {\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}}n \\bigg] = \\mathbb E \\bigg[ \\dfrac 1n \\sum(X_i-\\mu)^2\\bigg] - \\mathbb E[(\\bar X_n-\\mu)^2] = \\sigma^2-\\dfrac{\\sigma^2}n = \\left(\\dfrac{n-1}n\\right)\\sigma^2.\\] Para que \\(\\hat\\sigma_0^2\\) sea insesgado, \\[\\mathbb E \\bigg[\\dfrac n{n-1}\\hat\\sigma_0^2\\bigg] = \\mathbb E[\\hat\\sigma_1] = \\sigma^2.\\] Entonces \\(\\hat\\sigma_1\\) es estimador insesgado de \\(\\sigma^2\\). Ejemplo. Sean \\(X_1,\\dots,X_n \\overset{i.i.d}{\\sim}\\text{Poi}(\\theta)\\). \\(\\mathbb E(X_i) = \\text{Var}(X_i) = \\theta\\). Estimadores insesgados de \\(\\theta\\) son: \\(\\bar X_n\\). \\(\\hat \\sigma_1^2\\). Si \\(\\alpha \\in (0,1)\\), \\(T = \\alpha\\bar X_n + (1-\\alpha)\\hat\\sigma_1^2\\) también es un estimador insesgado. X &lt;- matrix(rpois(n = 1000 * 100, lambda = 2), nrow = 100) m &lt;- apply(X, 1, mean) v &lt;- apply(X, 1, var) a &lt;- apply(X, 1, function(x, alpha) { alpha * mean(x) + (1 - alpha) * var(x) }, alpha = 10) hist(m) hist(v) hist(a) Ejemplo. (Normal) ¿Cuál estimador tiene menor MSE, \\(\\hat\\sigma^2_0\\) o \\(\\hat\\sigma^2_1\\)? Defina \\(T_c = c\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}\\). Si \\(c = 1/n\\), \\(T_c = \\hat\\sigma_0\\) y si \\(c = 1/(n-1)\\), \\(T_c = \\hat\\sigma_1\\). De esta manera, \\[MSE_{\\sigma^2}(T_c) = \\mathbb E[(T_c-\\sigma^2)^2] =(\\mathbb E(T_c)-\\sigma^2)^2+\\text{Var}(T_c).\\] \\[\\begin{align*} \\mathbb E[T_c] &amp;= c\\mathbb E[\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}] \\\\ &amp;= c(n-1)\\mathbb E\\bigg[\\dfrac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}}{n-1}\\bigg] \\\\ &amp;= c(n-1)\\sigma^2. \\end{align*}\\] \\[\\begin{align*} \\text{Var}(T_c) &amp;= c^2\\text{Var}(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}) \\\\ &amp;= c^2\\text{Var}\\Bigg(\\sigma^2\\underbrace{\\sum\\dfrac{(X_i-\\bar X_n)}{\\sigma^2}}_{\\sim\\chi^2_{n-1}}\\Bigg) \\\\ &amp;= 2c^2\\sigma^4(n-1). \\end{align*}\\] Entonces \\[\\mathrm{MSE}_{\\sigma^2}(T_c) = [c(n-1)\\sigma^2-\\sigma^2]^2+2c^2\\sigma^4(n-1) = [[c(n-1)-1]^2+2c^2(n-1)]\\sigma^4.\\] Optimizando, \\[\\min_c \\mathrm{MSE}(T_c) = \\min_c[(n^2-1)c^2-2(n-1)c+1],\\] se encuentra que \\(\\hat c = \\dfrac 1{n+1}\\). Así, \\(T_{\\frac{1}{n+1}} = \\dfrac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}}{n+1}\\) es el mejor estimador de \\(\\sigma^2\\) en el sentido de MSE. Aunque se puede demostrar que este estimador es inadmisible. Ejercicio. Calcule el MSE de \\(\\hat\\sigma_0^2\\) y \\(\\hat\\sigma_1^2\\) y compare los resultados. 9.3 Información de Fisher ¿Cómo cuantificar la información de un estadístico? Sea \\(X\\sim f(x|\\theta)\\), \\(\\theta \\in \\Omega \\subset \\mathbb R\\) parámetro fijo. Supuesto 1: para cada \\(x \\in \\mathcal X\\) (espacio muestral de \\(X\\)) \\(f(x|\\theta)&gt; 0\\) \\(\\forall \\theta \\in \\Omega\\). Esto quiere decir que la imagen de la variable aleatoria no puede depender de \\(\\theta\\). Supuesto 2: \\(f(x|\\theta)\\) es dos veces diferenciable. Supuesto 3: \\(\\dfrac d{d\\theta}\\int_{\\mathcal X}f(x|\\theta)dx = \\int_{\\mathcal X}\\dfrac d{d\\theta}f(x|\\theta)dx\\). Ejemplo. \\(\\text{Unif}[0,\\theta]\\), \\(f(x|\\theta) = 1_{(0,\\theta)}(x)\\). No aplica el supuesto, ya que si \\(x&gt;\\theta\\), \\(f(x|\\theta) = 0\\). En otras palabras el dominio de la distribución no debe depender de \\(\\theta\\). Definición. Se define la función Score: \\[\\lambda(x|\\theta)=\\ln f(x|\\theta)\\] cuyas derivadas son \\[\\lambda&#39;(x|\\theta) = \\dfrac \\partial{\\partial \\theta}\\ln f(x|\\theta)\\] \\[\\lambda&#39;&#39;(x|\\theta) = \\dfrac {\\partial^2}{\\partial \\theta^2}\\ln f(x|\\theta)\\] Definición. Si \\(X\\) y \\(f(x|\\theta)\\) satisfacen los supuestos anteriores, la información de Fisher (\\(I(\\theta)\\)) de \\(X\\) es \\[I(\\theta) =\\mathbb E[(\\lambda&#39;(x|\\theta))^2]\\] donde la esperanza es integral o suma, dependiendo de \\(X\\). Por ejemplo si \\(f(x\\vert\\theta)\\) \\[\\begin{equation} I(\\theta)=\\int_{\\mathcal{X}}\\left[\\lambda^{\\prime}(x \\mid \\theta)\\right]^{2} f(x \\mid \\theta) d x \\end{equation}\\] Teorema. Bajo las condiciones anteriores, y suponiendo que las dos derivadas de \\(\\int_{\\mathcal X}f(x|\\theta)dx\\) con respecto a \\(\\theta\\) (Supuesto 3) se pueden calcular al intercambiar el orden de integración y derivación. Entonces \\[I(\\theta) = -\\mathbb E_{\\theta}[\\lambda&#39;&#39;(x|\\theta)] = \\text{Var}[\\lambda&#39;(x|\\theta)]\\]. Prueba: \\[\\begin{align*} \\mathbb E[\\lambda&#39;(x|\\theta)] &amp; = \\int_{\\mathcal X}\\lambda&#39;(x|\\theta)f(x|\\theta)dx \\\\ &amp; = \\int_{\\mathcal X} \\dfrac{f&#39;(x|\\theta)}{f(x|\\theta)}f(x|\\theta)dx \\\\ &amp; = \\int_{\\mathcal X}f&#39;(x|\\theta)dx \\\\ &amp; = \\dfrac d{d\\theta}\\int_{\\mathcal X}f(x|\\theta)dx \\quad \\text{(por supuesto 3.)} \\\\ &amp; = \\dfrac d{d\\theta}1 = 0 \\end{align*}\\] En consecuencia, \\[\\text{Var}(\\lambda&#39;(x|\\theta)) = \\mathbb E[(\\lambda&#39;(x|\\theta))^2]-0 = I(\\theta).\\] Además, \\[\\lambda&#39;&#39;(x|\\theta)= \\left(\\dfrac{f&#39;(x|\\theta)}{f(x|\\theta)}\\right)&#39; = \\dfrac{f(x|\\theta)f&#39;&#39;(x|\\theta)-f&#39;(x|\\theta)^2}{f^2(x|\\theta)} =\\dfrac{f&#39;&#39;(x|\\theta)}{f(x|\\theta)} - (\\lambda&#39;(x|\\theta))^2 \\] Note que (por los supuestos 2 y 3), \\[\\begin{align*} \\mathbb E\\bigg[\\dfrac{f&#39;&#39;(x|\\theta)}{f(x|\\theta)} \\bigg] &amp; = \\int_{\\mathcal X}\\dfrac{f&#39;&#39;(x|\\theta)}{f(x|\\theta)} f(x|\\theta)dx \\\\ &amp;=\\dfrac{d}{d\\theta}\\bigg[\\dfrac{d}{d\\theta}\\int_{\\mathcal X}f(x|\\theta)dx\\bigg]\\\\ &amp; = \\dfrac{d}{d\\theta}\\bigg[\\dfrac{d}{d\\theta}1\\bigg] = 0 \\end{align*}\\] Entonces, \\[\\mathbb E[\\lambda&#39;&#39;(x|\\theta)] =\\mathbb E\\bigg[\\dfrac{f&#39;&#39;(x|\\theta)}{f(x|\\theta)} \\bigg] - \\mathbb E[(\\lambda&#39;(x|\\theta))^2] = -I(\\theta). \\] Se concluye, además, que \\(\\lambda&#39;(x|\\theta)\\) es centrada y su varianza es \\(I(\\theta)\\). RESULTADO IMPORTANTE Si tenemos \\(\\lambda(x\\vert \\theta) = \\ln f(x\\vert \\theta)\\), entonces tenemos los siguientes resulados \\(\\lambda&#39;(x\\vert \\theta)\\) es una variable aleatoria. \\(\\mathbb{E}[\\lambda&#39;(x\\vert \\theta)] =0\\). \\(\\mathrm{Var}[\\lambda&#39;(x\\vert \\theta)] = - \\mathbb E[\\lambda&#39;&#39;(x|\\theta)] = I(\\theta)\\). A esta cantidad se le conoce como la información de Fisher. Ejemplo. Suponga que \\(X\\sim \\text{Bernoulli}(p)\\). \\(f(x|p) = p^x(1-p)^{1-x}\\), \\(x=0,1\\) satisface supuesto 1. \\(\\displaystyle\\int_{\\mathcal X}f(x|p)dx \\;``=&quot; f(0|p)+f(1|p)\\) satisface el supuesto 3. Entonces, \\(\\lambda(x|p) = \\ln[p^x(1-p)^{1-x}] = x\\ln p + (1-x)\\ln(1-p)\\). \\(\\lambda&#39;(x|p) = \\dfrac xp-\\dfrac{1-x}{1-p}\\). \\(\\lambda&#39;&#39;(x|p) = -\\dfrac x{p^2}-\\dfrac{1-x}{(1-p)^2}\\). De esta manera, \\[I(p) = \\mathbb E\\bigg[\\dfrac xp + \\dfrac{1-x}{(1-p)^2}\\bigg] = \\dfrac p{p^2}+\\dfrac{1-p}{(1-p)^2} = \\dfrac 1{p(1-p)} = \\dfrac 1{\\text{Var}(X)}.\\] Ejemplo. \\(X\\sim N(\\mu,\\sigma^2)\\), \\(\\mu\\) desconocida, \\(\\sigma^2\\) conocida. \\[f(x|\\mu) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\dfrac 1{2\\sigma^2}(x-\\mu)^2\\right)\\] Vea que \\[\\begin{align*} \\dfrac d{du}\\int_{\\mathbb R} f(x|\\mu)dx &amp; = \\int_{\\mathbb R}f&#39;(x|\\mu)dx\\\\ &amp; = \\int_{\\mathbb R} -\\dfrac 1{\\sqrt{2\\pi\\sigma^2}}\\dfrac {2(x-\\mu)^2}{2\\sigma^2} dx\\\\ &amp; = -\\dfrac 1\\sigma \\underbrace{\\int_{\\mathbb R}\\dfrac{u}{\\sqrt{2\\pi}}e^{-\\frac{u}2}du}_{\\mathbb E[N(0,1)]} = 0 \\quad \\text{usando el cambio de variable } \\dfrac{x-\\mu}\\sigma \\end{align*}\\] por lo que cumple el tercer supuesto. Entonces \\(\\lambda(x|\\mu) = \\dfrac 12 \\ln (2\\pi\\sigma^2)-\\dfrac 1{2\\sigma^2}(x-\\mu)^2\\). \\(\\lambda&#39;(x|\\mu) = \\dfrac 1{2\\sigma^2}2(x-\\mu) = \\dfrac{x-\\mu}{\\sigma^2}\\). \\(\\lambda&#39;&#39;(x-\\mu) = -\\dfrac 1{\\sigma^2}\\). Por lo que \\[I(\\mu) = -\\mathbb E[\\lambda&#39;&#39;(x|\\mu)] = \\dfrac{1}{\\text{Var}(X)}\\] Definición. Suponga que \\(X = (X_1,\\dots,X_n)\\) muestra de \\(f(x|\\theta)\\) donde \\(f\\) satisface las condiciones anteriores. Defina \\(\\lambda_n = \\ln f_n(x|\\theta)\\). La información de Fisher de \\(X\\) es \\[I_n(\\theta) = \\mathbb E[(\\lambda&#39;(x|\\theta))^2] = - \\mathbb E[\\lambda&#39;&#39;_n(x|\\theta)].\\] Nota. Observe que \\[\\lambda_n(x|\\theta) = \\ln f_n(x|\\theta) = \\sum_{i=1}^{n} \\lambda(X_i|\\theta)\\] lo que implica que \\[\\lambda&#39;&#39;_n(x|\\theta) = \\sum_{i=1}^n\\lambda(X_i|\\theta).\\] De esta forma, \\[I_n(\\theta) = -\\mathbb E[\\lambda&#39;&#39;(x|\\theta)] = - \\sum_{i=1}^n\\mathbb E[\\lambda&#39;&#39;(X_i|\\theta)] = nI(\\theta).\\] Ejemplo. Suponga que una compañía quiere conocer como se comportan sus clientes en sus tiendas. Hay dos propuestas para este modelo Un modelo Poisson de parámetro \\(t\\theta\\) (\\(t\\) es cualquier valor) para determinar la tasa promedio de llegada de clientes. \\(Y\\sim \\text{Poisson}(\\theta t)\\). Un modelo donde cada cliente es una v.a. exponencial con tasa de llegada \\(\\theta\\) y al final se sumará todas las variables para obtener una \\(\\mathrm{Gamma}(n,\\theta)\\). \\(X\\sim \\sum_{i=1}^{n}\\text{Exp}(\\theta) = \\Gamma(n,\\theta)\\) El tiempo de llegada de cada cliente es independiente. ¿Cuál variable contiene más información de \\(\\theta\\) \\(X\\) o \\(Y\\)? Solución: Para \\(Y\\), \\(f(y|\\theta) = e^{-t\\theta}\\dfrac{(t\\theta)^y}{y!}\\). \\(\\lambda(y|\\theta) = t\\theta + y\\ln (t\\theta) - \\ln y!\\). \\(\\lambda&#39;(y|\\theta) = -t+\\dfrac{ty}{t\\theta}.\\) \\(\\lambda&#39;&#39;(y|\\theta) = -\\dfrac y{\\theta^2}\\). Entonces, \\[I_Y(\\theta) =-\\mathbb E[ \\lambda&#39;&#39;(y|\\theta)] = \\dfrac{\\mathbb E[Y]}{\\theta^2} = \\dfrac{t}\\theta.\\] Como ejercicio, verifique que \\(I_X(\\theta) = \\dfrac n{\\theta^2}\\). Ambas variables tienen la misma información si \\[I_Y(\\theta) = I_X(\\theta) \\implies \\dfrac t\\theta = \\dfrac n{\\theta^2} \\implies n = \\dfrac{\\theta^2 t}{\\theta } = t\\theta.\\] A partir de este ejercicio vamos a hacer un pequeño ejemplo de simulación. Suponga que \\(t\\) es el tiempo que se quiere medir la cantidad de clientes (minutos), \\(\\theta\\) es la cantidad de clientes por minuto y \\(n\\) es el número de clientes que entran. t &lt;- 20 theta &lt;- 5 n &lt;- t / theta Y &lt;- rpois(n = 1000, lambda = t * theta) X &lt;- rgamma(n = 1000, shape = n, rate = theta) Ojo que según lo estimado ambas informaciones de Fisher debería dar aproximadamente igualdad. Para \\(Y\\) tenemos que mean(Y / theta^2) ## [1] 3.99068 Para \\(X\\) por otro lado la información de Fisher es constante (¿Por qué?) n / theta^2 ## [1] 0.16 Entonces bajo este criterio, ambas variables contienen la misma información, aunque modelen el problema desde ópticas diferentes. El proceso \\(Y\\) (Poisson) modela cuántas personas en total entran a la tienda en 20 minutos, asumiendo una tasa de entrada de 5 personas por minuto. hist(Y) El proceso \\(X\\) (Gamma) modela cuánto tiempo se debe esperar para que 4 personas entren a la tienda, asumiendo una tasa de entrada de 5 por minuto. hist(X) Ejercicio: Basado en los valores de la simulación, proponga dos valores de \\(t\\) para que \\(X\\) tenga más información que \\(Y\\). \\(Y\\) tenga más información que \\(X\\). 9.4 Desigualdad de Cramer-Rao Teorema. Si \\(X = (X_1,\\dots, X_n)\\) muestra de \\(f(x|\\theta)\\). Todos los supuestos anteriores son válidos para \\(f\\). Sea \\(T = r(X)\\) un estadístico con varianza finita. Sea \\(m(\\theta) = \\mathbb E_{\\theta}[T]\\) y asuma que \\(m\\) es diferenciable. Entonces: \\[\\text{Var}_\\theta(T)\\geq \\dfrac{[m&#39;(\\theta)]^2}{I_n(\\theta)} =\\dfrac{[m&#39;(\\theta)]^2}{nI(\\theta)} .\\] La igualdad se da si y solo si existen funciones \\(u(\\theta)\\) y \\(v(\\theta)\\) que solo dependen de \\(\\theta\\) tales que \\[T = u(\\theta)\\lambda_n&#39;(x|\\theta) + v(\\theta).\\] Prueba. Para el caso univariado: \\[\\int_{\\mathcal X}f&#39;(x|\\theta)dx = 0.\\] Para el caso multivariado: \\[\\begin{align*} \\int_{\\mathcal X^n}f&#39;_n(x|\\theta)dx_1\\cdots dx_n &amp; =\\int_{\\mathcal X^n}[f(x_1|\\theta)\\cdots f(x_n|\\theta)]&#39;dx_1\\cdots dx_n \\\\ &amp; = \\dfrac d{d\\theta} \\int_{\\mathcal X^n}f(x_1|\\theta)\\cdots f(x_n|\\theta)dx_1\\cdots dx_n \\\\ &amp;= \\dfrac d{d\\theta} 1 \\\\ &amp;= 0. \\end{align*}\\] Entonces \\[\\mathbb E[\\lambda_n&#39;(X|\\theta)] = \\int_{\\mathcal X^n}\\dfrac{f&#39;_n(x|\\theta)}{f(x|\\theta)} f_{n}(x\\vert \\theta)dx_1\\cdots dx_n = 0\\] Por lo tanto, Ahora \\[\\begin{align*} \\operatorname{Cov}_{\\theta}\\left[T, \\lambda_{n}^{\\prime}(\\boldsymbol{X} \\mid \\theta)\\right] \\\\ &amp;=E_{\\theta}\\left[T \\lambda_{n}^{\\prime}(\\boldsymbol{X} \\mid \\theta)\\right] \\\\ &amp;=\\int_{\\mathcal{X}^n} \\ldots \\int_{\\mathcal{X}^n} r(\\boldsymbol{x}) \\lambda_{n}^{\\prime}(\\boldsymbol{x} \\mid \\theta) f_{n}(\\boldsymbol{x} \\mid \\theta) d x_{1} \\ldots d x_{n} \\\\ &amp; =\\int_{\\mathcal X^n}r(x)\\dfrac{f&#39;_n(x|\\theta)}{f_n(x|\\theta)}f_n(x|\\theta)dx_1\\cdots dx_n\\\\ &amp;=\\int_{\\mathcal{X}^n} \\ldots \\int_{\\mathcal{X}^n} r(\\boldsymbol{x}) f_{n}^{\\prime}(\\boldsymbol{x} \\mid \\theta) d x_{1} \\ldots d x_{n} \\end{align*}\\] Escriba la expresión \\[\\begin{equation*} m(\\theta)=\\int_{\\mathcal{X}^n} \\ldots \\int_{S} r(\\boldsymbol{x}) f_{n}(\\boldsymbol{x} \\mid \\theta) d x_{1} \\ldots d x_{n} \\end{equation*}\\] Usando el supuesto de intercabio de integrales, tenemos que \\[\\begin{equation*} m^{\\prime}(\\theta)=\\int_{\\mathcal{X}^n} \\ldots \\int_{S} r(\\boldsymbol{x}) f_{n}^{\\prime}(\\boldsymbol{x} \\mid \\theta) d x_{1} \\ldots d x_{n} \\end{equation*}\\] Entonces tenemos que \\[\\begin{align*} \\text{Cov}[T,\\lambda_n&#39;(X|\\theta)] &amp; = \\dfrac d{d\\theta}\\int_{\\mathcal X^n}r(x)f_n(x|\\theta)dx_1\\cdots dx_n\\\\ &amp; = \\dfrac{d}{d\\theta}\\mathbb E_\\theta[r(X)] = \\dfrac{d}{d\\theta}E_\\theta[T] = m&#39;(\\theta) \\end{align*}\\] Considere el coeficiente de correlación \\[\\rho = \\dfrac{\\text{Cov}[T,\\lambda_n&#39;(X|\\theta)] }{\\sqrt{\\text{Var}(T)}\\sqrt{\\text{Var}(\\lambda_n&#39;(X|\\theta))}}.\\] Dado que \\(|p|\\leq 1 \\implies \\rho^2 \\leq 1\\), se tiene que \\[\\text{Cov}[T,\\lambda_n&#39;(X|\\theta)]^2 \\leq \\sqrt{\\text{Var}(T)}\\sqrt{\\text{Var}(\\lambda_n&#39;(X|\\theta))} \\implies [m&#39;(\\theta)]^2 \\leq \\text{Var}(T) I_n(\\theta). \\] Entonces \\(\\text{Var}(T)\\geq \\dfrac{[m&#39;(\\theta)]^2 }{I_n(\\theta)}\\). Caso particular. Si \\(T\\) es un estimador insesgado de \\(\\theta\\), entonces \\(\\text{Var}_\\theta(T)\\geq \\dfrac{1 }{I_n(\\theta)}\\). Ejemplo. \\(X_1,\\dots, X_n \\sim \\text{Exp}(\\beta)\\), \\(n&gt;2\\). \\(f(x|\\beta) = \\beta e^{-\\beta x}\\), \\(x&gt;0\\). \\(\\lambda(x|\\beta) = \\ln f(x|\\beta) = \\ln \\beta -\\beta x\\). \\(\\lambda&#39;(x|\\beta) = \\dfrac 1\\beta -x.\\) \\(\\lambda&#39;&#39; = -\\dfrac 1{\\beta^2}\\). Vea que \\[1 = \\int_{0}^\\infty \\beta e^{-\\beta x}dx = \\lim_{u\\to \\infty}F(u) = \\lim_{u\\to \\infty}[1-e^{-\\beta u}]\\] y el supuesto 3 se puede verificar por la diferenciabilidad de \\(1-e^{-\\beta u}\\). Así, \\[I(\\beta) = -\\mathbb E[\\lambda&#39;&#39;(x|\\beta)] = \\dfrac 1{\\beta^2}, \\quad I_n(\\beta) = \\dfrac{n}{\\beta^2}.\\] Por ejemplo generemos una secuencia de valores de \\(\\beta\\) de 1 hasta 5 para observar el comportamiento de su información de Fisher. beta &lt;- seq(1, 5, length.out = 100) n &lt;- 100 lista_muestras &lt;- lapply( X = beta, FUN = function(b) { matrix(rexp(n = n * 500, rate = b), nrow = 500) } ) plot(beta, n / beta^2) Considere el estadístico \\(T = \\dfrac{n-1}{\\sum_{i=1}^n X_i}\\) es un estimador insesgado de \\(\\beta\\). La varianza de \\(T\\) es \\(\\dfrac{\\beta^2}{n-2}\\). La cota de Cramer Rao, si \\(T\\) es insesgado, es \\[\\dfrac 1{I_n(\\beta)} = \\dfrac{\\beta^2}{n},\\] por lo que \\(T\\) no satisface la cota de Cramer Rao. Este comportamiento podemos observarlo con nuestro ejemplo numérico. estimador1 &lt;- sapply( X = lista_muestras, FUN = function(x) { apply(x, 1, function(xx) (n - 1) / sum(xx)) } ) plot(beta, apply(X = estimador1, MARGIN = 2, FUN = mean)) plot(beta, apply(X = estimador1, MARGIN = 2, FUN = var)) lines(beta, beta^2 / n, col = &quot;blue&quot;) lines(beta, beta^2 / (n - 2), col = &quot;red&quot;) Ahora, estime \\(\\theta = \\dfrac 1\\beta = m(\\beta)\\). Un estimador insesgado de \\(\\theta\\) es \\(T =\\bar X_n\\): \\[\\mathbb E[\\bar X_n] = \\mathbb E [X_1] = \\dfrac 1\\beta = \\theta, \\quad \\text{Var}(\\bar X_n) = \\dfrac{\\text{Var}(\\bar X_1) }{n} = \\dfrac 1{n\\beta^2}.\\] La cota de Cramer es \\[\\dfrac{(m&#39;(\\beta))^2}{I_n(\\beta)} = \\dfrac{(-1/\\beta^2)^2}{n/\\beta^2} = \\dfrac{\\beta^2}{n\\beta^4} = \\dfrac{1}{n\\beta^2}.\\] \\(\\bar X_n\\) satisface la cota de Cramer-Rao y además \\[\\lambda&#39;(X|\\beta) = \\dfrac n\\beta - n\\bar X_n =\\dfrac n\\beta - nT \\implies T = \\underbrace{-\\dfrac 1n}_{u(\\beta)}\\lambda_n&#39;(X|\\beta)+ \\underbrace{\\dfrac 1\\beta}_{v(\\beta)}. \\] estimador2 &lt;- sapply( X = lista_muestras, FUN = function(x) { apply(x, 1, function(xx) mean(xx)) } ) plot(1 / beta, apply(X = estimador2, MARGIN = 2, FUN = mean)) plot(beta, apply(X = estimador2, MARGIN = 2, FUN = var)) lines(beta, 1 / (n * beta^2), col = &quot;blue&quot;) 9.5 Estimadores eficientes Definición. \\(T\\) es un estimador eficiente de su esperanza \\(m(\\theta)\\) si su varianza es la cota de CR. Ejemplo. \\(X_1,\\dots, X_n\\sim \\text{Poi}(\\theta)\\). \\(\\bar X_n\\) es un estimador eficiente. Verosimilitud: \\(f_n(X|\\theta) = e ^{n\\theta}\\dfrac{\\theta^{n\\bar X_n}}{\\prod X_i!}\\). \\(\\lambda_n(X|\\theta) = -n\\theta + n\\bar X_n \\ln \\theta - \\ln \\prod X_i!\\). \\(\\lambda&#39;_n(X|\\theta) = -n+\\dfrac{c\\bar X_n}{\\theta}\\). \\(\\lambda_n&#39;&#39;(X) = -\\dfrac{n\\bar X_n}{\\theta^2}\\). Entonces \\[\\dfrac{n}{\\theta^2}\\mathbb E[\\bar X_n] = \\dfrac n{\\theta}.\\] La cota de CR es \\(\\dfrac \\theta n\\), pero \\[\\text{Var}(\\bar X_n) = \\dfrac{\\text{Var}(X_1)}{m} = \\dfrac \\theta n.\\] Por lo que \\(\\bar X_n\\) es eficiente. Los otros candidatos para estimar \\(\\theta\\) \\[\\sigma_1^2=\\dfrac 1{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2} = \\dfrac 1{n-1}\\sum (X_i-\\bar X_n)^2,\\] y \\[\\alpha \\bar X_n + (1-\\alpha)\\hat\\sigma^2_1\\] no son lineales con respecto a \\(\\lambda&#39;(X|\\theta)\\) por lo que tienen mayor varianza que \\(\\bar X_n\\). 9.6 Comportamiento asintótico del MLE Teorema. Bajo las condiciones anteriores y si \\(T\\) es un estimador eficiente de \\(m&#39;(\\theta)\\) y \\(m&#39;(\\theta) \\neq 0\\), entonces \\[\\dfrac 1{\\sqrt{CR}}[T-m(\\theta)]\\xrightarrow{d}N(0,1)\\] Prueba. Recuerde que \\(\\lambda&#39;_n(X|\\theta) = \\sum_{i=1}^n\\lambda&#39;(X_i|\\theta)\\). Como \\(X\\) es una muestra, \\(\\lambda&#39;(X_i|\\theta)\\) son i.i.d, y \\[\\mathbb E[\\lambda&#39;(X_i|\\theta)] = 0, \\quad \\text{Var}(\\lambda&#39;(X_i|\\theta)) = I(\\theta).\\] Como \\(T\\) es estimador eficiente de \\(m(\\theta)\\), \\[\\mathbb E[T] = m(\\theta), \\quad \\text{Var}(T) = \\dfrac{(m&#39;(\\theta))^2}{nI(\\theta)}\\] y existen \\(u(\\theta)\\) y \\(v(\\theta)\\) tal que \\[T = v(\\theta \\lambda&#39;(X|\\theta)) + v(\\theta).\\] \\(\\mathbb E [T]= u(\\theta)\\mathbb E[\\lambda&#39;(X|\\theta)] + v(\\theta) \\implies v(\\theta) = m(\\theta)\\). \\(\\text{Var}(T) = u^2(\\theta)I_n(\\theta) \\implies v(\\theta) = \\dfrac{m&#39;(\\theta)}{nI(\\theta)}\\). Entonces \\(T = \\dfrac{m&#39;(\\theta)}{nI(\\theta)}\\lambda&#39;(X|\\theta) + m(\\theta)\\). Por lo tanto, \\[\\bigg[\\dfrac{nI(\\theta)}{m&#39;(\\theta)^2}\\bigg]^{\\frac 12}[T-m(\\theta)] = \\bigg[\\dfrac 1 {nI(\\theta)}\\bigg]^{\\frac 12}\\lambda&#39;_n(x|\\theta) \\xrightarrow[n\\to\\infty]{} N(0,1).\\] Teorema. Suponga que el MLE \\(\\hat \\theta_n\\) se obtiene al resolver \\(\\lambda&#39;(x|\\theta) = 0\\). Además, \\(\\lambda&#39;&#39;(x|\\theta)\\) y \\(\\lambda&#39;&#39;&#39;(x|\\theta)\\) existen y las condiciones anteriores son ciertas. \\[[nI(\\theta)]^{1/2}(\\hat\\theta-\\theta) \\to N(0,1).\\] Ejemplo. \\(X_1,\\dots, X_n \\sim N(0,\\sigma^2)\\), \\(\\sigma\\) desconocida. \\(\\hat\\sigma = \\bigg[\\dfrac 1n \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2}\\bigg]^{1/2}\\) es MLE de \\(\\sigma\\) y \\(I(\\sigma) = \\dfrac 2{\\sigma^2}\\). Usando el teorema, \\[\\sqrt{\\dfrac{2n}{\\sigma^2}} (\\hat{\\sigma} - \\sigma) \\underset{n\\to\\infty}{\\sim} N\\left(0,1\\right).\\] O lo que es equivalente a \\[\\hat{\\sigma} \\underset{n\\to\\infty}{\\sim} N\\left(\\sigma,\\dfrac{\\sigma^2}{2n}\\right).\\] Ejercicio: Verifique que \\[ \\hat\\sigma_n\\pm z_{\\frac{1+\\gamma}{2}}\\sqrt{\\dfrac{\\sigma^2}{2n}}\\] es un intervalo de confianza para \\(\\sigma\\). Ejercicio: Suponga que se le da los siguientes datos X &lt;- rnorm(n = 1000, mean = 5, sd = 1) Trate de ajustar un intervalo de confianza usando la cota de Cramer-Rao para \\(\\sigma^2\\). Sugerencia: Fijese que la varianza de \\(\\hat\\sigma\\) depende del parámetro desconocido \\(\\sigma\\). Entonces, lo mejor ahí es usar el método Delta para encontra una función que estabilice la varianza. Consecuencia en estimación bayesiana. La previa de \\(\\theta\\) es positiva y diferenciable con respecto a \\(\\theta\\). Bajo todas las condiciones anteriores: \\[\\theta|X\\underset{n\\to\\infty}{\\sim} N\\left(\\hat\\theta_n,\\dfrac 1{nI(\\hat\\theta_n)}\\right).\\] Nota: un IC para \\(\\theta\\) en este caso tiene un error estándar que depende del MLE. La Gamma Inversa con paramétros \\(\\alpha\\) y \\(\\beta\\) tiene media \\(\\dfrac{\\beta}{\\alpha-1}.\\)↩︎ Si \\(X\\sim\\text{Gamma-Inversa}(\\alpha, \\beta)\\) entonces \\(\\text{Var}(X)=\\dfrac{\\beta ^{2}}{(\\alpha -1)^2(\\alpha-2)}\\).↩︎ "],
["pruebas-de-hipótesis.html", "Capítulo 10 Pruebas de hipótesis 10.1 Pruebas de hipótesis 10.2 Regiones críticas y estadísticas de prueba 10.3 Función de potencia y tipos de error 10.4 Valor \\(p\\) 10.5 Dualidad entre pruebas de hipótesis y regiones de confianza", " Capítulo 10 Pruebas de hipótesis 10.1 Pruebas de hipótesis Ejemplo: Suponga que se hace un experimento donde se seleccionan 52 nubes al azar y 26 se les inyecta nitrato de plata y a las otras 26 no. Entonces se quiere saber cuál de los dos grupos produce más lluvia. Los datos de la cantidad de lluvia para este experimento están acá. nubes &lt;- read.table(file = &quot;./data/clouds.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) head(nubes) ## Unseeded.Clouds Seeded.Clouds ## 1 1202.6 2745.6 ## 2 830.1 1697.8 ## 3 372.4 1656.0 ## 4 345.5 978.0 ## 5 321.2 703.4 ## 6 244.3 489.1 Sin embargo usaremos los datos en escala logarítmica para facilitar el cálculo lognubes &lt;- log(nubes) head(lognubes) ## Unseeded.Clouds Seeded.Clouds ## 1 7.092241 7.917755 ## 2 6.721546 7.437089 ## 3 5.919969 7.412160 ## 4 5.844993 6.885510 ## 5 5.772064 6.555926 ## 6 5.498397 6.192567 Observe que el comportamiento es distinto en ambos casos. df &lt;- as.data.frame(nubes) %&gt;% pivot_longer( cols = everything(), names_to = &quot;tratamiento&quot;, values_to = &quot;lluvia&quot; ) %&gt;% mutate(log_lluvia = log(lluvia)) ggplot(data = df) + geom_histogram(aes( x = lluvia, y = ..density.., fill = tratamiento ), color = &quot;black&quot;, bins = 10 ) + facet_wrap(. ~ tratamiento) ggplot(data = df) + geom_histogram(aes( x = log_lluvia, y = ..density.., fill = tratamiento ), color = &quot;black&quot;, bins = 10) + facet_wrap(. ~ tratamiento) En este caso supondremos que la variable log_lluvia se puede modelar como una \\(N(\\mu,\\sigma^2)\\), \\(\\mu,\\sigma\\) desconocidos. Ejercicio: Queda como ejercicio calcular \\(\\mathbb P (\\mu &gt; 4 \\vert X)\\) usando un método Bayesiano Normal-Gamma. En este caso la probabilidad tiene que dar \\(\\approx 99\\%\\). El valor de \\(\\mu&gt;4\\) nace a partir de una pregunta de investigación y se fórmula una hipótesis con respecto a los datos. En este caso sería \\(\\theta = (\\mu,\\sigma^2)\\), ¿Será cierto que para \\(\\theta\\in\\{(\\mu,\\sigma^2):\\mu&gt;4\\}\\)? Para el caso bayesiano, ya calculamos \\(\\mathbb P[\\mu&gt;4|X]\\). ¿Cómo resolverlo en el caso frecuentista? Suponga que \\(\\Omega = \\Omega_0 \\cup\\Omega_1\\) conjuntos disjuntos tales que \\[\\begin{align*} H_0 : \\text{hipótesis en donde }\\theta \\in \\Omega_0.\\\\ H_1 : \\text{hipótesis en donde }\\theta \\in \\Omega_1.\\\\ \\end{align*}\\] Objetivo. Decidir si \\(H_0\\) o \\(H_1\\) es cierto, con los datos disponibles (problema de pruebas de hipótesis). Definición. \\(H_0:\\) hipótesis nula. \\(H_1:\\) hipótesis alternativa. Una vez que se ha realizado una prueba de hipótesis si afirmamos \\(\\theta \\in \\Omega_1\\) decimos que rechazamos \\(H_0\\). Si \\(\\theta \\in \\Omega_0\\), decimos que no rechazamos \\(H_0\\). Suponga que \\(X_1,\\dots, X_n\\sim f(x|\\theta)\\), \\(\\theta \\in \\Omega\\), \\(\\Omega = \\Omega_0 \\cup\\Omega_1\\) y queremos probar la hipótesis \\(H_0: \\theta \\in \\Omega_0\\), \\(H_1: \\theta \\in \\Omega_1\\). Definición (\\(i = 0,1\\)) Si \\(\\Omega_i\\) tiene solamente un valor de \\(\\theta\\), \\(H_i\\) es una hipótesis simple. Si \\(\\Omega_i\\) tiene más de un valor de \\(\\theta\\), \\(H_i\\) es una hipótesis compuesta. Hipótesis compuestas de una cola. Si \\(\\Omega_0 = (-\\infty,\\theta_0]\\), \\(H_0: \\theta\\geq \\theta_0\\), \\(H_1: \\theta &gt;\\theta_0\\). Si \\(\\Omega_0 = [\\theta_0,+\\infty)\\), \\(H_0: \\theta\\leq \\theta_0\\), \\(H_1: \\theta&lt;\\theta_0\\). Si \\(H_1: \\theta \\ne \\theta_0\\) y \\(H_0: \\theta = \\theta_0\\) es una hipótesis de 2 colas. 10.2 Regiones críticas y estadísticas de prueba Ejemplo. Si \\(X_1,\\dots, X_n \\sim N(\\mu,\\sigma^2)\\), \\(\\mu\\) desconocido, \\(\\sigma^2\\) conocido. Queremos probar \\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu\\neq \\mu_0\\). La lógica es: rechazamos \\(H_0\\) si \\(\\mu\\) está “muy alejado” de \\(\\mu_0\\). Seleccione un número \\(c\\) tal que se rechaza \\(H_0\\) si \\(|\\bar X_n -\\mu_0|&gt;c\\). En general, suponga que queremos probar las hipótesis \\(H_0: \\theta \\in \\Omega_0\\) vs \\(H_1: \\theta\\in \\Omega_1\\). En general, suponga que queremos probar las hipótesis \\(H_0: \\theta \\in \\Omega_0\\) vs \\(H_1: \\theta \\in \\Omega_1\\). Cuando tenemos una muestra \\(X_1,\\dots,X_n \\sim f(x|\\theta)\\). Sea \\(S_0\\subset \\mathcal X\\): conjunto en donde no se rechaza \\(H_0\\) y \\(S_1\\subset \\mathcal X\\): conjunto en donde se rechaza \\(H_0\\). A \\(S_1\\) se le llama región crítica de la prueba de hipótesis. En el caso en que se rechaza \\(H_0\\) si \\(T&gt;c\\) con \\(T = |\\bar X_n-\\mu_0|\\) estadístico de prueba y \\((c,\\infty)\\) es la región de rechazo. X &lt;- matrix(rnorm(1000 * 1000, mean = 2, sd = 3), ncol = 1000) Xbar &lt;- apply(X, 2, mean) mu0 &lt;- 2 T &lt;- abs(Xbar - mu0) c &lt;- seq(-0.25, 0.5, length.out = 1000) df &lt;- data.frame(c = numeric(), test = logical(), region = character()) for (k in 1:length(c)) { df &lt;- rbind(df, data.frame(c = c[k], test = mean(T &gt;= c[k]), region = &quot;S_1&quot;)) } df &lt;- rbind(df, data.frame(c, test = 1 - df$test, region = &quot;S_0&quot;)) ggplot(df, aes(x = c, y = test, color = region)) + geom_line(size = 2) + ylab(&quot;Promedio de veces donde T &gt;= c&quot;) + theme_minimal() En este caso el valor donde decrece la curva es cercano a 0. Eso quiere decir que antes de ese valor, nos encontramos en la región de rechazo. Luego esa región se va haciendo cada vez más pequeña \\(\\vert \\overline{X} - \\mu \\vert \\approx 0\\). Ojo lo que pasaría si por ejemplo cambiamos a \\(\\mu = 4\\), mu0 &lt;- 4 T &lt;- abs(Xbar - mu0) c &lt;- seq(-0.25, 3, length.out = 1000) df &lt;- data.frame(c = numeric(), test = logical(), region = character()) for (k in 1:length(c)) { df &lt;- rbind( df, data.frame(c = c[k], test = mean(T &gt;= c[k]), region = &quot;S_1&quot;) ) } df &lt;- rbind(df, data.frame(c, test = 1 - df$test, region = &quot;S_0&quot;)) ggplot(df, aes(x = c, y = test, color = region)) + geom_line(size = 2) + ylab(&quot;Promedio de veces donde T &gt;= c&quot;) + theme_minimal() El valor donde comienza a crecer la curva se desvía a un valor cercano a 2. Nota. En la mayoría de los casos, la región crítica se define en términos de un estadístico \\(T = r(x)\\). Definición. Sea $ X$ una muestra aleatoria con distribución \\(f(x|\\theta)\\) y \\(T=r(X)\\) un estadístico y \\(R\\subset \\mathbb R\\). Suponga que se puede verificar las hipótesis al afirmar “rechazamos \\(H_0\\) si \\(T\\in R\\)”, entonces \\(T\\) es un estadístico de prueba y \\(R\\) es la región de rechazo de la prueba. Continuación del Ejemplo:. Para el caso del ejemplo de la lluvia definimos que \\[ H_0: \\mu \\leq 4 \\text{ versus } H_1: \\mu &gt; 4 \\] En este caso podríamos decir que rechazamos \\(H_0\\) si la media empírica es “más grande” que 4 y no rechazamos \\(H_0\\) si la media empírica es “más pequeña” que 4. El problema acá es que “más grande” y “más pequeña” no son términos precisos. Tenemos dos opciones Construya la región de critica de la forma \\[\\begin{equation} S_{0}=\\left\\{\\boldsymbol{x}:\\leq \\bar{X}_{n}-\\mu_{0} \\leq c\\right\\}, \\quad \\text { y } \\quad S_{1}=S_{0}^{C} \\end{equation}\\] y observe cuál es la probabilidad que ocurra para cada tipo de \\(c\\). El problema con esta construcción es que requiere conocer todos los posibles vectores de datos \\(\\mathbf{X}\\) y construir los conjuntos \\(S_0\\) y \\(S_1\\). Una mejor opción es tener un estadístico sencillo que cumpla dos condiciones: Un estadístico sencillo de calcular posiblemente suficiente, minimal y eficiente. Un estadístico con una distribución conocida. En ese caso \\(\\overline{X}_{n}\\) funciona muy bien, porque tiene todas las buenas propiedades de sufiencia, minimalidad y eficiencia, y además sabemos su distribución según lo estudiando en capítulos pasados. Entonces \\[\\begin{equation*} U = \\frac{n ^{1/2} (\\overline{X}_{n} - \\mu_0)}{\\sigma^\\prime} \\sim t_{n-1} \\end{equation*}\\] Lo natural debería ser rechazar \\(H_{0}\\) si \\(U\\) es grande. colnames(lognubes) ## [1] &quot;Unseeded.Clouds&quot; &quot;Seeded.Clouds&quot; Xbarra1 &lt;- mean(lognubes[, 1]) Xbarra2 &lt;- mean(lognubes[, 2]) sigma_prima1 &lt;- sd(lognubes[, 1]) sigma_prima2 &lt;- sd(lognubes[, 2]) n &lt;- dim(lognubes)[1] (U1 &lt;- sqrt(n) * (Xbarra1 - 4) / sigma_prima1) ## [1] -0.02979683 (U2 &lt;- sqrt(n) * (Xbarra2 - 4) / sigma_prima2) ## [1] 3.615624 ggplot(data = data.frame(x = (c(-1, 4))), mapping = aes(x)) + stat_function( fun = dt, args = list(df = n - 1), mapping = aes(color = &quot;Distribucioń t-student&quot;), size = 2 ) + geom_vline(mapping = aes( xintercept = U1, color = &quot;Nubes no tratadas&quot; ), size = 2) + geom_vline(mapping = aes( xintercept = U2, color = &quot;Nubes tratadas&quot; ), size = 2) + theme_minimal() 10.3 Función de potencia y tipos de error Sea \\(\\delta\\) un procedimiento de prueba (basado en una región crítica o en un estadístico de prueba). Sea \\(\\pi(\\theta|\\delta)\\) (función de potencia) la probabilidad de que se rechace \\(H_0\\) a través de \\(\\delta\\) para \\(\\theta\\in \\Omega\\). Si \\(S_1\\) es la región crítica de \\(\\delta\\) entonces \\(\\pi(\\theta|\\delta) = \\mathbb P(X\\in S_1|\\theta)\\) para \\(\\theta\\in\\Omega\\). Si \\(\\delta\\) se describe a través de un estadístico de prueba \\(T\\) con región de rechazo \\(R\\), entonces \\(\\pi(\\theta|\\delta) = \\mathbb P(T \\in R|\\theta)\\) para \\(\\theta\\in\\Omega\\). Nota. Función de potencia ideal: \\(\\pi(\\theta|\\delta) = 0\\) si \\(\\theta\\in\\Omega_0\\), y \\(\\pi(\\theta|\\delta) = 1\\) si \\(\\theta\\in\\Omega_1\\). Ejemplo. Estadístico de prueba: \\(T = |\\bar X_n-\\mu_0|\\). Región de rechazo: \\(R = (c,\\infty)\\). Como \\(X_1,\\dots, X_n \\sim N(\\mu, \\sigma^2)\\), \\(\\mu\\) desconocido, \\(\\sigma^2\\) conocido entonces \\(\\bar X_n \\sim N\\left(\\mu,\\dfrac{\\sigma^2}{n}\\right)\\) Función de potencia: \\[\\begin{align*} \\pi(\\theta|\\delta) = \\mathbb P[T\\in R|\\mu] &amp; = \\mathbb P [|\\bar X_n -\\mu_0|&gt;c|\\mu] \\\\ &amp;= \\mathbb P [\\bar X_n &gt; \\mu_0+c|\\mu] + \\mathbb P [\\bar X_n &lt; \\mu_0-c|\\mu]\\\\ &amp; = \\mathbb P \\bigg[\\sqrt n \\dfrac{(\\bar X_n-\\mu)}{\\sigma}&gt; \\dfrac{(\\mu_0+c-\\mu)}{\\sigma}\\sqrt n \\bigg|\\mu\\bigg] + \\mathbb P \\bigg[\\sqrt n \\dfrac{(\\bar X_n-\\mu)}{\\sigma}&lt; \\dfrac{(\\mu_0-c-\\mu)}{\\sigma}\\sqrt n \\bigg|\\mu\\bigg] \\\\ &amp; = 1-\\Phi\\left(\\sqrt n \\dfrac{(\\mu_0+c-\\mu)}{\\sigma} \\right) + \\Phi\\left(\\sqrt n \\dfrac{(\\mu_0-c-\\mu)}{\\sigma} \\right) \\end{align*}\\] mu0 &lt;- 4 c &lt;- 2 n &lt;- 100 sigma &lt;- 3 mu &lt;- seq(0, 8, length.out = 1000) funcion_de_poder &lt;- 1 - pnorm(sqrt(n) * (mu0 + c - mu) / sigma) + pnorm(sqrt(n) * (mu0 - c - mu) / sigma) df &lt;- data.frame(mu, funcion_de_poder, tipo = &quot;Función de poder&quot;) df &lt;- rbind(df, data.frame(mu, funcion_de_poder = 1 - df$funcion_de_poder, tipo = &quot;1 - Función de poder&quot; )) ggplot(df, aes(mu, funcion_de_poder, color = tipo)) + geom_line(size = 2) + theme_minimal() mu &lt;- seq(0, 8, length.out = 100) c &lt;- seq(0, 4, length.out = 100) mu_c &lt;- expand.grid(mu, c) funcion_de_poder_n_c &lt;- 1 - pnorm(sqrt(n) * (mu0 + mu_c[, 2] - mu_c[, 1]) / sigma) + pnorm(sqrt(n) * (mu0 - mu_c[, 2] - mu_c[, 1]) / sigma) library(scatterplot3d) scatterplot3d(mu_c[, 2], mu_c[, 1], funcion_de_poder_n_c, type = &quot;p&quot;, angle = 60, xlab = &quot;c&quot;, ylab = &quot;mu&quot;, zlab = &quot;Función de poder&quot; ) Tipos de error: En este tipo de pruebas se puede cometer dos tipos de errores, Error Tipo I: error de rechazar \\(H_0\\) si \\(\\theta \\in \\Omega_0\\). Error Tipo II: error de no rechazar \\(H_0\\) si \\(\\theta\\in\\Omega_1\\) en términos de la función de potencia. En términos de la función de poder tenemos que Si \\(\\theta \\in \\Omega_0\\): \\(\\pi(\\theta|\\delta)\\) es el error tipo I. Si \\(\\theta \\in \\Omega_1\\): \\(1-\\pi(\\theta|\\delta)\\) es el error tipo II. El objetivo es hacer \\(\\pi(\\theta|\\delta)\\) pequeño cuando \\(\\theta\\in\\Omega_0\\). También se requiere que \\(\\pi(\\theta|\\delta)\\) sea grande cuando \\(\\theta \\in\\Omega_1\\). Una forma de alcanzar ese balance es seleccionar \\(\\alpha_0 \\in (0,1)\\) tal que \\[\\pi(\\theta|\\delta) \\leq \\alpha_0\\;\\forall \\theta\\in\\Omega_0\\quad(*)\\] y entre todas las pruebas que cumplan \\((*)\\) se selecciona aquella que maximice la potencia para \\(\\theta \\in \\Omega_1\\). En nuestro ejemplo suponga que elegimos \\(\\alpha_{0} = 0.1\\). La región roja indica donde estaría ubicado \\(\\pi(\\theta\\vert \\delta)\\leq \\alpha_{0}\\). ggplot() + geom_line( data = df, mapping = aes(x = mu, y = funcion_de_poder, color = tipo), size = 2 ) + geom_rect( data = data.frame(xmin = 0, xmax = 8, ymin = 0, ymax = 0.10), mapping = aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.5, fill = &quot;red&quot; ) + geom_hline(yintercept = 0.05) + theme_minimal() Otra forma es minimizar; \\[w_1\\cdot\\text{Error I } + w_2\\cdot\\text{Error II};\\] \\(w_1,w_2\\) constantes. Nota. Bajo la primera solución se produce una asimetría entre las hipótesis, ya que resulta difícil (o muy costoso) que ambas condiciones se cumplan. Por lo general, se le da más énfasis a \\((*)\\), por lo que se trata de controlar el error más serio (Error tipo I). Definición. Una prueba que satisface \\((*)\\) se llama una prueba de nivel \\(\\alpha_0\\) y decimos que la prueba está a un nivel de significancia \\(\\alpha_0\\). Además el tamaño \\(\\alpha(\\delta)\\) de una prueba \\(\\delta\\) se define como: \\[\\alpha(\\delta) = \\sup_{\\theta\\in\\Omega}\\pi(\\theta|\\delta).\\] Corolario. Una prueba \\(\\delta\\) es una prueba de nivel \\(\\alpha_0\\) si y solo si su tamaño es a lo sumo \\(\\alpha_0\\) (\\(\\alpha(\\delta)\\leq\\alpha_0\\)). Ejemplo. Suponga \\(X_1,\\dots,X_n\\sim \\text{Unif}(0,\\theta)\\), \\(\\theta&gt;0\\) desconocido. Se quiere probar las siguientes hipótesis: \\[H_0: 3\\leq\\theta\\leq 4 \\quad H_1:\\theta&lt;3 \\text{ o }\\theta&gt;4. \\] El MLE de \\(\\theta\\) es \\(Y_n = X_{(n)}\\). Si \\(n\\) es grande, \\(Y_n\\) es muy cercano a \\(\\theta\\). Suponga que definimos que la prueba \\(\\delta\\) no rechaza \\(H_0\\) si \\(2.9&lt;Y_n&lt;4\\) y rechaza \\(H_0\\) si \\(Y_n\\geq4\\) o \\(Y_n\\leq2.9\\). Entonces \\(R = (-\\infty, 2.9] \\cup [4,+\\infty)\\) y la función de potencia \\[ \\pi(\\theta|\\delta) = \\mathbb P[Y_n\\leq 2.9|\\theta]+\\mathbb P[Y_n\\geq4|\\theta] \\] \\(\\pi(\\theta|\\delta)\\) se calcula en varios casos: Si \\(\\theta\\leq 2.9 \\implies \\mathbb P[Y_n\\leq 2.9|\\theta] = 1\\) y \\(\\mathbb P[Y_n\\geq4|\\theta] = 0\\). Si \\(2.9&lt;\\theta\\leq4 \\implies \\mathbb P[Y_n\\leq 2.9|\\theta] = 1 - \\prod_{i=1}^n \\mathbb P[X_i\\leq2.9|\\theta] = \\left(\\dfrac{2.9}{\\theta}\\right)^n\\) y \\(\\mathbb P[Y_n\\geq4|\\theta] = 0\\). Si \\(\\theta&gt;4 \\implies \\mathbb P[Y_n\\leq 2.9|\\theta] = \\left(\\dfrac{2.9}{\\theta}\\right)^n\\) y \\(\\mathbb P[Y_n\\geq 4|\\theta] = 1 - \\displaystyle\\prod_{i=1}^n \\mathbb P[X_i&lt;4|\\theta] = 1-\\left(\\dfrac 4\\theta\\right)^n\\). Entonces \\[ \\pi(\\theta|\\delta) = \\begin{cases}1 &amp; \\text{si } \\theta\\leq 2.9 \\\\ \\left(\\dfrac{2.9}{\\theta}\\right)^n&amp; \\text{si } 2.9 &lt;\\theta\\leq 4\\\\ 1+\\left(\\dfrac{2.9}\\theta\\right)^n-\\left(\\dfrac{4}\\theta\\right)^n &amp; \\text{si } \\theta &gt;4\\end{cases} \\] theta &lt;- seq(1, 5, length.out = 1000) n &lt;- 5 funcion_poder &lt;- numeric() for (k in 1:length(theta)) { if (theta[k] &lt; 2.9) { funcion_poder[k] &lt;- 0 } else if (theta[k] &gt; 2.9 &amp; theta[k] &lt;= 4) { funcion_poder[k] &lt;- (2.9 / theta[k])^n } else if (theta[k] &gt; 4) { funcion_poder[k] &lt;- (2.9 / theta[k])^n + 1 - (4 / theta[k])^n } } plot(theta, funcion_poder, type = &quot;l&quot;) Note, además, que el tamaño de prueba es \\[\\alpha(\\delta) = \\sup_{3\\leq\\theta\\leq 4} \\pi(\\theta|\\delta) = \\sup_{3\\leq\\theta\\leq 4}\\left(\\dfrac{2.9}{\\theta}\\right)^n = \\left(\\dfrac{2.9}{3}\\right)^n.\\] n &lt;- 1:100 plot(n, (2.9 / 3)^n) Si \\(n = 68 \\implies \\alpha(\\delta)= \\left(\\dfrac{2.9}{3}\\right)^{68} = 0.0997.\\) Entonces si \\(n = 68\\), entonces \\(\\delta\\) es una prueba con nivel de significancia \\(\\alpha_0\\geq 0.0997\\). Pregunta importante: ¿Cómo diseñar una prueba para que tenga un cierto nivel de significancia? Suponga que queremos probar \\(H_0: \\theta \\in \\Omega_0\\) vs \\(H_1: \\theta\\in\\Omega_1\\). Sea \\(T\\) un estadístico de prueba y suponga que si \\(T\\geq c\\), \\(c\\) constante, entonces rechazamos \\(H_0\\). Si queremos que nuestra prueba tenga nivel de significancia \\(\\alpha_0\\) entonces: \\[\\pi(\\theta|\\delta) = \\mathbb P(T\\geq c|\\theta)\\text{ y } \\sup_{\\theta \\in \\Omega_0}\\mathbb P[T\\geq c|\\theta] \\leq \\alpha_0 \\quad (*)\\] Note que \\(\\pi(\\theta|\\delta)\\) es función no-creciente de \\(c\\), entonces \\((*)\\) se cumple para valores grandes de \\(c\\), si \\(\\theta\\in\\Omega_0\\). Si \\(\\theta \\in \\Omega_1\\), debemos escoger \\(c\\) pequeño para maximizar \\(\\pi(\\theta|\\delta)\\). Ejemplo. En el caso normal, donde \\(H_0: \\mu = \\mu_0\\) y rechazamos \\(H_0\\) si \\(|\\bar X_n-\\mu_0|\\geq c\\). Entonces: \\[\\sup_{\\theta\\in\\Omega_0} \\mathbb P [T\\geq c|\\theta] = \\mathbb P_{\\mu_0}[|\\bar X_n-\\mu_0|\\geq c]\\geq \\alpha_0.\\] Como bajo \\(H_0\\): \\(Y = X_n-\\mu_0 \\sim N\\left(0,\\dfrac{\\sigma^2}{n}\\right)\\), entonces podemos encontrar \\(c\\) tal que \\[\\mathbb P[|\\bar X_n-\\mu_0|\\geq c] = \\alpha_0,\\] y cualquier \\(c\\) mayor va a cumplir \\((*)\\). De esta manera el problema se convierte en encontrar \\(c^*\\) tal que \\(\\mathbb P[|Z|&gt;c^*] = \\alpha_0\\), donde \\(Z = \\dfrac{\\bar X_n - \\mu_0}{\\sigma/\\sqrt n}\\). Note que \\[\\begin{align*} \\alpha_0 &amp;= \\mathbb{P}(|Z|&gt;c ^{*}) \\\\ &amp;= \\mathbb{P}(Z &gt;c ^{*}) + \\mathbb{P}(Z &lt; -c ^{*}) \\\\ &amp;= 1- \\mathbb{P}(Z \\leq c ^{*}) + \\mathbb{P}(Z &lt; -c ^{*}) \\\\ &amp;= 1- \\mathbb{P}(Z \\leq c ^{*}) + 1 - \\mathbb{P}(Z &lt; c ^{*}) \\\\ &amp;= 2 - 2 \\mathbb{P}(Z \\leq c ^{*}) \\end{align*}\\] Entonces \\[\\begin{equation*} \\mathbb{P}(Z \\leq c ^{*}) = 1- \\dfrac{\\alpha_0}{2} \\end{equation*}\\] Por lo tanto el \\(c ^{*}\\) que se busca es \\[\\begin{equation*} c^* = F^{-1}\\left(1- \\dfrac{\\alpha_0}{2}\\right) \\end{equation*}\\] En el caso particular de la normal denotaremos \\(F\\) como \\(\\Phi\\). Entonces, \\[\\Phi(c^*) = 1 - \\dfrac{\\alpha_0}2 \\implies c^* = z_{1-\\frac{\\alpha_0}2}.\\] Procedimiento: rechazamos \\(H_0\\) si \\[|Z| = \\bigg| \\dfrac{\\bar X_n-\\mu_0}{\\sigma/\\sqrt n}\\bigg| \\geq z_{1-\\frac{\\alpha_0}2}.\\] n &lt;- 10 alpha0 &lt;- 0.05 X &lt;- rnorm(n = n, mean = 5, sd = 1) Xbar &lt;- mean(X) mu0 &lt;- 5 Z &lt;- sqrt(n) * (Xbar - mu0) / 1 (q &lt;- qnorm(1 - alpha0 / 2)) ## [1] 1.959964 dnorm_limit &lt;- function(x, q) { y &lt;- dnorm(x) y[-q &lt;= x &amp; x &lt;= q] &lt;- NA return(y) } ggplot(data.frame(x = c(-3, 3)), aes(x)) + stat_function( fun = dnorm_limit, geom = &quot;area&quot;, args = list(q = q), fill = &quot;blue&quot;, alpha = 0.2 ) + stat_function(fun = dnorm) + theme_minimal() La pregunta que debemos siempre responder es ¿Rechazamos \\(H_0\\)? abs(Z) &gt; q ## [1] FALSE Si repetimos el ejercicio anterior, pero los datos tiene media igual a 1 y dejamos que \\(\\mu_0 = 5\\), entonces n &lt;- 10 alpha0 &lt;- 0.05 X &lt;- rnorm(n = n, mean = 1, sd = 1) Xbar &lt;- mean(X) mu0 &lt;- 5 Z &lt;- sqrt(n) * (Xbar - mu0) / 1 Si preguntamos ¿Rechazamos \\(H_0\\)? abs(Z) &gt; q ## [1] TRUE mu0 &lt;- 5 n &lt;- 10 sigma &lt;- 1 alpha0 &lt;- 0.05 c &lt;- qnorm(1 - alpha0 / 2) * sigma / sqrt(n) mu &lt;- seq(3, 7, length.out = 1000) funcion_de_poder &lt;- 1 - pnorm(sqrt(n) * (mu0 + c - mu) / sigma) + pnorm(sqrt(n) * (mu0 - c - mu) / sigma) plot(mu, funcion_de_poder, type = &quot;l&quot;, lwd = 2) abline(h = 0.05, col = &quot;red&quot;, lwd = 2) Ejemplo. \\(X_1,\\dots,X_n \\sim \\text{Ber}(p)\\). \\[H_0: p\\leq p_{0} \\text{ vs } H_1: p&gt;p_0\\] Sea \\(Y = \\sum_{i=1}^nX_i \\sim \\text{Binomial}(n,p)\\). La idea acá es que entre más grande es \\(p\\) entonces más grande esperamos que sea \\(Y\\). Podemos definir la regla que rechazo \\(H_0\\) si \\(Y\\geq c\\) para alguna constante \\(c\\). El error tipo I es \\[\\mathbb P[Y\\geq c|p] = \\sum_{y=c}^n{n\\choose y}p^y(1-p)^{n-y} = \\sum_{y=c}^n{n\\choose y} \\underbrace{\\left(\\dfrac p{1-p}\\right)^y(1-p)^n}_{g(p)}\\] \\(g(p)\\) es monótona con respecto a p.Entonces \\[\\sup_{p\\leq p_0} \\mathbb P[Y\\geq c|p] = \\mathbb P [Y\\geq c|p_0] \\leq \\alpha_0.\\] Si \\(n=10\\), \\(p_0 = 0.3\\), \\(\\alpha_0 = 10\\%\\), entonces c 0 1 2 3 4 5 \\(\\mathbb P[Y\\geq c|p_0]\\) 1 0.97 0.85 0.62 0.15 0.05 Para que el tamaño sea menor que \\(10\\%\\) seleccione \\(c&gt;5\\). Si \\(c\\in [5,6]\\) entonces el nivel de significancia es a lo sumo \\(0.15\\) y la prueba no cambia (ya que \\(Y\\) es una variable discreta). c &lt;- 5 n &lt;- 10 alpha0 &lt;- 0.05 p &lt;- seq(0, 1, length.out = 1000) funcion_de_poder &lt;- 1 - pbinom(q = c, size = n, prob = p) plot(p, funcion_de_poder, type = &quot;l&quot;, lwd = 2) abline(h = 0.05, col = &quot;red&quot;, lwd = 2) Procedimiento: rechazamos \\(H_0: p \\leq 0.3\\) si \\(Y\\geq c\\), \\(c\\in(5,6]\\) con un nivel de significancia de \\(10\\%\\) a lo sumo. 10.4 Valor \\(p\\) Restricción. El procedimiento de prueba depende de \\(\\alpha_0\\). Pregunta. ¿Será posible construir un estadístico que resuma el grado de evidencia en los datos en contra de \\(H_0\\)? Respuesta. Cualquier procedimiento usa las siguientes dos fuentes: El valor observado del estadístico de prueba. Todos los valores de \\(\\alpha_0\\) en donde rechazamos la nula. Ejemplo (Caso Normal). Se rechaza \\(H_0: \\mu = \\mu_0\\) si \\(|Z|&gt;z_{1-\\frac{\\alpha_0}2}\\) Ahora si \\(\\alpha_0 = 0.05\\) y \\(z_{1-\\frac{\\alpha_0}2} = 1.96\\), entonces para \\(Z = 1.97\\) y \\(Z = 2.78\\) y \\(Z = 6.97\\) todos cumplen esa condición. ¿Entonces la pregunta es cuál es mejor? Una forma de estimar esa “fuerza”, es partir del cuantil real de la distribución \\[\\begin{align*} \\Phi(Z) &amp;&gt; 1-\\dfrac{\\alpha_0}2 \\\\ \\alpha_0 &amp;&gt; 2(1-\\Phi(Z)) \\end{align*}\\] Si \\(Z=1.97\\) entonces \\(\\alpha _{0} =0.0488384\\) Si \\(Z=2.78\\) entonces \\(\\alpha _{0} =0.0054359\\) Si \\(Z=6.97\\) entonces \\(\\alpha _{0} =3.1694647\\times 10^{-12}\\) En cada caso se estimó usando el comando 2*(1-pnorm(1.97)) por ejemplo. El valor \\(\\alpha_0\\) se le conoce como valor de significancia. Definición. El valor-\\(p\\) es el nivel más pequeño de significancia en donde rechazaríamos \\(H_0\\) bajo los datos observados. Nota. El valor-\\(p\\) es un estadístico. Si \\(\\text{valor-}p&lt;\\alpha_0\\), rechazo \\(H_0\\). (El valor-\\(p\\) es muy pequeño). Si \\(\\text{valor-}p&gt;\\alpha_0\\), no rechazo \\(H_0\\). (El valor-\\(p\\) es muy grande). Cálculo del valor-\\(p\\) Región de rechazo: \\(T\\geq c\\). Decisión de rechazo: para cada \\(t\\), rechazamos \\(H_0\\) si \\(T\\geq t\\) con \\(t\\geq F^{-1}(1-\\alpha_0)\\), \\(F\\) distribución de \\(T\\). Entonces \\[F(t) \\geq 1-\\alpha_0 \\implies \\alpha_0 \\geq \\mathbb P_\\theta[T\\geq t] \\implies \\alpha_0 \\geq \\sup_{\\theta\\in\\Omega}P_{\\theta}[T\\geq t]\\] El tamaño de la prueba es \\(c=t\\). Ejemplo. Retomando el ejemplo con las variables aleatorias Bernouilli, rechazamos \\(H_0: p\\leq p_0\\) si \\(Y\\geq c\\). Así, \\[\\text{valor-$p$} = \\sup_{p\\in\\Omega}P_{p}[Y\\geq y] =P_{p}[Y\\geq y] \\] Si \\(p_0 = 0.3, n=10, y =6\\), entonces el valor correspondiente es \\(P_{p}[Y\\geq 6] 0.047349\\). El código R es pbinom(q = 5, size = 10, prob = 0.3, lower.tail = FALSE) 10.5 Dualidad entre pruebas de hipótesis y regiones de confianza Teorema. Sea \\(X = (X_1,\\dots,X_n)\\) una muestra con distribución \\(F_\\theta\\). Sea \\(g(\\theta)\\) una función tal que para cada valor \\(g_0\\) de \\(g(\\theta)\\), existe una prueba con nivel \\(\\alpha_0\\) de las hipótesis: \\[H_{0,g_0}: g(\\theta) = g_0 \\text{ vs } H_{1,g_0}: g(\\theta) \\neq g_0. \\] Defina para cada \\(x\\in X\\) \\[\\omega(x) = \\{g_0: \\delta_{g_0} \\text{ no rechaza }H_{0,g_0}\\text{ si }X=x\\} \\quad (*)\\] Sea \\(\\gamma = 1-\\alpha_0\\). Entonces \\[\\mathbb P[g(\\theta_0)\\in \\omega(x)|\\theta = \\theta_0 ] \\geq \\gamma, \\;\\forall \\theta_0 \\in \\Omega.\\] Definición. Si \\(\\omega(x)\\) satisface \\((*)\\) \\(\\forall \\theta_0 \\in \\Omega\\), entonces \\(\\omega(x)\\) es un conjunto de confianza con coeficiente \\(\\gamma\\) donde \\(\\gamma = 1-\\alpha_0\\). Teorema. Bajo las condiciones anteriores, si \\(\\omega(x)\\) es un conjunto de confianza para \\(g_0\\), entonces construimos \\(\\delta_{g_0}\\): no rechazo \\(H_{0,g_0}\\) si y solo si \\(g_0 \\in \\omega(X)\\), entonces \\(\\delta_{g_0}\\) es una prueba con nivel \\(\\alpha_0 = 1-\\gamma\\) para \\(H_{0,g_0}\\). Ejemplo. \\(X_1,\\dots,X_n\\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu,\\sigma^2)\\) (desconocidos). En este caso \\(g(\\theta) = \\mu\\). El intervalo de confianza con nivel \\(gamma\\) es \\[\\bar X_n\\pm t_{n-1,\\frac{1+\\gamma}2}\\dfrac{\\sigma&#39;}{\\sqrt n}.\\] La hipótesis de interés corresponde a \\[ H_0: \\mu = \\mu_0 \\text{ vs } H_1: \\mu \\ne \\mu_0.\\] Por los teoremas anteriores, \\(H_0\\) se rechaza si \\(\\mu_0\\) no está en el IC, es decir, si y solo si \\[\\mu_0 &gt; \\bar X_n+ t_{n-1,\\frac{1+\\gamma}2}\\dfrac{\\sigma&#39;}{\\sqrt n} \\text{ o } \\mu_0 &lt; \\bar X_n- t_{n-1,\\frac{1+\\gamma}2}\\dfrac{\\sigma&#39;}{\\sqrt n},\\] que se puede resumir como \\[\\bigg|\\dfrac{\\bar X_n-\\mu_0}{\\sigma&#39;/\\sqrt n}\\bigg|&gt;t_{n-1,1-\\frac{\\alpha}2}.\\] n &lt;- 1000 gamma &lt;- 0.95 alpha &lt;- 0.95 X &lt;- rnorm(n = n, mean = 1, sd = 2) mu0 &lt;- 1 Xbar &lt;- mean(X) sigma_prima &lt;- sd(X) t_quantil &lt;- qt(p = (1 + gamma) / 2, df = n - 1) El intervalo de confianza es c(Xbar - t_quantil * sigma_prima / sqrt(n), Xbar + t_quantil * sigma_prima / sqrt(n)) ## [1] 0.7678789 1.0139517 \\[ H_0: \\mu = 1 \\text{ vs } H_1: \\mu \\ne 1.\\] Para probar esta prueba se debe comprobar que Z &lt;- abs ((Xbar - mu0) / (sigma_prima / sqrt(n))) Preguntamos ¿Rechazamos \\(H_0\\)? Z &gt; t_quantil ## [1] FALSE Si tuvieramos otros datos con otra media, el resultado será diferente. n &lt;- 1000 gamma &lt;- 0.95 alpha &lt;- 0.95 X &lt;- rnorm(n = n, mean = 5, sd = 2) mu0 &lt;- 1 Xbar &lt;- mean(X) sigma_prima &lt;- sd(X) t_quantil &lt;- qt(p = (1 + gamma) / 2, df = n - 1) c(Xbar - t_quantil * sigma_prima / sqrt(n), Xbar + t_quantil * sigma_prima / sqrt(n)) ## [1] 4.853676 5.103191 ¿Rechazamos \\(H_0\\)? Z &lt;- abs ((Xbar - mu0) / (sigma_prima / sqrt(n))) Z &gt; t_quantil ## [1] TRUE Ejemplo. \\(X_1,\\dots,X_n\\sim N(\\mu,\\sigma^2)\\), \\(\\mu\\) desconocido, \\(\\sigma^2\\) conocido. Construya un intervalo de confianza con nivel \\(\\gamma\\) a partir de \\[ H_0: \\mu = \\mu_0 \\text{ vs } H_1: \\mu \\ne \\mu_0.\\] Rechazamos \\(H_0\\) si \\[\\bigg|\\dfrac{\\bar X_n-\\mu_0}{\\sigma/\\sqrt n}\\bigg|\\geq z_{1-\\frac{\\alpha_0}2}.\\] al nivel \\(\\alpha_0\\). Usando los teoremas anteriores, una región de confianza con nivel \\(\\gamma = 1-\\alpha_0\\) satisface: \\[\\mu\\in\\bigg\\{ \\bigg|\\dfrac{\\bar X_n-\\mu}{\\sigma/\\sqrt n}\\bigg|&lt; z_{1-\\frac{\\alpha_0}2}\\bigg\\} = \\omega(x)\\] Por tanto, \\[\\begin{align*} \\bigg|\\dfrac{\\bar X_n-\\mu}{\\sigma/\\sqrt n}\\bigg| &amp; \\Leftrightarrow -\\dfrac{\\sigma}{\\sqrt n}z_{1-\\frac{\\alpha_0}2}&lt;\\bar X_n - \\mu&lt;\\dfrac{\\sigma}{\\sqrt n}z_{1-\\frac{\\alpha_0}2}\\\\ &amp; = \\Leftrightarrow \\bar X_n-\\dfrac{\\sigma}{\\sqrt n}z_{1-\\frac{\\alpha_0}2}&lt; \\mu&lt;\\bar X_n + \\dfrac{\\sigma}{\\sqrt n}z_{1-\\frac{\\alpha_0}2} \\end{align*}\\] que es el IC con nivel \\(\\gamma\\) para \\(\\mu\\). 10.5.1 Dualidad en pruebas unilaterales Si \\(X = (X_1,\\dots, X_n)\\) es una muestra según \\(F_\\theta\\) y \\(g(\\theta)\\) es una función de variable real, suponga que para cada \\(g_0\\in Im(g)\\) existe una prueba \\(\\delta_{g_0}\\) con nivel \\(\\alpha_0\\) de las hipótesis anteriores. Si \\[\\omega(x) = \\{g_0: \\delta_{g_0} \\text{ no rechaza }H_{0,g_0}\\text{ si }X=x\\}\\] y si \\(\\gamma = 1-\\alpha_0\\), entonces \\(\\omega(x)\\) es una región de confianza para \\(g(\\theta)\\) con nivel \\(\\gamma\\). Ejemplo (Bernoulli). \\[ H_0: p \\leq p_0 \\text{ vs } H_1: p&gt;p_0.\\] El criterio de rechazo al nivel \\(\\alpha_0\\) es \\[Y = \\sum_{i=1}^nX_i\\geq c(p_0)\\] donde \\[\\sup_{p\\leq p_0} \\mathbb P_p[Y\\geq c] = \\mathbb P_{p_0}[Y\\geq c] \\leq \\alpha_0.\\] Entonces \\[\\omega(x) = \\{p_0: Y&lt;c(p_0)\\} = \\{p_0: \\text{valor-}p&gt;\\alpha_0\\}.\\] Si \\(n=10\\), \\(Y=6\\), \\(\\alpha_0 = 10\\%\\), \\[\\omega(x) =\\{p_0: P_{p_0}[Y&gt; 6] &gt;0.1\\}.\\] Numéricamente, si \\(p_0 &gt; 35.42\\% \\implies p_0 \\in \\omega(x)\\), entonces \\(\\omega(x) = (0.3542,1]\\) si \\(\\alpha_0=10\\%\\) y es un IC para \\(p_0\\) con nivel de 90%. Ejemplo. \\(X = (X_1,\\dots, X_n)\\sim N(\\mu,\\sigma^2)\\), \\(\\theta = (\\mu,\\sigma^2)\\) desconocido. Queremos probar \\[ H_0: \\mu \\leq \\mu_0 \\text{ vs } H_1: \\mu &gt; \\mu_0.\\] Por dualidad, basta con conocer un IC unilateral para \\(\\mu\\): \\[ \\left(\\bar X_n-t_{n-1,\\gamma}\\dfrac{\\sigma&#39;}{\\sqrt n},\\infty\\right).\\] Rechazamos \\(H_0\\) si \\[\\mu_0\\leq \\bar X_n-t_{n-1,\\gamma}\\dfrac{\\sigma&#39;}{\\sqrt n} \\Leftrightarrow T = \\dfrac{\\bar X_n -\\mu_0}{\\sigma&#39;/\\sqrt n}\\geq t_{n-1,\\gamma}\\] (rechazando en la cola derecha de T). 10.5.2 Pruebas de cociente de verosimilitud (LRT) Si \\(H_0:\\theta \\in \\Omega_0\\) vs \\(H_1: \\theta \\in \\Omega_0^c = \\Omega_1\\). El estadístico LRT se define como \\[\\Lambda (x) = \\dfrac{\\sup_{\\theta\\in \\Omega_0} f_n(x|\\theta)}{\\sup_{\\theta\\in \\Omega} f_n(x|\\theta)}.\\] Una prueba de cociente de verosimilitud rechaza \\(H_0\\) si \\(\\Lambda(x)\\leq k\\), para una constante \\(k\\). Ejemplo. Supongamos que se observa \\(Y\\) el número de éxitos en el experimento \\(\\text{Bernoulli}(\\theta)\\) con tamaño de muestra \\(n\\). \\[H_0: \\theta = \\theta_0 \\text{ vs } H_1: \\theta\\ne\\theta_0.\\] Verosimilitud: \\(f(y|\\theta) = {n\\choose y}\\theta ^y(1-\\theta)^{n-y}\\). \\(\\Omega_0 = \\{\\theta_0\\}\\), \\(\\Omega_1 = [0,1]\\setminus \\{\\theta_0\\}\\). Numerador: \\(f(y|\\theta_0)\\). Denominador: \\(f(y|\\bar y) = \\displaystyle{n\\choose y}{\\bar y}^{y}(1-\\bar y)^{n-y}\\). \\[\\Lambda(y) = \\dfrac{f(y|\\theta_0)}{f(y|\\bar y)} = \\left(\\dfrac{n\\theta_0}{y}\\right)^y\\left(\\dfrac{n(1-\\theta_0)}{n-y}\\right)^{n-y}, \\quad y=0,\\dots,n.\\] Si \\(n=10\\), \\(\\theta_0 = 0.3\\), \\(y = 6\\), \\(\\alpha_0=0.05\\). n &lt;- 10 p0 &lt;- 0.3 y &lt;- 0:10 alpha0 &lt;- 0.05 p &lt;- choose(n, y) * p0^y * (1 - p0)^(n - y) Lambda &lt;- numeric(n) Lambda[y == 0] &lt;- (1 - p0)^n ym1 &lt;- y[y != 0] Lambda[y != 0] &lt;- (n * p0 / ym1)^ym1 * ((n * (1 - p0)) / (n - ym1))^(n - ym1) plot(y, Lambda, type = &quot;l&quot;, col = &quot;blue&quot;) lines(y, p, type = &quot;l&quot;, col = &quot;red&quot;) knitr::kable(cbind(y, Lambda, p)) y Lambda p 0 0.0282475 0.0282475 1 0.3124791 0.1210608 2 0.7731201 0.2334744 3 1.0000000 0.2668279 4 0.7978583 0.2001209 5 0.4182119 0.1029193 6 0.1465454 0.0367569 7 0.0337359 0.0090017 8 0.0047906 0.0014467 9 0.0003556 0.0001378 10 0.0000059 0.0000059 ix &lt;- order(p) knitr::kable(cbind(y[ix], cumsum(p[ix]))) 10 0.0000059 9 0.0001437 8 0.0015904 7 0.0105921 0 0.0388396 6 0.0755965 5 0.1785159 1 0.2995767 4 0.4996976 2 0.7331721 3 1.0000000 Rechazamos \\(H_0\\) con nivel \\(\\alpha_0 = 0.05\\) en \\(y \\in\\{10,9,8,7,0\\}\\) y \\(k\\in [0.028,0.147)\\) si rechazo cuando \\(\\Lambda(y)\\leq k\\). El tamaño de prueba es \\[\\mathbb P_{0.3}[\\text{Rechazo}] = \\mathbb{P}_{0.3}[Y\\in \\{10,9,8,7,0\\}] = 0.039.\\] Teorema. Sea \\(\\Omega\\) un abierto en \\(\\mathbb R^p\\) y suponga que \\(H_0\\) especifica \\(k\\) coordenadas de \\(\\theta\\), igualándolas a valores fijos. Asuma que \\(H_0\\) es cierto y que todas las condiciones de regularidad de \\(\\theta\\) son ciertas. \\[-2\\ln\\Lambda(x)\\xrightarrow[H_0]{d}\\chi^2_k.\\] Ejemplo. Del caso anterior, \\(k=1\\), \\(\\alpha_0 = 5\\%\\). Rechazamos \\(H_0\\): \\[-2\\ln \\Lambda(y)&gt;\\chi^2_{1,1-0.05} = F^{-1}_{\\chi^2_1}(0.95) = 3.841.\\] Rechazamos \\(H_0\\) bajo la misma región del ejemplo anterior. -2 * log(Lambda) ## [1] 7.1334989 2.3264351 0.5146418 0.0000000 0.4516484 1.7435339 ## [7] 3.8408399 6.7783829 10.6822162 15.8832009 24.0794561 qchisq(p = 0.95, df = 1) ## [1] 3.841459 ¿Rechazamos \\(H_0\\)? knitr::kable(data.frame(y, test = -2 * log(Lambda) &gt; qchisq(p = 0.95, df = 1))) y test 0 TRUE 1 FALSE 2 FALSE 3 FALSE 4 FALSE 5 FALSE 6 FALSE 7 TRUE 8 TRUE 9 TRUE 10 TRUE "],
["pruebas-con-hipótesis-simples.html", "Capítulo 11 Pruebas con hipótesis simples 11.1 Hipótesis simples 11.2 Criterio de Neyman-Pearson 11.3 Pruebas insesgadas 11.4 Prueba \\(t\\)", " Capítulo 11 Pruebas con hipótesis simples 11.1 Hipótesis simples Ejemplo. Sea \\(X_1,\\dots, X_n\\) el tiempo de servicio del cliente #i en el sistema. El administrador del sistema no sabe si la distribución con que se atienden a los clientes es \\[f_1(x) = \\begin{cases}\\dfrac{2(n!)}{(2+\\sum X_i)^{n+1}} &amp; X_i&gt;0\\\\0 &amp; \\text{si no}\\end{cases}\\] O si es una \\(\\text{Exp}(1/2)\\). \\[f_0(x)=\\begin{cases}\\dfrac 1{2^n}e^{-\\frac12\\sum X_i} &amp; \\text{si }X_i&gt;0\\\\0 &amp; \\text{si no} \\end{cases}\\] Si \\(H_0: f=f_0\\) vs \\(H_1:f=f_1\\), ¿Cuál hipótesis es cierta? n &lt;- 1 x &lt;- seq(0, 10, length.out = 1000) f1 &lt;- 2 / (2 + x)^2 f0 &lt;- 1 / 2 * exp(-1 / 2 * x) df &lt;- data.frame(x = c(x, x), f = c(f0, f1), `Distribución` = c(rep(&quot;f_0&quot;, 1000), rep(&quot;f_1&quot;, 1000))) ggplot(df, aes(x, f, color = `Distribución`)) + geom_line() + theme_minimal() Podemos redefinir las hipótesis si \\(\\Omega=\\{\\theta_0,\\theta_1\\}\\) donde si \\(\\theta = \\theta_i\\), seleccionamos \\(f = f_i\\) y se prueba \\(H_0: \\theta=\\theta_0\\) vs \\(H_1:\\theta=\\theta_1\\). Asuma que \\(X_1,\\dots,X_n\\sim f_i(X)\\) donde se pueden tener dos posibilidades \\((i=0,1)\\). Sea \\(\\Omega =\\{\\theta_0,\\theta_1\\}\\) donde \\(\\theta_1\\) es el parámetro que indica a cuál densidad se selecciona como hipótesis. \\[H_0: \\theta=\\theta_0 \\text{ vs } H_1:\\theta=\\theta_1\\] Si \\(\\delta\\) es un procedimiento de prueba, se denota los errores tipo I y II: \\(\\alpha(\\delta) = \\mathbb P[\\text{Rechazo }H_0|\\theta=\\theta_0 ]\\). \\(\\beta(\\delta) = \\mathbb P[\\text{No rechazo }H_0|\\theta=\\theta_1 ]\\). Del ejemplo anterior, si se asume (o se comprueba) que \\(f_1\\) da probabilidades más altas que \\(f_0\\) entonces un criterio de rechazo puede ser \\(X_1&gt;4\\) si se observa solo \\(n=1\\). En este caso, \\[\\alpha(\\delta) = \\mathbb P[X_1&gt;4|\\theta=\\theta_0] = 1-(1-e^{-0.5\\cdot 4}) = 0.135\\] 1 - pexp(q = 4, rate = 1 / 2) ## [1] 0.1353353 \\[\\beta(\\delta) = \\mathbb P[X_1&lt;4|\\theta=\\theta_1] = \\int_{0}^{4}\\dfrac2{(2+x_1)^2}dx_1=0.667.\\] Observación: Para densidades no usuales, hay dos formas de calcular los valores Teóricamente calculando la integral directamente. Si la integral es muy díficil y solo se necesita una aproximación númerica se puede usar integrate: densidad_f1 &lt;- function(x) { 2 / (x + 2)^2 } integrate(densidad_f1, lower = 0, upper = 4) ## 0.6666667 with absolute error &lt; 2.9e-12 Nota: Esta distribución se debe estimar teóricamente ya que no hay fórmula predefinida en R. Objetivo. Encontrar un procedimiento de prueba \\(\\delta\\) tal que \\(\\alpha(\\delta)\\) y \\(\\beta(\\delta)\\) se reduzcan simultáneamente o al menos si \\(a,b&gt;0\\), que \\(a\\alpha(\\delta) + b\\beta(\\delta)\\) sea mínimo. Teorema. Sea \\(\\delta^*\\) un procedimiento de prueba tal que no se rechaza \\(H_0:\\theta=\\theta_0\\) si \\(af_0(x) &gt; bf_1(x)\\) y se rechaza \\(H_0\\) si \\(af_0(x) &lt; bf_1(x)\\). Si \\(af_0(x) = bf_1(x)\\) se puede rechazar o no \\(H_0\\). Entonces para cualquier otro procedimiento de prueba \\(\\delta\\) \\[a\\alpha(\\delta^*) + b\\beta(\\delta^*) \\leq a\\alpha(\\delta) + b\\beta(\\delta). \\] Prueba. Caso discreto solamente. Sea \\(S_1\\) región crítica de \\(\\delta\\) (procedimiento arbitrario). \\[\\begin{align*} a\\alpha(\\delta) + b\\beta(\\delta) &amp; = a\\sum_{x\\in S_1}f_0(x) + b\\sum_{x\\in S_1^c}f_1(x) \\\\ &amp; = a\\sum_{x\\in S_1}f_0(x) + b\\bigg[1-\\sum_{x\\in S_1}f_1(x)\\bigg]\\\\ &amp; = b + \\sum_{x\\in S_1}(af_0-bf_1(x)) \\end{align*}\\] y lo anterior es mínimo si \\(af_0(x)-bf_1(x)&lt;0\\) en toda la muestra y no hay punto en donde \\(af_0(x)-bf_1(x)&gt;0\\). \\(\\qedsymbol\\) Definición. Define el Cociente de verosimilitud como: \\[\\dfrac{f_1(x)}{f_0(x)}.\\] Note que el estadístico del cociente de verosimilitud (LR) está relacionado con el anterior de la siguiente forma: \\[\\Lambda(x) = \\dfrac{f_0(x)}{\\max\\{f_0(x),f_1(x)\\}} = \\dfrac{\\sup_{\\Omega_0}f(x|\\theta)}{\\sup_{\\Omega}f(x|\\theta)}.\\] Corolario. Bajo las condiciones del teorema anterior, si \\(a,b&gt;0\\) entonces la prueba \\(\\delta\\) para la cual \\(a\\alpha(\\delta) + b\\beta(\\delta)\\) es un mínimo rechaza \\(H_0\\) si el cociente de verosimilitud es mayor a \\(\\dfrac ab\\). Ejemplo: Continuando con el ejemplo del servicio al cliente. En lugar de rechazar \\(H_0: \\theta = \\theta_0\\) si \\(X_1&gt;4\\) hay que encontrar \\(a\\) y \\(b\\) que puedan balancear ambos tipos de errores. Supogamos que tomamos \\(a=b\\), entonces basado en el colorario anterior rechace \\(H_0\\) si \\[\\dfrac{f_1(x)}{f_0(x)}&gt;1\\Leftrightarrow \\dfrac 4{(2+X_1)^2}\\exp\\left(\\dfrac{X_1}2\\right)&gt;1\\quad(*)\\] Entonces \\((*)\\) es cierto si \\(X_1&gt;c\\). Se puede comprobar numéricamente que \\(c\\approx5.03\\). Por lo tanto, rechazamos \\(H_0\\) si \\(X_1&gt;5.03\\). En este caso \\(\\alpha(\\delta^*)\\) es igual a 1 - pexp(q = 5.03, rate = 1 / 2) ## [1] 0.08086291 y \\(\\beta(\\delta^*)\\) densidad_f1 &lt;- function(x) { 2 / (x + 2)^2 } integrate(densidad_f1, lower = 0, upper = 5.03) ## 0.715505 with absolute error &lt; 1.3e-10 11.2 Criterio de Neyman-Pearson Encontrar un procedimiento \\(\\delta\\) tal que \\(\\alpha(\\delta) \\leq \\alpha_0\\) (\\(\\alpha_0\\): nivel de significancia). \\(\\beta(\\delta)\\) es mínimo. Lema de Neyman-Pearson. Suponga que \\(\\delta&#39;\\) es un procedimiento de prueba que no rechaza \\(H_0\\) si \\(f_1(x)&lt;kf_0(x)\\) rechaza \\(H_0\\). Si \\(f_1(x)&gt;kf_0(x)\\) y decide cualquiera de los dos si \\(f_1(x)=kf_0(x)\\) para \\(k&gt;0\\). Si \\(\\delta\\) es otro procedimiento de prueba tal que \\(\\alpha(\\delta)\\leq \\alpha(\\delta&#39;)\\), entonces \\(\\beta(\\delta)\\geq \\beta(\\delta&#39;)\\). Si \\(\\alpha(\\delta) &lt;\\alpha(\\delta&#39;)\\), \\(\\beta(\\delta)&gt; \\beta(\\delta&#39;)\\). Prueba. Tome \\(a=k\\) y \\(b=1\\) en el corolario y teoremas anteriores. Como \\[k\\alpha(\\delta&#39;)+\\beta(\\delta&#39;)\\leq k\\alpha(\\delta&#39;)+\\beta(\\delta&#39;),\\] entonces \\[\\alpha(\\delta)\\leq \\alpha(\\delta&#39;)\\implies \\beta(\\delta&#39;)\\geq \\beta(\\delta&#39;).\\] Consecuencia. Si queremos encontrar una prueba \\(\\delta&#39;\\) que satisfaga el criterio de Neyman-Pearson, debemos encontrar \\(k\\) tal que \\(\\alpha(\\delta&#39;) = \\alpha_0\\), y se rechace \\(H_0\\) si \\(f_1(x)&gt;kf_0(x) \\Leftrightarrow\\dfrac{f_0(x)}{f_1(x)}&lt;k^{-1}\\). Ejemplo. Suponga que \\(X_1,\\dots,X_n\\sim N(\\theta,1)\\) y se quiere probar \\(H_0: \\theta = 0\\) vs \\(H_1: \\theta = 1\\) usando una prueba según el criterio de Neyman-Pearson con \\(\\alpha = 0.05\\). Note que \\(f_0(x) = (2\\pi)^{-n/2}\\exp\\bigg[-\\dfrac 12 \\sum_{i=1}^{n} X_i^2\\bigg]\\). \\(f_1(x) = (2\\pi)^{-n/2}\\exp\\bigg[-\\dfrac 12 \\sum_{i=1}^{n} (X_i-1)^2\\bigg]\\). Entonces \\[\\begin{align*} \\dfrac{f_1(x)}{f_0(x)} &amp;= \\exp\\bigg[-\\dfrac 12 \\sum_{i=1}^{n} (X_i^2-2X_i+1-X_1^2)\\bigg]\\\\ &amp;= \\exp\\bigg[n\\bar X_n - \\dfrac n2\\bigg] \\\\ &amp;= \\exp\\bigg[n\\left(\\bar X_n - \\dfrac 12\\right)\\bigg] \\end{align*}\\] Rechazamos \\(H_0\\) si \\[\\dfrac{f*1(x)}{f_0(x)} = \\exp\\bigg[n\\left(\\bar X_n - \\dfrac 12\\right)\\bigg]&gt;k \\Leftrightarrow \\bar X_n &gt; \\underbrace{\\dfrac 12 + \\dfrac{\\ln k}{n}}*{k&#39;} .\\] Entonces buscamos \\(k&#39;\\) tal que \\[\\mathbb P[\\bar X_n&gt;k&#39;|\\theta = 0]=0.05 \\Leftrightarrow\\mathbb P\\bigg[\\dfrac{\\bar X_n}{1/\\sqrt n}&gt;\\dfrac{k&#39;}{1/\\sqrt n}\\bigg|\\theta = 0\\bigg]=0.05\\] Despejando, \\[k&#39;\\sqrt n= z_{0.95} \\implies k&#39;=\\dfrac{z_{0.95}}{\\sqrt n}.\\] Entonces, entre todas las pruebas en donde \\(\\alpha(\\delta)\\leq 0.05\\), la que tiene el error tipo II más pequeño es la que rechaza \\(H_0\\) si \\[\\bar X_n &gt; \\dfrac{z_{0.95}}{\\sqrt n} = \\dfrac{1.645}{\\sqrt n}.\\] El error tipo II de esta prueba sería \\[\\begin{align*} \\beta(\\delta&#39;) = \\mathbb P[\\bar X_n&lt;1.645n^{-1/2}|\\theta = 1]\\\\ &amp; = \\mathbb P\\bigg[Z &lt; \\dfrac{1.645n^{-1/2}-1}{n^{-1/2}}\\bigg] = \\Phi(1.645-n^{1/2}) \\end{align*}\\] Si \\(n=9\\), por ejemplo, \\(\\beta(\\delta&#39;) = \\Phi(1.645-3) =0.0877.\\) Ejemplo. \\(X_1,\\dots,X_n\\sim\\text{Ber}(p)\\) y considere las hipótesis \\[H_0: p = 0.2 \\text{ vs } H_1: p = 0.4.\\] Queremos encontrar un procedimiento de prueba en donde \\(\\alpha(\\delta) = 0.05\\) y \\(\\beta(\\delta)\\) es mínimo. Sea \\(y = \\sum_{i=1}^{n} X_i\\). \\[f_0(x) = 0.2^y0.8^{n-y}\\] \\[f_1(x) = 0.4^y0.6^{n-y}\\] Entonces el cociente de verosimilitud es \\[\\dfrac{f_1(x)}{f_0(x)}=\\left(\\dfrac 34\\right)^n\\left(\\dfrac 83\\right)^y\\] y se rechaza \\(H_0\\) si \\[\\begin{align*} \\dfrac{f_1(x)}{f_0(x)}&gt;k &amp; \\Leftrightarrow -n\\ln \\left(\\dfrac 43 \\right) + y \\ln \\left(\\dfrac 83 \\right)&gt;\\ln k\\\\ &amp; \\Leftrightarrow y&gt;\\dfrac{\\ln k + n\\ln(4/3)}{\\ln (8/3)} = k&#39;. \\end{align*}\\] Entonces basta con encontrar \\(k&#39;\\) tal que \\[\\mathbb P(Y&gt;k&#39;|p = 0.2) = 0.05,\\] pero como \\(Y\\) es una variable discreta (Binomial), no es posible encontrar ese \\(k&#39;\\). Note que \\[\\mathbb P(Y&gt;4|p=0.2) = 0.0328\\] \\[\\mathbb P(Y&gt;3|p=0.2) = 0.1209\\] Por lo tanto, se puede especificar una prueba con nivel 0.05, \\(\\alpha(\\delta) = 0.0328\\) y potencia mínima si \\(Y&gt;4\\) como región de rechazo. 11.3 Pruebas insesgadas Definición. Considere las hipótesis \\(H_0:\\theta \\in \\Omega_0\\) vs \\(H_1: \\theta\\in \\Omega_1\\). Decimos que una prueba de hipótesis \\(\\delta\\) es insesgada si \\(\\forall \\theta\\in\\Omega_0\\) y \\(\\forall \\theta\\in \\Omega_1\\): \\[\\pi(\\theta|\\delta) \\leq \\pi(\\theta&#39;|\\delta).\\] Esto quiere decir que la probabilidad de que la prueba \\(\\delta\\) rechace la hipótesis nula es siempre más alta cuando la hipótesis alternativa es verdadera que cuando la hipótesis nula es verdadera. Más simplemente, una prueba insesgada dará, en la mayoría de los casos, una decisión correcta sin importar si la hipótesis nula es correcta o no. Una forma fácil de crear pruebas insesgadas es resolviendo númericamente las ecuaciones \\[\\begin{align*} \\pi\\left(\\theta_{0} \\mid \\delta\\right) &amp;=\\alpha_{0}, \\text { and } \\\\ \\left.\\frac{d}{d \\theta} \\pi(\\theta \\mid \\delta)\\right|_{\\theta=\\theta_{0}} &amp;=0 \\end{align*}\\] donde la segunda ecuación es la derivada con respecto al parámetro evaluada en \\(\\theta_0\\). Ejemplo: Retomando el ejemplo del servicio al cliente, suponga que se quiere ver si \\[ H_0: \\theta = \\frac{1}{2} \\quad vs \\quad H_1: \\theta \\neq \\frac{1}{2} \\] La pregunta sería cómo encontrar un test que sea insesgado? Primero podemos hacer una prueba del cociente de verosimilitud para encontrar que tipo de prueba funciona. Definamos \\(\\Lambda (x)\\) como antes de la forma con \\(t = \\sum_{i=1}^{n} X_i\\): \\[ \\Lambda(\\boldsymbol{x})=\\frac{(1 / 2)^{n} \\exp (-t / 2)}{(n / t)^{n} \\exp (-n)}=\\left(\\frac{t}{2 n}\\right)^{n} \\exp (n-t / 2) \\] Esta prueba rechaza \\(H_0\\) si \\(\\Lambda (x) \\leq c\\) para algún \\(c\\) Por ejemplo, poniendo \\(n=10\\) y \\(c = 0.2\\), vemos que \\(\\Lambda (x)\\) tiene esta forma. n &lt;- 10 t &lt;- 1:50 f &lt;- (t / (2 * n))^n * exp(n - t / 2) plot(t, f) abline(h = 0.2, col = &quot;red&quot;) Para resolver correctamente el ejercicio se deben encontrar valores \\(c_1\\) y \\(c_2\\) de modo que \\[ \\left(\\frac{c_{1}}{2 n}\\right)^{n} \\exp \\left(n-c_{1} / 2\\right)=\\left(\\frac{c_{2}}{2 n}\\right)^{n} \\exp \\left(n-c_{2} / 2\\right) \\] Además, recuerde que \\(T = \\sum_{i=1}^{n} X_i\\) es \\(\\Gamma (n, \\frac{1}{2})\\) Para obtener un nivel \\(\\alpha\\) (e.g. 5%), \\(c_{1}\\) y \\(c_{2}\\) deben satisfacer \\[\\begin{align*} \\mathbb P (T \\leq c_1) + \\mathbb P(T\\geq c_2) =&amp; \\alpha G\\left(c_{1} ; n, 1 / 2\\right)+1-G\\left(c_{2} ; n, 1 / 2\\right)=&amp;\\alpha \\end{align*}\\] Donde \\(G\\) es la función de distribución de una gamma. Se debe resolver esta dos ecuaciones simulaneamente para \\(c_1\\) y \\(c_2\\) fn &lt;- function(c1, c2, n) { zero &lt;- abs((c1 / (2 * n))^n * exp(n - c1 / 2) - (c2 / (2 * n))^n * exp(n - c2 / 2)) alpha &lt;- pgamma(q = c1, shape = n, rate = 1 / 2) + pgamma( q = c2, shape = n, rate = 1 / 2, lower.tail = FALSE ) return(c(zero, alpha)) } fn2 &lt;- function(x, n) { crossprod(fn(x[1], x[2], n) - c(0, 0.05)) } sol &lt;- optim(c(1, 1), fn2, n = 3) sol$par ## [1] 1.425302 15.895757 Entonces rechazamos \\(H_0\\) si \\(T\\geq 15.895757\\) o \\(T\\leq 1.4253018\\). Ahora si se desea encontrar una prueba insesgada, lo que se debe considerar es encontrar una prueba de modo que se cumplan las ecuaciones vistas en la definición \\[\\begin{align*} \\pi(\\theta \\mid \\delta)=G\\left(c_{1} ; n, \\theta\\right)+1-G\\left(c_{2} ; n, \\theta\\right) &amp;= \\alpha \\\\ G\\left(c_{1} ; 3,1 / 2\\right)+1-G\\left(c_{2} ; 3,1 / 2\\right)&amp;=0.05 \\end{align*}\\] Derivando cuidadosamente (ver 9.4.13 del libro), se obtiene que \\[\\begin{equation*} \\frac{\\partial}{\\partial \\theta} G(x ; n, \\theta)=\\frac{n}{\\theta}[G(x ; n, \\theta)-G(x ; n+1, \\theta)] \\end{equation*}\\] por lo tanto \\[\\begin{align*} \\left.\\frac{d}{d \\theta} \\pi(\\theta \\mid \\delta)\\right|_{\\theta=\\theta_{0}} &amp;=0 \\\\ G^\\prime(c_1; 3, 1/2) - G^\\prime(c_2; 3, 1/2) &amp; = 0 \\\\ \\end{align*}\\] Finalmente la ecuación que se debe resolver es \\[\\begin{equation*} \\frac{3}{1/2}[G(c_1 ; 3,1/2)-G(c_1 ; 4,1/2)] - \\left(\\frac{3}{1/2}[G(c_2 ; 3,1/2)-G(c_2 ; 4,1/2)]\\right) = 0 \\end{equation*}\\] fn &lt;- function(c1, c2, n) { zero &lt;- n / 0.5 * abs( pgamma(q = c1, shape = n, rate = 1 / 2) - pgamma(q = c1, shape = n + 1, rate = 1 / 2) - pgamma(q = c2, shape = n, rate = 1 / 2) + pgamma(q = c2, shape = n + 1, rate = 1 / 2) ) alpha &lt;- pgamma(q = c1, shape = n, rate = 1 / 2) + pgamma( q = c2, shape = n, rate = 1 / 2, lower.tail = FALSE ) return(c(zero, alpha)) } fn2 &lt;- function(x, n) { crossprod(fn(x[1], x[2], n) - c(0, 0.05)) } sol &lt;- optim(c(1, 1), fn2, n = 3) sol$par ## [1] 1.424926 15.896302 Que son resultan en los mismos valores que la prueba anterior. 11.4 Prueba \\(t\\) La prueba \\(t\\) esta pensada para cuando tanto la media \\(\\mu\\) como la desviación estándar \\(\\sigma\\) desconocidas. Suponga que \\(X_1,\\dots, X_n \\sim N(\\mu,\\sigma^2)\\), con \\((\\mu,\\sigma^2)\\) desconocidos, y considere las siguientes hipótesis: \\[H_0: \\mu\\leq\\mu_0 \\text{ vs } H_1:\\mu&gt;\\mu_0.\\] Recuerde que si \\(U = \\dfrac{\\bar X_n -\\mu_0}{\\sigma&#39; /\\sqrt n}\\), entonces la prueba rechaza \\(H_0\\) si \\(U\\geq c\\). Si \\(\\mu=\\mu_0\\) entonces \\(U \\sim t_{n-1}\\). Si \\(H_0: \\mu\\geq\\mu_0\\) vs \\(H_1: \\mu&lt;\\mu_0\\), entonces se rechaza \\(H_0\\) si \\(U\\leq c\\). Ejemplo: Recordemos el ejemplo de los días que un paciente tarda en una casa de cuido en Nuevo México load(&quot;./data/Nursing.rda&quot;) hist(Nursing$InPatientDays) Se quiere probar la hipótesis de \\(H_{0}: \\mu \\leq 200\\) versus \\(H_{1}: \\mu &gt; 200\\), es decir queremos saber si los pacientes duran más de 200 días en cuidados. El estadístico de prueba sería \\[ U = \\sqrt{n} \\frac{(\\overline{X} - \\mu)}{\\sigma ^{\\prime}} \\] y el test rechaza \\(H_{0}\\) si \\(U&gt;t_{n-1, 1-\\alpha}\\). x &lt;- Nursing$InPatientDays n &lt;- length(x) xbar &lt;- mean(x) sigma_prima &lt;- sd(x) alpha &lt;- 0.1 mu0 &lt;- 200 quantil_t &lt;- qt(p = 1 - alpha, df = n - 1) U &lt;- sqrt(n) * (xbar - 200) / sigma_prima ¿Rechazamos \\(H_0\\)? U &gt; quantil_t ## [1] FALSE 11.4.1 Propiedades de las pruebas \\(t\\) Teorema. Sea \\(X_1,\\dots,X_n\\sim N(\\mu,\\sigma^2)\\). Sea \\(U\\) definido anteriormente, \\(c=t_{n-1,1-\\alpha_0}\\). Sea \\(\\delta\\) la prueba que rechaza \\(H_0\\) si \\(U\\geq c\\). Entonces \\(\\pi(\\mu,\\sigma^2|\\delta) = \\alpha_0\\) si \\(\\mu=\\mu_0\\). \\(\\pi(\\mu,\\sigma^2|\\delta) &lt; \\alpha_0\\) si \\(\\mu&gt;\\mu_0\\). \\(\\pi(\\mu,\\sigma^2|\\delta) &gt;\\alpha_0\\) si \\(\\mu&gt;\\mu_0\\). \\(\\pi(\\mu,\\sigma^2|\\delta) \\to 0\\) si \\(\\mu\\to-\\infty\\). \\(\\pi(\\mu,\\sigma^2|\\delta) \\to 1\\) si \\(\\mu\\to+\\infty\\). Entonces, la prueba tiene tamaño \\(\\alpha_0\\) y es insesgada. Prueba. Ver en el libro. En el caso en donde \\(H_0:\\mu\\geq \\mu_0\\) las desigualdades se intercambian y la prueba también tiene tamaño \\(\\alpha_0\\) y es insesgada. Teorema. Bajo cualquiera de los dos casos anteriores, sea \\(U\\) el valor observado de \\(U\\). Entonces, el valor-p de la prueba \\(\\delta\\) que rechaza \\(H_0: \\mu\\leq\\mu_0\\) es \\(1-T_{n-1}(u)\\) donde \\(T_{n-1}\\) es c.d.f de \\(t_{n-1}\\) y si se rechaza \\(H_0 \\mu\\geq \\mu_0\\), el valor-p es \\(T_{n-1}(u)\\). Prueba. El caso \\(H_0:\\mu\\leq\\mu_0\\) es análogo al cálculo del valor-p que se hizo en el capítulo anterior. El caso \\(H_0: \\mu\\geq \\mu_0\\) se rechaza si \\[U\\leq T_{n-1}^{-1}(\\alpha_0) \\Leftrightarrow T_{n-1}(u)\\leq \\alpha_0.\\] Es decir, el nivel más pequeño de significancia observada es \\(T_{n-1}(u)\\) \\(\\qedsymbol\\) Ejemplo: ¿Cuál es el valor \\(p\\) del ejemplo de casas de cuido? 1 - pt(q = U, df = n - 1) ## [1] 0.9064029 t.test(x, alternative = &quot;greater&quot;, mu = 200) ## ## One Sample t-test ## ## data: x ## t = -1.3369, df = 51, p-value = 0.9064 ## alternative hypothesis: true mean is greater than 200 ## 95 percent confidence interval: ## 163.6466 Inf ## sample estimates: ## mean of x ## 183.8654 Considere el caso \\(H_0: \\mu\\geq \\mu_0\\) vs \\(H_1: \\mu&gt;\\mu_0\\). Región de rechazo: \\(U\\geq c\\) con \\(U= \\dfrac{\\bar X_n -\\mu_0}{\\sigma&#39; /\\sqrt n}\\). Ejercicio: es una prueba insesgada con nivel \\(\\alpha_0\\) si \\(c = t_{n-1,1-\\alpha_0}\\). Valor-p: si observamos \\(U=u\\), se rechaza \\(H_0\\) si \\(u\\geq t_{n-1,1-\\alpha_0}\\), \\[T_{n-1}(u) \\geq T_{n-1}(t_{n-1,1-\\alpha_0}) = 1-\\alpha_0 \\implies 1-T_{n-1(u)} = \\bar T_{n-1}(u).\\] Función de potencia: \\[\\begin{align*} \\mathbb P[\\text{Rechazo}|\\mu] &amp; = \\mathbb P\\bigg[ \\dfrac{\\bar X_n -\\mu_0}{\\sigma&#39; /\\sqrt n}\\geq t_{n-1,1-\\alpha_0}\\bigg| \\mu \\bigg]\\\\ &amp;= = \\mathbb P\\bigg[ \\dfrac{\\bar X_n +\\mu-\\mu-\\mu_0}{\\sigma&#39; /\\sqrt n}\\geq t_{n-1,1-\\alpha_0}\\bigg| \\mu \\bigg]\\\\ &amp; = \\mathbb P\\bigg[ \\underbrace{\\dfrac{\\bar X_n -\\mu}{\\sigma&#39; /\\sqrt n}}_{\\Delta}+ \\dfrac{\\mu-\\mu_0}{\\sigma&#39; /\\sqrt n}\\geq t_{n-1,1-\\alpha_0}\\bigg| \\mu \\bigg]\\\\ \\end{align*}\\] Observe que \\[\\Delta = \\dfrac{\\bar X_n -\\mu}{\\sigma&#39; /\\sqrt n}\\cdot\\dfrac\\sigma\\sigma = \\dfrac{\\dfrac{\\sqrt n(\\bar X_n-\\mu)}{\\sigma} }{\\dfrac{\\sigma&#39;}\\sigma } \\sim \\dfrac{N(0,1)}{\\sqrt{\\dfrac{\\chi^2_{n-1}}{n-1}}} \\sim t_{n-1}.\\] De igual forma, vea que \\[ U = \\dfrac{\\dfrac{\\sqrt n(\\bar X_n-\\mu_0)}{\\sigma}}{\\dfrac{\\sigma&#39;}{\\sigma}} = \\dfrac{\\dfrac{\\sqrt n}{\\sigma}(\\bar X_n-\\mu) +\\overbrace{\\dfrac{\\sqrt n}{\\sigma}(\\mu-\\mu_0)}^{\\psi}}{\\dfrac{\\sigma&#39;}{\\sigma}} \\sim \\dfrac{N(\\psi,1)}{\\sqrt{\\dfrac{\\chi^2_{n-1}}{n-1}}}. \\] Definición. Si \\(Y\\), \\(W\\) son independientes con \\(W\\sim N(\\psi,1)\\) y \\(Y\\sim \\chi^2_m\\), entonces \\(X\\) se distribuye como una \\(t\\)-Student no centrada con parámetro \\(\\psi\\) si \\[X = \\dfrac W{\\sqrt{\\dfrac{Y}{m}}}.\\] Si \\(T_m(t|\\psi)\\) es c.d.f de \\(X\\), entonces \\[\\pi(\\mu|\\delta)= T_{n-1}(t_{n-1,1-\\alpha_0}).\\] En el caso que la prueba sea \\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu&lt;\\mu_0\\). \\[\\pi(\\mu|\\delta)= \\mathbb P[U\\leq t_{n-1,1-\\alpha_0}] = T_{n-1}(t_{n-1,\\alpha_0}).\\] Conclusión: a partir del error tipo II se puede determinar un tamaño de muestra dado, siempre y cuando existan restricciones sobre \\(\\mu\\) y \\(\\sigma^2\\). En el caso de las casas de cuido, suponga que queremos ver el poder cuando \\(\\mu = 200 + sigma/2\\), entonces el parámetro de no centralidad queda en \\(\\psi = \\dfrac{sqrt{n}}{2}\\). n &lt;- length(x) alpha0 &lt;- 0.05 q &lt;- qt(p = 1 - alpha0, df = n - 1) parametro_no_central &lt;- sqrt(n) / 2 (poder &lt;- pt(q = q, df = n - 1, ncp = parametro_no_central)) ## [1] 0.02792138 11.4.2 Prueba \\(t\\) pareada A veces se quiere comparar la misma variable pero medida bajo dos condiciones distintas. Es decir ver si la media de un experimento es menor o mayor que la otra. Los usual en estos casos es restar ambas medias y hacer una prueba de hipótesis con \\(\\mu_0 = 0\\). Ejemplo: Suponga que se tiene los datos de muñecos de prueba para probar carros en simulaciones de accidentes de tránsito. Defina \\(X_i ^{t_1}\\) el daño reportado al conductor y \\(D_i ^{t_2}\\) el daño al pasajero. Defina el logaritmo del daño como \\(Y_i ^{t_1} = \\ln(D_{i}^{t_1})\\) y \\(Y_i ^{t_2} = \\ln(D_{i}^{t_2})\\) \\(X_i = Y_i^{t_1}-Y_i^{t_2} = \\ln\\left(\\dfrac{D_{i}^{t_1}}{D_{i}^{t_2}}\\right) \\implies D_{i}^{t_2}\\cdot e^{X_i} = D_{i}^{t_1}\\) Evaluemos la prueba \\(H_0:\\mu\\leq 0\\) vs \\(H_1:\\mu&gt;0\\) al 1%. Si \\(X_1,\\dots, X_n \\sim N(\\mu,\\sigma^2)\\) ambos parámetros desconocidos, y \\(n=164\\), \\(\\bar X_n = 0.2199\\), \\(\\sigma&#39;=0.5342\\), rechazamos \\(H_0\\) si \\[ U = \\dfrac{0.2199-0}{\\dfrac{0.5342}{\\sqrt {164}}} = 5.271 &gt;t_{163,1-0.01} = 2.35. \\] El valor-p de la prueba es \\[ 1-\\mathbb P[t_{163}&lt;5.271] = 1\\times10^{-6}&lt;1\\%. \\] Entonces rechazo \\(H_0\\) con nivel de significancia de \\(1%\\). Suponga que la diferencia media entre conductor y pasajero es \\(\\dfrac\\sigma 4\\). ¿Cuál es el error tipo II? \\[ \\mu =\\dfrac\\sigma 4\\implies \\psi = \\dfrac{\\mu-\\mu_0}{\\sigma/\\sqrt n} = \\dfrac{\\sigma/4-0}{\\sigma/\\sqrt{164}} = \\dfrac{\\sqrt{164}}{3} = 3.2. \\] El error tipo II es \\(T_{163}(2.35|\\psi =3.2) = 1-0.802 = 0.198\\). 11.4.3 Pruebas \\(t\\) de dos colas Región de rechazo: \\(|U|\\geq t_{n-1,1-\\frac{\\alpha_0}2}\\). Función de potencia: \\[ \\pi(\\mu|\\delta) = \\mathbb P[U\\geq t_{n-1,1-\\frac{\\alpha_0}2}|\\mu]+\\mathbb P[U\\leq t_{n-1,1-\\frac{\\alpha_0}2}|\\mu] = T_{n-1}(-c|\\psi) + 1-T_{n-1}(c|\\psi). \\] Valor-p: si observamos \\(U=u\\), rechazamos \\(H_0\\) si \\[ | u | \\geq t_{n-1,1-\\frac{\\alpha_0}2} \\Leftrightarrow T_{n-1}( | U | )\\geq 1-\\dfrac{\\alpha_0}2 \\Leftrightarrow \\alpha_0\\geq \\underbrace{2[1-T_{n-1}( | u | )]}_{\\text{valor-}p}. \\] Propiedad. La prueba-\\(t\\) unilateral es equivalente a una prueba de cociente de verosimilitud (LRT). Entonces la prueba "],
["prueba-de-comparación-de-medias-en-2-poblaciones.html", "Capítulo 12 Prueba de comparación de medias en 2 poblaciones 12.1 Comparación de medias normales 12.2 Prueba \\(t\\) de dos muestras 12.3 Prueba \\(F\\)", " Capítulo 12 Prueba de comparación de medias en 2 poblaciones 12.1 Comparación de medias normales Asuma que \\(X_1,\\dots, X_n\\overset{i.i.d}{\\sim} N(\\mu_1,\\sigma^2)\\) y \\(Y_1,\\dots, Y_n\\overset{i.i.d}{\\sim} N(\\mu_2,\\sigma^2)\\). Los parámetros desconocidos son \\(\\mu_1,\\mu_2,\\sigma^2\\). Asuma que \\((X_i,Y_i)\\) son independientes y la varianza es la misma (homocedasticidad). Hipótesis: \\(H_0: \\mu_1\\leq\\mu_2\\) vs \\(H_1: \\mu_1&gt;\\mu_2\\). Notación: \\(\\bar X_m,\\bar Y_n\\), \\(\\displaystyle S_X^2 = \\sum_{i=1}^m(X_i-\\bar X_m)^2\\), \\(\\displaystyle S_Y^2 = \\sum_{i=1}^m(Y_i-\\bar Y_n)^2\\). Teorema. Considere \\[ U = \\dfrac{(m+n-2)^{1/2}(\\bar X_m-\\bar Y_n)}{\\left(\\dfrac 1m+\\dfrac 1n\\right)^{1/2}(S_X^2+S_Y^2)^{1/2}}. \\] Si \\(\\mu_1=\\mu_2 \\implies U\\sim t_{m+n-2}.\\) 12.2 Prueba \\(t\\) de dos muestras Dada una región de rechazo \\(U\\geq c\\), \\[\\begin{align*} \\sup_{\\mu_1\\leq \\mu_2}\\mathbb P[U\\geq c|\\mu_1,\\mu_2,\\sigma^2]\\leq \\alpha_0 &amp; \\implies \\mathbb P[U\\geq c|\\mu_1=\\mu_2,\\sigma^2] = 1-T_{n+m-2}(c) \\leq \\alpha_0 \\\\ &amp; \\implies c = T_{n+m-2}^{-1}(1-\\alpha_0) \\end{align*}\\] Rechazo \\(H_0\\) si \\(U&gt; T_{n+m-2}^{-1}(1-\\alpha_0): \\delta\\). Teorema. La función de potencia \\(\\pi(\\mu_1,\\mu_2,\\sigma^2|\\delta)\\) tiene las siguientes propiedades: \\(\\pi(\\mu_1,\\mu_2,\\sigma^2|\\delta) = \\alpha_0\\) si \\(\\mu_1 = \\mu_2\\). \\(\\pi(\\mu_1,\\mu_2,\\sigma^2|\\delta) &lt; \\alpha_0\\) si \\(\\mu_1 &lt; \\mu_2\\). \\(\\pi(\\mu_1,\\mu_2,\\sigma^2|\\delta) &gt; \\alpha_0\\) si \\(\\mu_1 &gt; \\mu_2\\). Conclusión. \\(\\delta\\) es una prueba insesgada con tamaño \\(\\alpha_0\\). Los límites cuando \\(\\mu_1-\\mu_2\\to -\\infty (+\\infty)\\) son los mismos del caso de una muestra. Observe que para el caso II: \\(H_0: \\mu_1\\geq\\mu_2\\) vs \\(H_1: \\mu_1&lt;\\mu_2\\). \\[\\delta: \\text{Rechazo } H_0 \\text{ si } U&lt;T^{-1}_{n+m-2}(\\alpha_0) = -T_{n+m-2}^{-1}(1-\\alpha_0).\\] Los p-valores son: Caso I: \\(1-T_{n+m-2}(u)\\) si observamos \\(U = u\\). Caso II: \\(T_{n+m-2}(u)\\). Ejemplo: En el caso de las lluvia suponga que queremos probar \\[ H_0: \\mu_{\\text{con trat.}} \\leq \\mu_{\\text{sin trat.}} \\quad vs \\quad H_1: \\mu_{\\text{con trat.}} &gt; \\mu_{\\text{sin trat.}} \\] nubes &lt;- read.table( file = &quot;./data/clouds.txt&quot;, sep = &quot;\\t&quot;, header = TRUE ) log_lluvia &lt;- log(nubes) n &lt;- nrow(nubes) con_tratamiento &lt;- log_lluvia$Seeded.Clouds sin_tratamiento &lt;- log_lluvia$Unseeded.Clouds (Xbar &lt;- mean(con_tratamiento)) ## [1] 5.134187 (Ybar &lt;- mean(sin_tratamiento)) ## [1] 3.990406 (S2_X &lt;- (n - 1) * var(con_tratamiento)) ## [1] 63.96109 (S2_Y &lt;- (n - 1) * var(sin_tratamiento)) ## [1] 67.39158 Entonces el estadístico que queremos construir para comparar la medias es (OJO en este caso \\(m=n\\) porque tienen la misma cantidad de datos: ) (U &lt;- sqrt(n + n - 2) * (Xbar - Ybar) / (sqrt(1 / n + 1 / n) * sqrt(S2_X + S2_Y))) ## [1] 2.544369 Por tanto se debe comparar con una \\(t\\)-student con \\(26+26 - 2 = 50\\) grados de libertad. Asuma un \\(\\alpha = 0.01\\) (qnt &lt;- qt(p = 1 - 0.01, df = n + n - 2)) ## [1] 2.403272 ¿ Rechazamos \\(H_0\\)? U &gt; qnt ## [1] TRUE ¿Cuál es el \\(p\\)-valor? 1 - pt(q = U, df = n + n - 2) ## [1] 0.007041329 Interpretación: rechazamos al nivel 1% de significancia la hipótesis de que las nubes irradiadas tienen una log-precipitación media menor a la de las nubes no irradiadas. 12.2.1 Prueba de 2 colas Hipótesis. \\(H_0: \\mu_1=\\mu_2\\) vs \\(H_1: \\mu_1\\ne\\mu_2\\) (Prueba ANOVA). Prueba. \\(\\delta:\\) Rechazo \\(H_0\\) si \\(|U|\\geq T^{-1}_{m+n-2}\\left(1-\\dfrac{\\alpha_0}2\\right)\\). Valor-p: \\(2[1-T_{m+n-2}(|u|)]\\) donde \\(U=u\\). Ejemplo. Minas de cobre. Sean \\(X_1,\\dots,X_8\\) la cantidad de cobre (gramos) en 8 minas en un lugar 1, y \\(X_1,\\dots,X_{10}\\) en 10 minas en un lugar 2. Después de recolectar los datos se obtiene lo siguiente \\(\\bar X_8 = 2.6\\) \\(\\bar Y_{10} = 2.3\\) \\(S_X^2 = 0.32\\) y \\(S_Y^2=0.22\\) El ingeniero de la mina se pregunta: ¿Las dos localizaciones generan el mismo nivel de cobre? Entonces plantea hacer la prueba de hipótesis \\[ H_0: \\mu_1=\\mu_2 \\quad H_1: \\mu_1\\neq\\mu_2 \\] Con el supuesto que \\(X_i\\sim N(\\mu_1,\\sigma^2)\\), \\(Y_j\\sim N(\\mu_2,\\sigma^2)\\). n &lt;- 8 m &lt;- 10 n + m - 2 ## [1] 16 Xbar &lt;- 2.6 Ybar &lt;- 2.3 S2_X &lt;- 0.32 S2_Y &lt;- 0.22 (U &lt;- sqrt(n + m - 2) * (Xbar - Ybar) / (sqrt(1 / n + 1 / m) * sqrt(S2_X + S2_Y))) ## [1] 3.442652 Si \\(\\alpha_0 = 1\\%\\) (qnt &lt;- qt(p = 1 - 0.01 / 2, df = n + m - 2)) ## [1] 2.920782 Entonces, ¿Rechazamos \\(H_0\\)? abs(U) &gt; qnt ## [1] TRUE El valor \\(p\\) es \\(2[1-T_{16}(|3.442|)]\\) 2 * (1 - pt(q = U, df = n + m - 2)) ## [1] 0.003345064 Interpretación: Rechazamos al 1% de significancia la hipótesis de una diferencia no significativa entre las cantidades medias de cobre en cada localización. Ejercicio. La prueba \\(t\\) de 2 muestras es un LRT. 12.3 Prueba \\(F\\) Definición Si \\(Y\\) y \\(W\\) son variables aleatorias independientes, \\(Y\\sim \\chi^2_m\\) y \\(W\\sim \\chi ^2_n\\), \\(m,n\\in \\mathbb Z^+\\). Defina \\[X = \\dfrac{Y/m}{W/n}\\sim F_{m,n}\\] \\(X\\) tiene una distribución \\(F\\) con \\(m\\) y \\(n\\) grados de libertad. La función de densidad es \\[\\begin{equation} f(x)= \\begin{cases} \\displaystyle \\frac{\\Gamma\\left[\\frac{1}{2}(m+n)\\right] m^{m / 2} n^{n / 2}}{\\Gamma\\left(\\frac{1}{2} m\\right) \\Gamma\\left(\\frac{1}{2} n\\right)} \\cdot \\frac{x^{(m / 2)-1}}{(m x+n)^{(m+n) / 2}} &amp; x&gt;0 \\\\ 0 &amp; x\\leq 0. \\end{cases} \\end{equation}\\] Propiedades: Si \\(X\\sim F_{m,n} \\implies 1/X\\sim F_{n,m}\\). Si \\(Y\\sim t_n \\implies Y^2\\sim F_{1,n}\\). Sean \\(X_1,\\dots, X_n\\overset{i.i.d}{\\sim} N(\\mu_1,\\sigma_1^2)\\) y \\(Y_1,\\dots, Y_n\\overset{i.i.d}{\\sim} N(\\mu_2,\\sigma_2^2)\\). Considere el esquema \\[\\begin{align*} U\\sim t_{n-1}\\text{ }&amp; \\quad \\quad U^2\\sim F_{1,n-1}\\\\ H_0: \\mu=\\mu_0\\text{ } &amp; \\Leftrightarrow \\text{ } H_0: \\mu=\\mu_0 \\\\ |U|\\geq |c|\\text{ } &amp; \\quad \\quad U^2\\geq c^* \\end{align*}\\] Bajo el esquema anterior y si \\((X,Y)\\) son independientes, considere: \\[H_0: \\sigma_1^2\\leq \\sigma_2^2 \\text { vs } H_1: \\sigma_1^2&gt; \\sigma_2^2 \\] y tome \\(\\alpha_0 \\in (0,1)\\). La lógica de esta prueba es, como \\(\\dfrac{S_X^2}{\\sigma_1^2} \\sim \\chi^2_{m-1}\\) y \\(\\dfrac{S_Y^2}{\\sigma_2^2} \\sim \\chi^2_{n-1}\\), calculamos \\(V^* = \\dfrac{\\dfrac{S_X^2/\\sigma_1^2}{m-1}}{\\dfrac{S_Y^2/\\sigma_2^2}{n-1}}\\sim F_{m-1,n-1}\\). Bajo el supuesto de homocedasticidad, \\(V = \\dfrac{\\dfrac{S_X^2}{m-1}}{\\dfrac{S_Y^2}{n-1}}\\sim F_{m-1,n-1}\\). \\(\\delta:\\) Rechazo \\(H_0\\) si \\(V\\geq c\\). Teorema. La distribución de \\(V^*\\sim F_{m-1,n-1}\\) y si \\(\\sigma_1=\\sigma_2\\), \\(V \\sim F_{m-1,n-1}\\). Usando el \\(\\delta\\) anterior \\[\\sup_{\\sigma_1^2\\leq\\sigma^2_2}\\mathbb P[V\\geq c|\\mu_1\\mu_2,\\sigma^2_1,\\sigma_2^2]\\leq \\alpha_0,\\] resuelve \\[\\mathbb P[V\\geq c|\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2] = \\alpha_0 \\implies c = F^{-1}_{m-1,n-1}(1-\\alpha_0) = G^{-1}_{m-1,n-1}(1-\\alpha_0).\\] Teorema. si \\(\\delta\\) se define según lo anterior, \\[\\begin{align*} \\pi(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2|\\delta) &amp; = \\mathbb P[V\\geq G^{-1}_{m-1,n-1}(1-\\alpha_0)]\\\\ &amp; = \\mathbb P\\bigg[V^* \\geq \\dfrac{\\sigma_2^2}{\\sigma_1^2}c\\bigg]\\\\ &amp; = 1-G_{m-1,n-1}\\left(\\dfrac{\\sigma_2^2}{\\sigma_1^2}c\\right) \\end{align*}\\] \\(\\pi(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,|\\delta) = \\alpha_0\\) si \\(\\sigma_1^2 = \\sigma_2^2\\). \\(\\pi(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2|\\delta) &lt; \\alpha_0\\) si \\(\\sigma_1^2 &lt; \\sigma_2^2\\). \\(\\pi(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2|\\delta) &gt; \\alpha_0\\) si \\(\\sigma_1^2 &gt; \\sigma_2^2\\). \\(\\dfrac{\\sigma_1^2 }{\\sigma_2^2 }\\to 0 \\implies \\pi \\to 0\\). \\(\\dfrac{\\sigma_1^2 }{\\sigma_2^2 }\\to \\infty \\implies \\pi \\to 1\\). Por (i)-(iv) \\(\\delta\\) es insesgada con tamaño \\(\\alpha_0\\). El valor-p es \\(1-G_{m-1,n-1}(v)\\), \\(V=v\\). Ejemplo. \\(X_1,\\dots,X_{6}\\sim N(\\mu_1,\\sigma_1^2)\\), \\(S_X ^2 =30\\), \\(Y_1,\\dots,Y_{21}\\sim N(\\mu_2,\\sigma_2^2)\\), \\(S_Y^2=30\\). La hipótesis nula es \\(H_0: \\sigma_1^2\\leq \\sigma_2^2\\). Se calcula \\(V = \\dfrac{30/5}{40/20} = 3\\) y \\(F^{-1}_{5,20}(1-0.05) = 2.71.\\) El valor-p corresponde a \\(1-G_{5,20}(3) = 0.035.\\) Si \\(\\alpha_0 = 1\\%\\), no rechazo. Si \\(\\alpha_0 = 5\\%\\) rechazo. Ejemplo: Suponga que se tienen los siguientes datos m &lt;- 20 X &lt;- rnorm(n = m, mean = 0, sd = sqrt(6)) head(X) ## [1] 1.2357315 -1.5023828 -0.4425341 4.0979017 1.4732579 0.8937449 n &lt;- 40 Y &lt;- rnorm(n = n, mean = 10, sd = sqrt(2)) head(Y) ## [1] 9.858871 8.104590 7.166432 10.397898 12.673469 8.113331 Es decir tener 20 datos normales con \\(\\sigma_1^2 = 6\\) y 40 datos normales con \\(\\sigma_2^2 = 2\\). En todo caso asuma que \\(\\sigma\\) es desconocidos para cada caso y solo tenemos los datos. Además queremos hacer la prueba de hipótesis \\[ \\begin{array}{ll} H_{0}: &amp; \\sigma_{1}^{2} \\leq \\sigma_{2}^{2} \\\\ H_{1}: &amp; \\sigma_{1}^{2}&gt;\\sigma_{2}^{2} \\end{array} \\] OJO: Según la forma que planteamos el ejercicio, deberíamos de rechazar \\(H_0\\) ya que \\(\\sigma_1^2 = 6 &gt; 2 = \\sigma_2^2\\) Calculamos el estadístico \\(V\\) (S2_X_divido_m_1 &lt;- var(X)) ## [1] 3.863891 (S2_Y_divido_n_1 &lt;- var(Y)) ## [1] 1.541968 (V &lt;- S2_X_divido_m_1 / S2_Y_divido_n_1) ## [1] 2.505818 Para calcular un cuantil te tamaño \\(1-\\alpha = 0.95\\) se usa la siguiente función (qnt &lt;- qf(p = 1 - 0.05, df1 = m - 1, df2 = n - 1)) ## [1] 1.85992 ¿Rechazamos \\(H_0\\)? V &gt; qnt ## [1] TRUE y el valor-\\(p\\) de la prueba es 1 - pf(q = V, df1 = m - 1, df2 = n - 1) ## [1] 0.007493243 Interpretación: Rechazamos la hipótesis que \\(\\sigma_{1}^{2} \\leq \\sigma_{2}^{2}\\) con un valor-\\(p\\) de 0.02. 12.3.1 Prueba de 2 colas (prueba de homocedasticidad) Bajo las hipótesis \\(H_0: \\sigma^2_1=\\sigma^2_2\\) vs \\(H_1: \\sigma^2_1\\ne\\sigma^2_2\\), se rechaza si \\(V\\geq c_2\\) o \\(V\\leq c_1\\) con \\(c_1,c_2\\) tales que \\[\\mathbb P[V\\leq c_1] = \\dfrac{\\alpha_0}{2} \\text{ y } \\mathbb P[V\\geq c_2] = \\dfrac{\\alpha_0}{2} \\implies c_1 = G_{m-1,n-1}^{-1}\\left(\\dfrac{\\alpha_0}{2}\\right) \\text{ y } c_2 = G_{m-1,n-1}^{-1}\\left(1-\\dfrac{\\alpha_0}{2}\\right)\\] Ejemplo. Mismo ejemplo de las nubes. \\[ H_0: \\sigma^{2}_{\\text{con trat.}} = \\sigma^{2}_{\\text{sin trat.}} \\quad vs \\quad H_1: \\sigma^{2}_{\\text{con trat.}} \\neq \\sigma^{2}_{\\text{sin trat.}} \\] (m &lt;- length(con_tratamiento)) ## [1] 26 (n &lt;- length(sin_tratamiento)) ## [1] 26 (S2_X_divido_m_1 &lt;- var(con_tratamiento)) ## [1] 2.558444 (S2_Y_divido_n_1 &lt;- var(sin_tratamiento)) ## [1] 2.695663 (V &lt;- S2_X_divido_m_1 / S2_Y_divido_n_1) ## [1] 0.9490963 \\[V = \\dfrac{\\dfrac{63.96}{25}}{\\dfrac{67.39}{25}} = 0.9491\\] Se tiene que \\(c_1 = G^{-1}_{25,25}(0.0025) = 0.4484\\) y \\(c_2 = G^{-1}_{25,25}(0.975) = 2.23\\). (c1 &lt;- qf(0.025, df1 = m - 1, df2 = n - 1)) ## [1] 0.4483698 (c2 &lt;- qf(0.975, df1 = m - 1, df2 = n - 1)) ## [1] 2.230302 ¿Rechazamos \\(H_0\\)? V &lt; c1 ## [1] FALSE V &gt; c2 ## [1] FALSE No rechazamos la hipótesis nula. Si observamos \\(V=v\\), podemos rechazar si \\[ v\\leq G^{-1}_{m-1,n-1}\\left(\\dfrac{\\alpha_0}2\\right) \\implies 2G_{m-1,n-1}(v)\\leq \\alpha_0 \\] o tambien si \\[v\\geq G^{-1}_{m-1,n-1}\\left(1-\\dfrac{\\alpha_0}2 \\right) \\implies G_{m-1,n-1}(v) \\geq 1-\\dfrac{\\alpha_0}2 \\implies \\alpha_0\\geq 2\\bar G_{m-1,n-1}(v) \\] Por lo tanto, el p-valor es \\[\\text{valor-}p = 2\\min[1-G_{m-1,n-1}(v), G_{m-1,n-1}(v)]\\] 2 * min(1 - pf(q = V, df1 = m - 1, df2 = n - 1), pf(q = V, df1 = m - 1, df2 = n - 1)) ## [1] 0.8971154 Interpretación: La prueba de hipótesis no rechaza la hipótesis de homocedasticidad con un nivel de confianza del 5%. Propiedad. La prueba \\(F\\) es un LRT. –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; –&gt; "],
["bondad-de-ajuste.html", "Capítulo 13 Bondad de ajuste 13.1 Prueba \\(\\chi^2\\) 13.2 Pruebas \\(\\chi^2\\) con hipótesis parametrizadas", " Capítulo 13 Bondad de ajuste En los ejemplos que hemos visto siempre hemos necesitado una distribución de referencia que nos permita calcular los cuantiles correspondientes. Sin embargo, es posible que ese supuesto no se cumpla, o que la distribución no sea normal por ejemplo. En estos casos primero se debe de considerar una prueba de hipótesis para revisar las condiciones de nuestros datos. Una familia de problemas no paramétricos surgen a partir de esta observación. 13.1 Prueba \\(\\chi^2\\) Suponga que se tienen datos categóricos los cuales el rango de la variable asume un número finito de categorías o estados Ejemplo: Por ejemplo suponga que tienen la variable de tipo de sangre en la población, entonces Categoría Tipo de sangre 1 A 2 B 3 AB 4 O Suponga que tenemos \\(k\\) categorías, \\[p_i = \\mathbb P[\\text{Categoría }i],\\;i=1,\\dotsc,k\\] y \\(\\sum_{i=1}^kp_i = 1\\). Sea \\(p_1^0,\\dotsc,p_k^0\\) probabilidades propuestas, \\(\\sum_{i=1}^kp_i^0\\). Suponga \\[ H_0: p_i = p_i^0\\text{ para } i=1,\\dots,k\\] \\[ H_1: p_i \\ne p_i^0\\text{ para al menos un }i\\] Ejemplo: Siguiendo con el ejemplo, suponga que se quiere hacer la siguiente hipótesis de la población Categoría Tipo de sangre Hipótesis (\\(p_i ^{0}\\)) 1 A 1/3 2 B 1/8 3 AB 1/24 4 O 1/2 Suponga una muestra de \\(n\\) elementos. \\(N_i\\) el número de elementos en la categoría \\(i\\), \\(\\sum _{i=1}^kN_i = n\\). Note que \\[(N_1,\\dots,N_k)\\sim\\text{Multinomial}(p_1, \\dots, p_k)\\] Nota: Una distribución multinomial tiene la siguiente forma: \\[\\begin{align*} \\operatorname{Pr}\\left(X_{1}=x_{1}, \\ldots, X_{k}=x_{k}\\right) = \\left\\{\\begin{array}{rlr} \\binom {n} {x_{1}, \\ldots, x_{k}} p_{1}^{x_{1}} \\cdots p_{k}^{x_{k}} &amp; \\text { if } x_{1}+\\cdots+x_{k}=n \\\\ 0 &amp; \\text { otherwise. } \\end{array}\\right. \\end{align*}\\] donde \\[\\begin{equation*} \\binom{n}{x_{1}, \\ldots, x_{k}} =\\frac{n !}{x_{1} ! x_{2} ! \\cdots x_{k} !} \\end{equation*}\\] El número esperado de elementos en la celda \\(i\\) es \\(n\\cdot p_i^0\\). Si \\(N_i -np_i^0\\) es cercano a 0 para todo \\(i\\), es indicador de que \\(H_0\\) es cierto. El estadístico \\(\\chi^2\\) se define como \\[Q = \\sum_{i=1}^k\\dfrac{(N_i-np_i^0)^2}{np_i^0}.\\] En 1900, Karl Pearson probó que cuando \\(n\\) es grande y \\(k\\) es “relativamente” pequeño con respecto a \\(n\\), \\[Q \\xrightarrow[H_0]{}\\chi^2_{k-1}.\\] En la prueba \\(\\chi^2\\), \\(\\delta\\): Rechazo \\(H_0\\) si \\(Q\\geq c\\). Dado un nivel de significancia \\(\\alpha_0\\), \\[\\mathbb P_{H_0}[Q\\geq c]\\le \\alpha_0\\implies c = F^{-1}_{\\chi^2_{k-1}}(1-\\alpha_0)\\] Nota: El estadístico \\(Q\\) se puede interpretar de la forma \\[Q = \\sum_{i=1}^k\\dfrac{(\\text{observado}_{i} - \\text{esperado}_{i})^2}{\\text{esperado}_{i}}.\\] Reglas empíricas La aproximación \\((Q\\sim\\chi^{k-1})\\) funciona muy bien si \\(np_i^0\\geq 5\\). La aproximación es buena si \\(np_i^0\\ge 1.5\\), \\(i=1,\\dots,k\\). Ejemplo: Continuando con el ejemplo se observan 6004 personas de raza blanca en California y se obtiene este resultado \\(\\text{Grupo}\\) \\(\\text{Observado}\\) (\\(n_i\\)) \\(\\text{Teórico}\\) (\\(p_i ^{0}\\)) \\(\\text A\\) \\(2162\\) \\(1/3\\) \\(\\text B\\) \\(738\\) \\(1/8\\) \\(\\text {AB}\\) \\(228\\) \\(1/24\\) \\(\\text O\\) \\(2876\\) \\(1/2\\) Queremos probar \\(H_0: p_i = p_i^0\\), \\(i=1,2,3,4\\). \\(np_1^0 = 6004\\cdot1/3 = 2001.3\\). \\(np_2^0 = 6004\\cdot1/8 = 750.5\\). \\(np_3^0 = 6004\\cdot1/24 = 250.2\\). \\(np_4^0 = 6004\\cdot1/2 = 3002\\). \\[Q = \\dfrac{(2162-2001.3)^2}{2001.3} + \\dfrac{(738-750.5)^2}{750.5} + \\dfrac{(228-250.2)^2}{250.2} + \\dfrac{(2876-3002)^2}{3002} = 20.37.\\] El valor-p es \\(F_{\\chi^2_{3}}(20.37) = 1.42\\times 10^{-4}\\). En R el test se puede hacer con la función chisq.test: observado &lt;- c(2162, 738, 228, 2876) probabilidad_hipotetica &lt;- c(1 / 3, 1 / 8, 1 / 24, 1 / 2) chisq.test(x = observado, p = probabilidad_hipotetica) ## ## Chi-squared test for given probabilities ## ## data: observado ## X-squared = 20.359, df = 3, p-value = 0.000143 Rechazamos la hipótesis de que las probabilidades teóricas de tipo de sangre son igual al valor hipotético. Ejemplo. Sean \\(0&lt;X_i&lt;1\\), \\(i=1,2,\\dots,100\\). \\(X_i~f\\), \\(f\\) una densidad continua. \\[H_0: f=\\text{Unif}(0,1) \\text{ vs } H_1: f \\ne\\text{Unif}(0,1). \\] Se definen 20 niveles, que corresponden a intervalos de [0,1]. Una observación \\(X_j\\) está en el nivel \\(i\\) si \\[\\dfrac{i-1}{20}\\leq X_j &lt;\\dfrac{i}{20}\\]. \\(\\text{Nivel}\\) \\(1\\) \\(2\\) \\(\\cdots\\) \\(20\\) \\(\\text{Frecuencia}\\) \\(N_1\\) \\(N_2\\) \\(\\cdots\\) \\(N_{20}\\) donde \\(N_i\\) es el número de observaciones que están en el intervalo \\(i\\). \\(i\\) \\(X_i\\) \\(\\text{Grupo}\\) \\(1\\) \\(X_1\\) \\(2\\) \\(2\\) \\(X_2\\) \\(4\\) \\(3\\) \\(X_3\\) \\(17\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(100\\) \\(X_{100}\\) \\(20\\) Las hipótesis anteriores son equivalentes a \\[H_0: p_i = \\dfrac{1}{20}, \\;i=1,\\dots,20.\\] \\(np_i^0 = 100\\cdot\\dfrac 1{20} = 5,\\;i = 1,\\dots,20\\). Entonces \\[Q = \\sum_{i=1}^{20}\\dfrac{(N_i-5)^2}{5}.\\] Rechazamos la hipótesis \\(f = \\text{Unif}(0,1)\\) si \\(Q&gt;\\chi^2_{19}(1-\\alpha_0)\\). Nota: Este método funciona para cualquier tipo de distribución. El siguiente procedimiento se debe seguir para estos casos Particione en \\(k\\) subintervalos disjuntos la recta real o cualquier intervalo en el que esté contenidos sus datos de modo que este tenga probalidad 1. Es decir, todos sus datos deben estar contenidos en este intervalo. Determine las probabilidades \\(p_i ^{0}\\) hipotéticas que se asignará cada subintervalo. El valor teórico para cada subintervalo será \\(n p_i ^{0}\\) Cuente las observaciones que caen en cada subintervalo. Llame este valor \\(N_i\\) Calcule \\(Q\\) según el procedimiento anterior y tome una decisión con respecto a la hipótesis nula. La hipótesis nula deberá tener una distribución \\(\\chi ^{2}\\) con \\(k-1\\) grados de libertad. Ejemplo. Supongamos que tenemos 23 datos de unas partes mecánicas para automóvil. Se registró el sus tiempos de vida útil. Trabajemos con el ejemplo de log-tiempo de vida de los dispositivos. x &lt;- c( 17.88, 28.92, 33, 41.52, 42.12, 45.6, 48.8, 51.84, 51.96, 54.12, 55.56, 67.8, 68.44, 68.64, 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92, 128.04, 173.4 ) log_x &lt;- log(x) hist(x) hist(log_x) Suponga que se quiere hacer la prueba de hipótesis \\[H_0: f = N(\\log(50),0.25) \\quad vs \\quad H_1: f \\neq N(\\log(50),0.25)\\] Seleccione \\(k\\) tal que \\[ p_i^0 = \\mathbb P[\\text{log-tiempo perteneza al }i\\text{-ésimo intervalo}]\\geq \\dfrac 5{23}\\approx \\dfrac 14. \\] Podemos tomar \\(k = 4\\) grupos (intervalos regulares) Grupo 1: \\((F^{-1}_{H_0}(0),F^{-1}_{H_0}(0.25)] = (-\\infty,3.575]\\). Grupo 2: \\((F^{-1}_{H_0}(0.25),F^{-1}_{H_0}(0.5)] = (3.575,3.912]\\). Grupo 3: \\((F^{-1}_{H_0}(0.5),F^{-1}_{H_0}(0.75)]=(3.912,4.249]\\). Grupo 4: \\((F^{-1}_{H_0}(0.75),F^{-1}_{H_0}(1))=(4.249,+\\infty)\\). Entonces solo para efectos de construir la partición cortes &lt;- qnorm( p = c(0, 1 / 4, 2 / 4, 3 / 4, 1), mean = log(50), sd = sqrt(0.25) ) (intervalos &lt;- cut(c(0, 10), breaks = cortes)) ## [1] (-Inf,3.57] (4.25, Inf] ## Levels: (-Inf,3.57] (3.57,3.91] (3.91,4.25] (4.25, Inf] (conteos &lt;- cut(log_x, breaks = cortes)) ## [1] (-Inf,3.57] (-Inf,3.57] (-Inf,3.57] (3.57,3.91] (3.57,3.91] (3.57,3.91] ## [7] (3.57,3.91] (3.91,4.25] (3.91,4.25] (3.91,4.25] (3.91,4.25] (3.91,4.25] ## [13] (3.91,4.25] (3.91,4.25] (3.91,4.25] (4.25, Inf] (4.25, Inf] (4.25, Inf] ## [19] (4.25, Inf] (4.25, Inf] (4.25, Inf] (4.25, Inf] (4.25, Inf] ## Levels: (-Inf,3.57] (3.57,3.91] (3.91,4.25] (4.25, Inf] summary(conteos) ## (-Inf,3.57] (3.57,3.91] (3.91,4.25] (4.25, Inf] ## 3 4 8 8 \\(G_1\\) \\(G_2\\) \\(G_3\\) \\(G_4\\) \\(3\\) \\(4\\) \\(8\\) \\(8\\) \\[Q = \\dfrac{(3-23\\cdot1/4)^2}{23\\cdot 1/4} + \\dfrac{(4-23\\cdot1/4)^2}{23\\cdot 1/4}+\\dfrac{(8-23\\cdot1/4)^2}{23\\cdot 1/4} + \\dfrac{(8-23\\cdot1/4)^2}{23\\cdot 1/4}= 3.609.\\] El valor-\\(p\\) corresponde a \\(F_{\\chi^2_3}(3.609) = 0.307\\). conteos &lt;- summary(conteos) chisq.test(conteos) ## ## Chi-squared test for given probabilities ## ## data: conteos ## X-squared = 3.6087, df = 3, p-value = 0.3069 Nota: La función chisq.test si no se llama con ninguna hipótesis nula p, esta asume que \\(p = 1/n\\) para cada categoría. En este caso como son 4 categorías sería \\(1/4\\). Con un nivel de 30%, no se rechaza la hipótesis de normalidad bajo esa escogencia de parámetros. Nota: Otra escogencia de paramétros podría aceptar la hipótesis nula. 13.2 Pruebas \\(\\chi^2\\) con hipótesis parametrizadas Ejemplo. En el caso anterior, probamos que la distribución Normal con media \\(\\log(50) = 3.912023\\) y desviación estándar \\(0.25\\) no funcionabapara nuestros datos. La pregunta es entonces, ¿Cuáles serían los parámetros correctos? ¿Los datos pertenecen a una familia normal? En esta sección veremos una técnica para lidiar con este problema. Escriba cada \\(p_i\\) \\((i=1,\\dots,k)\\) como \\[p_i = \\pi_i(\\theta),\\quad \\theta = (\\theta_1,\\dots,\\theta_s)\\] Es decir, cada probabilidad es igual a una función particular con respecto a algunos paramétros \\(\\mathbf{\\theta}\\). Asuma que \\(s&lt;k-1\\). Las entradas de \\(\\theta\\) no se pueden escribir como función de ellas mismas. Además \\(\\sum \\pi_i(\\theta) = 1\\). \\[H_0: p_i = \\pi_i(\\theta)\\text{ para algún parámetro }\\theta\\in \\Omega,\\;i=1,\\dots,k\\] \\[H_1: \\text{lo anterior no es cierto}\\] El estadístico es \\[Q = \\sum_{i=1}^k\\dfrac{[N_i-n\\pi_i(\\hat\\theta)]^2}{n\\pi_i(\\hat\\theta)}\\] con \\(\\hat\\theta\\) el MLE de \\(\\theta\\) usando la distribución de \\((N_1,\\dots,N_k)\\). Teorema. Bajo \\(H_0\\), conforme \\(n\\to \\infty\\), \\(Q\\to \\chi^2_{k-1-s}\\). Ejemplo. Suponga que se tienen 3 grupos y defina una parámetro \\(0&lt;\\theta&lt;1\\). El análisista hace el siguiente supuesto: \\[\\begin{align*} p_1 &amp;= \\theta^2=\\pi_1(\\theta) , \\\\ p_2 &amp;= 2\\theta(1-\\theta)=\\pi_2(\\theta),\\\\ p_3 &amp;= (1-\\theta)^2=\\pi_3(\\theta). \\end{align*}\\] Se observa que \\(p_1+p_2+p_3 = \\theta^2 + 2\\theta (1-\\theta +(1-\\theta)^2 =[\\theta+(1-\\theta)]^2 = 1\\). \\(s = 1\\), \\(\\Omega = [0,1]\\). Como la distribución de \\((N_1,\\dots,N_k)\\underset{H_0}{\\sim} \\text{Multinomial}(n,p_1,\\dots,p_k)\\), se obtiene la verosimilitud \\[L (\\theta|N_1,\\dots,N_k) = {n \\choose {N_1\\cdots N_k}}(\\pi_1(\\theta))^{N_1}\\cdots(\\pi_k(\\theta))^{N_k}\\] \\[\\ell = \\log (L) \\propto N_1\\ln\\pi_1(\\theta)+\\cdots+N_k\\ln\\pi_k(\\theta)\\] Retomando el ejemplo, \\[\\begin{align*} \\ln L(\\theta) &amp; \\propto N_1\\ln \\theta^2 + N_2 \\ln 2\\theta(1-\\theta) + N_3\\ln (1-\\theta)^2\\\\ &amp; = (2N_1+N_2)\\ln \\theta + (2N_3+N_2)\\ln(1-\\theta) + N_2\\ln 2 \\end{align*}\\] \\[\\dfrac{\\partial \\ln L(\\theta)}{\\partial\\theta} = \\dfrac{2N_1+N_2}{\\theta}-\\dfrac{2N_3+N_2}{1-\\theta} = 0 \\implies \\hat\\theta = \\dfrac{2N_1+N_2}{2n}\\] Con esto se calcula \\(\\pi_1(\\hat \\theta)\\),\\(\\pi_2(\\hat \\theta)\\),\\(\\pi_3(\\hat \\theta)\\) y \\(Q\\). Ejemplo (Partes de automóvil). Sean \\(X_1,\\dots,X_n\\sim f\\), \\(H_0: f = N(\\mu,\\sigma^2)\\) donde \\(\\mu\\) y \\(\\sigma^2\\) son desconocidos. Vamos a construir las funciones \\(\\pi\\) tratando de ajustar los cuantiles que habíamos definido antes con los valores teóricos de \\(\\mu\\) y \\(\\sigma\\). Entonces, \\[\\pi_i(\\mu,\\sigma^2) = \\int_{a_i}^{b_i}(2\\pi\\sigma^2)^{-1/2}\\exp\\left(-\\dfrac 1{2\\sigma^2}(x-\\mu)^2\\right)dx = \\Phi\\left(\\dfrac{b_i-\\mu}{\\sigma}\\right)-\\Phi\\left(\\dfrac{a_i-\\mu}{\\sigma}\\right)\\] Asumiendo que la i-ésima partición es \\((a_i,b_i)\\), los 4 intervalos son \\[(-\\infty,3.575],(3.575,3.912],(3.912,4.249], (4.249,+\\infty).\\] La verosimilitud es \\[\\ln L(\\mu,\\sigma^2) = N_1\\ln \\pi_1(\\mu,\\sigma^2)+\\cdots+N_4\\ln\\pi_4(\\mu,\\sigma^2)\\] y se optimiza numéricamente. cortes &lt;- qnorm( p = c(0, 1 / 4, 2 / 4, 3 / 4, 1), mean = log(50), sd = sqrt(0.25) ) log_versomilitud &lt;- function(par, cortes, log_x) { G &lt;- length(cortes) mu &lt;- par[1] sigma &lt;- par[2] pi &lt;- numeric() for (k in 1:(G - 1)) { pi[k] &lt;- pnorm(q = cortes[k + 1], mean = mu, sd = sigma) - pnorm(q = cortes[k], mean = mu, sd = sigma) } conteos &lt;- cut(log_x, breaks = cortes) conteos &lt;- summary(conteos) l &lt;- -sum(conteos * log(pi)) return(l) } sol &lt;- optim( par = c(0, 1), fn = log_versomilitud, cortes = cortes, log_x = log_x ) sol$par ## [1] 4.0926455 0.4331326 Para otra solución, considere el siguiente teorema: Teorema (1954). \\(X_1,\\dots, X_n\\sim F_\\theta\\), \\(\\theta: p\\)-dimensional. \\(\\hat\\theta_n\\) es el MLE de \\(\\theta\\) (basado en \\(X_1,\\dots, X_n\\)). Tome una partición de \\(\\mathbb R\\) con \\(k\\) intervalos disjuntos \\((I_1,\\dots,I_k)\\). \\(N_i\\) es la cantidad de \\(X_i\\)’s que pertenecen a \\(I_i\\) y \\(\\pi_i(\\theta)=\\mathbb P_\\theta[X_i\\in I_i]\\), \\[Q&#39; = \\sum_{i=1}^k\\dfrac{[N_i-n\\pi_i(\\hat\\theta_n)]^2}{n\\pi_i(\\hat\\theta_n)}\\] Bajo las condiciones de regularidad del MLE, si \\(n\\to\\infty\\), el cdf de \\(Q&#39;\\) bajo \\(H_0\\) está entre \\(\\chi^2_{k-p-1}\\) y \\(\\chi^2_{k-1}\\). Del ejemplo anterior (tiempo de vida de los dispositivos), tome \\(\\hat\\mu = \\bar X_n =4.1506137\\) y \\(\\hat\\sigma^2 = \\dfrac{s_n^2}{n} = 0.5332049\\). \\(\\pi_1(\\hat\\mu,\\hat\\sigma^2) = \\Phi\\left(\\dfrac{3.575-4.15}{0.2843^{1/2}}\\right)-\\Phi(-\\infty) = 0.14\\). \\(\\pi_2(\\hat\\mu,\\hat\\sigma^2) = \\Phi\\left(\\dfrac{3.912-4.15}{0.2843^{1/2}}\\right) - \\Phi\\left(\\dfrac{3.575-4.15}{0.2843^{1/2}}\\right) = 0.187\\). \\(\\pi_3(\\hat\\mu,\\hat\\sigma^2) = \\Phi\\left(\\dfrac{4.249-4.15}{0.2843^{1/2}}\\right) - \\Phi\\left(\\dfrac{3.912-4.15}{0.2843^{1/2}}\\right) = 0.246\\). \\(\\pi_4(\\hat\\mu,\\hat\\sigma^2) = 1 - \\Phi\\left(\\dfrac{4.249-4.15}{0.2843^{1/2}}\\right) = 0.4266\\). Es decir podemos calcular lo siguiente en R G &lt;- length(cortes) mu &lt;- mean(log_x) sigma &lt;- sd(log_x) pi &lt;- numeric() for (k in 1:(G - 1)) { pi[k] &lt;- pnorm(q = cortes[k + 1], mean = mu, sd = sigma) - pnorm(q = cortes[k], mean = mu, sd = sigma) } pi ## [1] 0.1400819 0.1871877 0.2461242 0.4266062 chisq.test(conteos, p = pi) ## ## Chi-squared test for given probabilities ## ## data: conteos ## X-squared = 1.3381, df = 3, p-value = 0.7201 Entonces \\[Q&#39; = \\dfrac{(3-23\\cdot 0.14)^2}{23\\cdot 0.14} + \\dfrac{(4-23\\cdot 0.187)^2}{23\\cdot 0.187} + \\dfrac{(8-23\\cdot 0.246)^2}{23\\cdot 0.246} +\\dfrac{(8-23\\cdot 0.4266)^2}{23\\cdot 0.4266} = 1.3381.\\] \\(\\text{valor-}p_1 = F_{\\chi^2_{4-2-1}}(1.3381) = 0.7526307\\). \\(\\text{valor-}p_2 = F_{\\chi^2_{4-1}}(1.3381) = 0.2798937\\). Rechazamos \\(H_0\\) (hipótesis de normalidad) si \\(\\alpha_0&lt;0.2798\\). ggplot(data = data.frame(x = c(2.5, 6)), aes(x)) + geom_histogram( data = data.frame(x = log_x), aes(x, y = ..density..), color = &quot;white&quot; ) + stat_function( fun = dnorm, args = list(mean = log(50), sd = sqrt(0.25)), aes(color = &quot;Hipótesis Manual&quot;), size = 2 ) + stat_function( fun = dnorm, args = list(mean = sol$par[1], sd = sol$par[2]), aes(color = &quot;Hipótesis con Optimización&quot;), size = 2 ) + stat_function( fun = dnorm, args = list(mean = mean(log_x), sd = sd(log_x)), aes(color = &quot;Hipótesis con MLE&quot;), size = 2 ) + theme_minimal() Ejemplo. Suponga que se tiene el número de muertes por patadas de caballo en el ejercito Prusiano. \\(\\text{Conteos}\\) \\(0\\) \\(1\\) \\(2\\) \\(3\\) \\(\\ge 4\\) \\(\\text{Total}\\) \\(\\text{Núm. de obs.}\\) \\(144\\) \\(91\\) \\(32\\) \\(11\\) \\(2\\) \\(280\\) ¿Será la variable Poisson? df &lt;- data.frame( conteos = c(0, 1, 2, 3, 4), observaciones = c(144, 91, 32, 11, 2) ) ggplot(df, aes(x = conteos, y = observaciones)) + geom_col() + theme_minimal() \\(H_0: f = \\text{Poisson}(\\theta), \\theta&gt;0\\). El MLE de \\(\\hat\\theta\\) es \\[\\dfrac{0\\cdot 144+1\\cdot91+2\\cdot32+3\\cdot 11+2\\cdot4}{280} = \\dfrac{196}{280} = 0.7\\] \\(\\pi_1(\\hat\\theta) = e^{-\\hat\\theta} = e^{-0.7}=0.4966\\). \\(\\pi_2(\\hat\\theta) = \\dfrac{e^{-\\hat\\theta}\\hat\\theta}{1!} = 0.3476\\). \\(\\pi_3(\\hat\\theta) = \\dfrac{e^{-\\hat\\theta}\\hat\\theta^2}{2!} = 0.1217\\). \\(\\pi_4(\\hat\\theta) = \\dfrac{e^{-\\hat\\theta}\\hat\\theta^3}{3!} = 0.0283\\). \\(\\pi_5(\\hat\\theta) = \\bar F_{\\text{Poisson}(\\hat\\theta)}(4) = 0.0058\\) \\[\\begin{align*} Q&#39; &amp; = \\dfrac{(144-280\\cdot0.4966)^2}{280\\cdot0.4966}+\\dfrac{(91-280\\cdot0.3476)^2}{280\\cdot0.3476}+\\dfrac{(32-280\\cdot0.1217)^2}{280\\cdot0.1217}\\\\ &amp; +\\dfrac{(11-280\\cdot0.0283)^2}{280\\cdot0.0283} +\\dfrac{(2-280\\cdot0.0058)^2}{280\\cdot0.0058} = 1.979. \\end{align*}\\] \\(\\text{valor-}p_1 = F_{\\chi^2_{5-1-1}}(1.979) = 0.5768\\). \\(\\text{valor-}p_2 = F_{\\chi^2_{5-1}}(1.979) = 0.7396\\). Interpretación: con un nivel de significancia del 5% no rechazamos la hipótesis Poisson en los datos. total &lt;- sum(df$observaciones) ggplot(data = data.frame(x = c(0, 4)), aes(x)) + geom_col(data = df, aes(x = conteos, y = observaciones / total)) + stat_function( fun = dpois, args = list(lambda = 0.7), aes(color = &quot;Hipótesis con MLE&quot;), size = 2, geom = &quot;col&quot; ) + theme_minimal() "],
["tablas-de-contingencia.html", "Capítulo 14 Tablas de contingencia 14.1 Prueba de independencia 14.2 Prueba de homogeneidad 14.3 Similitudes entre las pruebas de independecia y homogeneidad 14.4 Comparación de dos o más proporciones 14.5 Paradoja de Simpson", " Capítulo 14 Tablas de contingencia En este capítulo veremos cómo hacer un prueba para determinar si dos variables categóricas son independientes. Ejemplo. Considere una muestra de 200 estudiantes de una población universitaria, según currículum (área de estudio) y candidato preferido en una elecciones universitarias (A, B o indeciso). Área/Candidato A B Indeciso Totales Ingeniería 24 23 12 59 Humanidades 24 14 10 48 Artes 17 8 13 38 Administración 27 19 9 55 Totales 92 64 44 200 Antes de continuar con este ejemplo, vamos definimos lo siguiente: Tabla de contingencia: arreglo en donde cada observación se puede clasificar de dos o más formas. Declaramos la siguiente notaciones: \\(R\\) = El número de filas en la tabla. \\(C\\) = El número de columnas en la tabla. \\(N_{ij}\\) = número de individuos en la muestra clasificados en la fila \\(i\\) y \\(\\displaystyle N_{i+} = \\sum_{j=1}^CN_{ij}\\). \\(\\displaystyle N_{+j} = \\sum_{i=1}^RN_{ij}\\). \\(\\displaystyle \\sum_{i=1}^R\\sum_{j=1}^C N_{ij} = n\\). \\(p_{ij}\\) = \\(\\displaystyle\\mathbb P[\\text{Individuo en la población pertenezca a la celda }i,j]\\), \\(i=1,\\dots,R\\); \\(j=1,\\dots, C\\). \\(p_{i+}\\) = \\(\\displaystyle\\mathbb P[\\text{Individuo se clasifique en la fila }i] = \\sum_{j=1}^C p_{ij}\\). \\(p_{+j}\\) = \\(\\displaystyle\\mathbb P[\\text{Individuo se clasifique en la fila }j] = \\sum_{i=1}^R p_{ij}\\). columna \\(j\\). \\(\\displaystyle \\sum_{i=1}^R\\sum_{j=1}^Cp_{ij} = 1\\). Por ejemplo para la tabla anterior \\(N_{11} = 24\\) son los estudiantes de ingeniería que van a votar por el candidatos A. \\(N_{2+} = 48\\) son todos los estudiantes de Humanidades. \\(N_{+3} = 44\\) son todos los estudiantes indecisos para votar. \\(n = N_{++} = 200\\) es cualquier estudiante. 14.1 Prueba de independencia La hipótesis nula que queremos probar es: \\[H_0: p_{ij} = p_{i+}\\cdot p_{+j},\\;i=1,\\dots,R \\; ;j=1,\\dots, c\\] Es decir, que las probabilidades conjuntas de la tabla es el producto de las probabilidades individuales, i.e., que ambas variables son independientes. Vectorizando la tabla de contingencia se puede utilizar la hipótesis de distribución multinomial. El número de celdas es \\(k=RC\\). El número de parámetros bajo \\(H_0\\) es \\(R-1+C-1 = R+C-2\\). El MLE corresponde a \\(\\hat p_{i+} = \\dfrac{N_{i+}}{n}\\) y \\(\\hat p_{+j} = \\dfrac{N_{+j}}{n}\\). El MLE del conteo en la celda \\(i,j\\) (valor esperado bajo \\(H_0\\)) es \\[\\hat E_{ij} = n\\hat p_{i+} \\hat p_{+j} = n\\dfrac{N_{i+}}{n}\\dfrac{N_{+j}}{n} = \\dfrac{N_{i+}N_{+j}}{n}.\\] El estadístico \\(\\chi^2\\) se calcula como \\[Q = \\sum_{i=1}^R\\sum_{j=1}^C \\dfrac{(N_{ij}-\\hat E_{ij})^2}{\\hat E_{ij}} \\underset{n\\text{ grande, }H_0}{\\sim} \\chi^2_{k-s-1}\\] donde \\(k-s-1 = RC-(R+C-2)-1 = (R-1)(C-1).\\) Dado \\(\\alpha_0\\), rechazamos \\(H_0\\) si \\(Q&gt;\\chi^2_{(R-1)(C-1)}(1-\\alpha_0)\\). Del ejemplo anterior, \\[\\hat E_{11} = \\dfrac{59\\cdot92}{200} = 27.14\\] \\[\\hat E_{32} = \\dfrac{38\\cdot64}{200} = 12.165\\] La tabla de valores esperados bajo \\(H_0\\) es Área/Candidato A B Indeciso Totales Ingeniería 27.14 18.88 12.98 59 Humanidades 22.08 15.36 10.56 48 Artes 17.48 12.16 8.36 38 Administración 25.30 17.60 12.10 55 Totales 92 64 44 200 \\[Q = \\dfrac{(24-27.14)^2}{27.14} + \\cdots+\\dfrac{(8-12.16)^2}{12.16}+\\cdots = 6.68\\] El valor-p es \\(\\bar F_{\\chi^2_6}(6.68) = 0.3\\). Rechazamos la hipótesis de independencia entre el currículum y la preferencia electoral con un nivel de significancia del 10%. En R este análisis se hace con la función chisq.test Primero definamos la tabla de datos M &lt;- as.table(rbind( c(24, 23, 12), c(24, 14, 10), c(17, 8, 13), c(27, 19, 9) )) dimnames(M) &lt;- list( Carrera = c( &quot;Ingeniería&quot;, &quot;Humanidades&quot;, &quot;Artes&quot;, &quot;Administración&quot; ), Voto = c(&quot;A&quot;, &quot;B&quot;, &quot;Indeciso&quot;) ) ## knitr::kable(M) Luego el test se ejecuta sobre la matriz de los datos chisq.test(M) ## ## Pearson&#39;s Chi-squared test ## ## data: M ## X-squared = 6.6849, df = 6, p-value = 0.351 14.2 Prueba de homogeneidad Suponga que seleccionamos algunos individuos de distintas poblaciones y cada uno de ellos se le observa una variable aleatoria discreta. La pregunta que nos interesa es ver si la distribución de esa variable es igual a través de cada una de las poblaciones. Ejemplo: Siguiendo con el ejemplo anterior, se toma muestras de tamaño 59,48,38 y 55 de cada área. ¿La distribución de la variable preferencia es la misma sin importar el área? Es decir, la forma en que votan los estudiantes es homogénea sin importar la carrera que cursan? Sean \\(R\\) el número de individuos por población y \\(C\\) el número de tipos de celdas por población. Recuerde que definimos \\[p_{ij} = \\mathbb P[\\text{una observación pertenece a la }i\\text{-ésima población y a la categoría }j]\\] Estas probabilidades cumplen \\(\\sum_{j=1}^Cp_{ij} = 1\\), \\(i=1,\\dots,R\\). La hipótesis de homogeneidad es \\[H_0: p_{1j} = p_{2j} = \\dots = p_{Rj} \\text{ para }j=1,\\dots, C\\] para una fila fija \\(i\\) y probabilidades \\(p_{ij}\\) conocidas: \\[Q^{(i)}=\\sum_{j=1}^c\\dfrac{(N_{ij}-N_{i+}p_{ij})}{N_{i+}p_{ij}}\\] Bajo \\(H_0\\) \\(Q^{(i)}\\sim \\chi^2_{C-1}\\). Supuesto. Las poblaciones son independientes. Esto implica que \\(Q^{(i)}\\) son variables independientes y \\[Q = \\sum_{i=1}^R\\sum_{i=1}^C\\dfrac{(N_{ij}-N_{i+}p_{ij})^2}{N_{i+}p_{ij}}\\sim \\chi^2_{R(C-1)}\\] Nota: Este resultado viene del hecho que sumas de variables \\(\\chi ^{2}\\) son igualmente variables \\(\\chi ^{2}\\) con la suma de sus grados de libertad. Como los valores \\(p_{ij}\\) no son conocidos, tenemos que estimarlos (MLE sobre \\(N_{ij}\\) y sobre \\(H_0\\)): \\(\\hat p_{ij} = \\dfrac{N_{+j}}{n}\\). Sustituyendo, \\[Q = \\sum_{i=1}^R\\sum_{j=1}^C\\dfrac{(N_{ij}-\\hat E_{ij})^2}{\\hat E_{ij}}\\] donde \\(\\hat E_{ij} = \\dfrac{N_{i+}\\cdot N_{+j}}{n}\\). Los grados de libertad de la prueba son \\[k-1-s = R(C-1)-(C-1) = (R-1)(C-1).\\] Se rechaza \\(H_0\\) bajo el mismo criterio de la prueba de independencia. 14.3 Similitudes entre las pruebas de independecia y homogeneidad La prueba de independencia y de homogeneidad se estiman exactamente igual. Sin embargo la interpretación de ambos es ligeramente diferente. Para la prueba de independencia, analiza la hipótesis analiza si la distribución condicional de las columnas y las filas son iguales. Sin embargo, para la prueba de homogeneidad cada fila es considerada como una subpoblación y se desea analizar si para cada una de esta subpoblaciones la distribución de las columnas son iguales. 14.4 Comparación de dos o más proporciones Ejemplo. Suponga que se hace una encuesta y se pregunta si vieron cierto programa o no en varias ciudades. Entonces se tiene la siguiente tabla. Ciudad Vio el programa No lo vio \\(1\\) \\(N_{11}\\) \\(N_{12}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(R\\) \\(N_{R_1}\\) \\(N_{R2}\\) ¿La proporción de audiencia es la misma en todas las ciudades? \\[Q = \\sum_{i=1}^R\\sum_{j=1}^2\\dfrac{(N_{ij}-\\hat E_{ij})^{2}}{\\hat E_{ij}}\\underset{H_0}\\sim \\chi^2_{R-1}\\] donde \\(H_0\\) es la hipótesis de homogeneidad. Ejemplo. 100 personas se seleccionan aleatoriamente en una ciudad. Se les pregunta si los bomberos trabajan bien. Ocurre un incendio y después se les pregunta lo mismo. Satisfactoria No satisfactoria Antes del incendio 80 20 Después del incendio 72 28 En este caso no es apropiado hacer una prueba de independencia u homogeneidad \\(\\chi ^{2}\\) porque se sabe directamente que los datos son correlacionados. Lo que se debe reponder las siguientes preguntas: ¿Cuál es la proporción de personas en la ciudad cambió su opinión sobre el servicio de bomberos después de un incendio? ¿Cuál cambio de opinión fue la más predominante entre los que cambiaron de decisión? Se puede enfocar el análisis usando una tabla de confusión: Después del incendio Satisfactoria No satisfactoria Antes del incendio Satisfactorio 70 10 No Satisfactorio 2 28 Esta tabla sigue la estructura basada en el siguiente esquema. Modelo predictivo A B Observado A Precisión Error B Error Precisión En este caso tenemos que el MLE \\(\\hat{theta}\\) de la proporción de personas que cambiaron de opinión es \\(\\dfrac{12}{100} = 0.12\\). Ya que 10+2 = 12 fueron las personas que cambiaron de opinión y teniamos 100 personas encuestadas. De las personas que cambiaron de opinión \\(\\dfrac{10}{12} = \\dfrac{5}{6} = 0.83\\) fueron las que cambiaron la opinión de satisfactoria insatisfactoria. Esto nos permite dar inferencias sobre el comportamiento general de la población. 14.5 Paradoja de Simpson Cuando tabulamos datos discretos, hay que tener cuidado con la forma en que agrupamos estos. Ejemplo Hacemos un experimento para comparar dos tratamiento (nuevo y viejo). La muestra fue de 80 sujetos, a los cuales 40 se les aplicó el tratamiento nuevo y a 40 el viejo. Se evalúa la evolución de cada paciente. Mejoró No mejoró % mejora Nuevo 20 20 50 Viejo 24 16 60 Conclusión: el tratamiento viejo tiene un porcentaje de mejora mayor. Si vemos estos resultados según el sexo, para hombres Mejoró No mejoró % mejora Nuevo 12 18 40 Viejo 3 7 30 y para mujeres Mejoró No mejoró % mejora Nuevo 8 2 80 Viejo 21 9 70 A este proceso de separar las tablas se le conoce como desagregación. Paradoja de Simpson. Desagregar tablas de contingencia pone en evidencia variables “ocultas” dentro de los datos. La variable “sexo” influye en la capacidad de recuperación. Las mujeres se recuperan más rápido que los hombres en cualquiera de los procedimientos. Además la mayoría de las mujeres recibieron la mayor parte del tratamiento viejo, mientras que la mayoría de los hombres recibieron la mayor parte del tratamiento nuevo. Sin embargo, proporcionalmente el efecto total es mayor porque los hombres tuvieron una mayor influencia globalmente con respecto a la efectividad del tratamiento viejo. Nota. La paradoja puede persistir en muestras grandes. Es decir, se puede obtener este mismo resultado con bases de datos grandes o pequeñas. El problema no es la escala de los datos pero si su proporción. 14.5.1 ¿Cómo evitamos esta paradoja? Hay un par de condiciones que se deben cumplir para evitar problemas en este caso Considere los eventos: “Hombre” si se selecciona a un hombre. “\\(\\text{Hombre}^c\\)” si se selecciona a una mujer. “Nuevo” si es el tratamiento nuevo. “Mejora” si hubo una mejora en el tratamiento. La paradora de Simpson nos dice que es posible tener las siguientes desigualdades: \\[\\mathbb P[\\text{Mejora}|\\text{Hombre}\\cap \\text{Nuevo}]&gt;\\mathbb P[\\text{Mejora}|\\text{Hombre}\\cap \\text{Nuevo}^c] \\hfill (\\star)\\] \\[\\mathbb P[\\text{Mejora}|\\text{Hombre}^c\\cap \\text{Nuevo}]&gt;\\mathbb P[\\text{Mejora}|\\text{Hombre} ^c\\cap \\text{Nuevo}^c] \\hfill (\\star\\star)\\] \\[\\mathbb P[\\text{Mejora}|\\text{Nuevo}]&lt;\\mathbb P[\\text{Mejora}|\\text{Nuevo}^c]\\hfill (\\star\\star\\star)\\] Si tenemos el supuesto que \\[ \\mathbb P[\\text{Hombre}|\\text{Nuevo}] = \\mathbb P[\\text{Hombre}|\\text{Nuevo}^c]\\hfill (\\triangle) \\] entonces \\[\\begin{align*} \\mathbb P[\\text{Mejora}|\\text{Nuevo}] &amp; = \\mathbb P[\\text{Mejora}|\\text{Hombre} \\cap \\text{Nuevo}] \\cdot \\mathbb P[\\text{Hombre}|\\text{Nuevo}] \\\\ &amp; \\quad + \\mathbb P[\\text{Mejora}|\\text{Hombre}^c \\cap \\text{Nuevo}] \\cdot \\mathbb P[\\text{Hombre}^c|\\text{Nuevo}] \\\\ \\intertext{Usando $(\\star)$ y $(\\star\\star)$, vemos que tenemos la desigualdad} &amp; &gt; \\mathbb P[\\text{Mejora}|\\text{Hombre}\\cap \\text{Nuevo}^c] \\cdot \\mathbb P[\\text{Hombre}|\\text{Nuevo}] \\\\ &amp; \\quad + \\mathbb P[\\text{Mejora}|\\text{Hombre}^c\\cap\\text{Nuevo}^c] \\cdot \\mathbb P[\\text{Hombre}^c|\\text{Nuevo}]\\\\ \\intertext{Usando el supuesto en ($\\triangle$), la igualdad se convierte en} &amp; = \\mathbb P[\\text{Mejora}|\\text{Hombre}\\cap \\text{Nuevo}^c] \\cdot \\mathbb P[\\text{Hombre}|\\text{Nuevo}^c] \\\\ &amp; \\quad + \\mathbb P[\\text{Mejora}|\\text{Hombre}^c\\cap \\text{Nuevo}^c] \\cdot \\mathbb P[\\text{Hombre}^c|\\text{Nuevo}^c] \\\\ \\intertext{Finalmente usando la propiedades de probabilidades condicionales, esta expresión se puede escribir como,} &amp; = \\mathbb P[\\text{Mejora}|\\text{Nuevo}^c] \\end{align*}\\] Resumiendo los cálculos se tiene que \\[ \\mathbb P[\\text{Mejora}|\\text{Nuevo}] &gt; \\mathbb P[\\text{Mejora}|\\text{Nuevo}^{c}] \\] por lo que no se cumple la paradoja. Nota: Otra forma de que la paradoja nos e cumpla es si \\(\\mathbb{P}[\\text{Nuevo}|\\text{Hombre}] = \\mathbb{P}[\\text{Nuevo}|\\text{Hombre}^{c}]\\). (Tarea) "],
["pruebas-de-kolmogorov-smirnov.html", "Capítulo 15 Pruebas de Kolmogorov-Smirnov 15.1 Prueba de Kolmogorov-Smirnov para una muestra 15.2 Prueba de 2 muestras", " Capítulo 15 Pruebas de Kolmogorov-Smirnov En capítulos anteriores, hicimos una prueba \\(\\chi ^2\\) para determinar si cierto conjunto de datos se ajustaba a una distribución continua o no. Una mejor prueba para este caso es la prueba Kolmogorov-Smirnov. Sean \\(X_1,\\dots,X_n\\sim F\\), \\(F\\) una distribución continua. Asumimos que los valores observados \\(x_1,\\dots,x_n\\) son diferentes. Definición. Sean \\(x_1,\\dots,x_n\\) los valores observados de la muestra aleatoria \\(X_1,\\dots, X_n\\). Para cada \\(x\\) defina \\(F_n(x)\\) como la proporción de valores observados en la muestra que son menores o iguales a \\(x\\). Es decir, si hay \\(k\\) valores observados menores o iguales a \\(x\\), \\[ F_n(x) = \\frac{k}{n}. \\] La función \\(F_n(x)\\) se conoce como la función de distribución de la muestra (empírica). \\(F_n\\) es una distribución a pasos con salto de magnitud \\(\\dfrac 1n\\) entre los valores \\(x_1,\\dots,x_n\\). Se puede expresar como \\[F_n(x) = \\begin{cases} 0 &amp; \\text{si } x&lt;X_{(1)}\\\\ \\displaystyle \\dfrac 1n \\sum_{i=1}^n 1_{\\{x_i\\leq x\\}} &amp; \\text{si } X_{(1)}\\leq x&lt;X_{(n)}\\\\ 1 &amp; \\text{si} X_{(n)}\\geq x \\end{cases} \\] Como \\(\\{x_i\\}_{i=1}^n\\) son independientes, \\(\\{1_{\\{x_i\\leq x\\}}\\}_{i=1}^n\\) son independientes. Entonces, por la ley de grandes números \\[ F_n(x) = \\dfrac 1n \\sum_{i=1}^n 1_{\\{x_i\\leq x\\}} \\xrightarrow[n\\to\\infty]{\\mathbb P} \\mathbb E[1_{X_i\\leq x}] = F(x) \\] por lo que \\(F_n(x)\\) es consistente. Ejemplo x &lt;- c( 17.88, 28.92, 33, 41.52, 42.12, 45.6, 48.8, 51.84, 51.96, 54.12, 55.56, 67.8, 68.44, 68.64, 68.88, 84.12, 93.12, 98.64, 105.12, 105.84, 127.92, 128.04, 173.4 ) df &lt;- as.data.frame(x) Para este ejemplo tenemos que \\(F_n(x)\\) se ve de la forma: ggplot() + stat_ecdf( data = df, mapping = aes(x, color = &quot;F_n(x)&quot;), size = 2 ) + theme_minimal() Y en los capítulos anteriores hicimos una comprobación de que los parámetros \\(\\mu =3.912\\) y \\(\\sigma ^{2} = 0.25\\) se ajustaban bien para los log-valores. En este caso podemos comprobar este ajuste ggplot() + stat_ecdf( data = df, mapping = aes(log(x), color = &quot;F_n(x)&quot;), size = 2 ) + stat_function( data = data.frame(x = c(3, 5)), fun = pnorm, aes(color = &quot;F(x)&quot;), size = 2, args = list(mean = 3.912, sd = sqrt(0.25)) ) + theme_minimal() La prueba de Kolmogorov-Smirnov está basada en el siguiente teorema. Teorema (Lema de Glivenko-Cantelli) Sea \\(F_n(x)\\) la distribución empírica de una muestra \\(X_1, \\dots , X_n\\) provenientes de la distribución \\(F\\). Defina \\[ D_n = \\sup_{-\\infty&lt;x&lt;\\infty}|F_n(x)-F(x)| \\] Entonces \\(D_n\\xrightarrow[]{\\mathbb P} 0\\). \\(\\qed\\) Esto quiere decir que si una distribución empírica \\(F_n(x)\\) realmente es tomada de la distribución teórica \\(F(x)\\) entonces la diferencia de estas dos van a converger en probabilidad cuando \\(n \\to \\infty\\). La prueba de esto no es parte del curso, por lo que la omitiremos. 15.1 Prueba de Kolmogorov-Smirnov para una muestra La pregunta que responde esta prueba es ¿Será que \\(F = F^*\\), donde \\(F ^{*}\\) es una distribución hipotética?. La hipótesis es \\[ H_0: F = F^* \\text{ vs } H_1: F\\neq F^* \\] En este caso se define el estadístico \\[ D_n^* = \\sup_{-\\infty&lt;x&lt;\\infty}|F_n(x)-F^*(x)| \\] Nota: Si \\(H_0\\) es cierto, \\(D_n^*\\) no depende de \\(F^*\\). Note que si \\(Z_i = F^*(X_i)\\), \\(i=1,\\dots,n\\) \\((X_1,\\dots,X_n \\sim F)\\). Vea que \\[ \\mathbb P(Z_i\\leq z) = \\mathbb P(F^*(X_i)\\leq z) = \\mathbb P(X_i\\leq ((F^*)^{-1}(z))) = z \\] Entonces \\(Z_1,\\dots, Z_n \\underset{H_0}{\\sim} \\text{Unif}(0,1)\\) Considere la hipótesis \\(H_0: G = \\text{Unif}(0,1)\\) donde \\(G\\) es la distribución de \\(Z_i\\). Entonces \\[D_n^{*,G} = \\sup_{0&lt;z&lt;1}|G_n(z)-G^*(z)| = \\sup_{0&lt;z&lt;1}|G_n(z)-F_{\\text{Unif}(0,1)}(z)| = \\sup_{0&lt;z&lt;1}|G_n(z)-z|\\] Observe que \\[G_n(z) = \\dfrac 1n \\sum_{i=1}^n 1_{\\{Z_i\\leq x\\}} = \\dfrac 1n \\sum_{i=1}^n 1_{\\{F^*(X_i)\\leq z\\}} = \\sum_{i=1}^n 1_{\\{X_i\\leq (F^*)^{-1}(z)\\}} = F_n((F^*)^{-1}(z))\\] Entonces, \\[D_n^{*,G} \\underset{H_0}{=} \\sup|F_n(x)-F^*(x)| = D_n^*\\] por lo que \\(D_n^*\\) no depende de \\(F^*\\) bajo \\(H_0\\). \\(\\qed\\) De manera práctica si la distribución \\(F_n(x)\\) es cercano a \\(F ^{*}\\) entonces \\(D_n ^{*}\\) será cercano a 0. Entonces la podemos rechazar la hipótesis nula \\(H_0\\) si \\(n^{\\frac{1}{2}}D_n^*\\geq c\\), para algún valor \\(c\\) particular. Este valor en particular se debe estimar a partir de la distribución de Kolmogorov-Smirnov. Teorema (de Kolmogorov-Smirnov (1930)). Si \\(H_0\\) es cierto, para \\(t&gt;0\\), \\[\\lim_{n\\to \\infty} \\mathbb P(n^{1/2}D_n^*\\leq t) = 1-2\\sum_{i=1}^\\infty (-1)^{i-1}e^{-2i^2t^2} = H(t).\\] Rechazamos \\(H_0\\) si \\(n^{1/2}D_n^*\\geq c\\), \\(n\\) grande. Para un nivel de significancia \\(\\alpha_0\\), \\(c = H^{-1}(1-\\alpha_0)\\), donde \\(H\\) denota el valor de la parte derecha de la ecuación anterior. La función \\(H(t)\\) es algo complicada de estimar, y sus cuantiles lo son aún más. Estos normalmente son definidos a través de métodos números que están fuera del alcance del este curso. La siguiente tabla muestra el conjunto de valores estimados para cada \\(t\\) Los valores más comunes de cuantiles para las pruebas son \\(\\alpha\\) \\(H^{-1}(1-\\alpha)\\) 0.01 1.63 0.05 1.36 0.1 1.22 Solo como ilustración, el paquete KSgeneral tiene programada la estimación de \\(\\mathbb P (D_n \\leq t)\\). En este gráfico se presentan algunos ejemplos con \\(n\\) igual a 10, 50 y 100. library(KSgeneral) t &lt;- seq(0.001, 0.999, length.out = 1000) df &lt;- rbind( data.frame(t, ks = Vectorize(cont_ks_cdf)(t, 10), n = 10), data.frame(t, ks = Vectorize(cont_ks_cdf)(t, 50), n = 50), data.frame(t, ks = Vectorize(cont_ks_cdf)(t, 100), n = 100) ) ggplot(df, aes(x = t, y = ks, color = as.factor(n))) + geom_line() + theme_minimal() Ejemplo: En el caso de las partes mecánicas quisiéramos saber si los log-valores siguen o no una distribución normal. Dado que queremos comparar estos valores con un \\(N (\\hat{\\mu } , {\\sigma^{&#39;}}^{2})\\), entonces ks.test( x = log(x), y = &quot;pnorm&quot;, mean = mean(log(x)), sd = sd(log(x)) ) ## ## One-sample Kolmogorov-Smirnov test ## ## data: log(x) ## D = 0.091246, p-value = 0.9815 ## alternative hypothesis: two-sided Note que esta localización es muy importante ya que si se quisiera comparar con una distribución \\(N(0,1)\\) el resultado es diferente. ks.test( x = log(x), y = &quot;pnorm&quot;, mean = 0, sd = 1 ) ## ## One-sample Kolmogorov-Smirnov test ## ## data: log(x) ## D = 0.99803, p-value = 4.441e-16 ## alternative hypothesis: two-sided 15.2 Prueba de 2 muestras Suponga que se tiene \\(X_1,\\dots,X_m\\sim N(\\mu_1,\\sigma^2)\\) y \\(Y_1,\\dots,Y_n\\sim N(\\mu_2,\\sigma^2)\\) y se desea saber si ambas muestras tienen la misma distribución. Una opción es probar que \\[ H_0:\\mu_1 = \\mu_2 \\text{ vs } H_1: \\mu_{1} \\neq \\mu_{2} \\] Uno de los supuestos fuertes para este tipo de pruebas es la normalidad. Otra opción es decir que \\[ H_0: F_1 = F_2 \\text{ vs } H_1: F_{1} \\neq F_{2} \\] donde \\(F_1\\) es la distribución de \\(X\\) y \\(F_2\\) la de \\(Y\\). Igual en este caso estamos probando dos distribuciones normales, pero ¿Es posible del todo quitar el supuesto de normalidad? Es decir, para \\(X_1,\\dots,X_m\\sim F\\) y \\(Y_1,\\dots,Y_m\\sim G\\) continuas, sin valores en común, probar \\(H_0: F(x) = G(x)\\), \\(x \\in \\mathbb R\\). Considere \\[ D_{mn} = \\sup_{-\\infty&lt;x&lt;\\infty}|F_m(x)-G_n(x)| \\] Se tiene por el teorema de Glivenko-Cantelli que \\(D_{mn}\\xrightarrow[]{\\mathbb P} 0\\), \\(m,n\\to\\infty\\) cuando \\(H_0\\) es verdadera Para el caso de dos muestras se puede probar que si \\(H(t)\\) es la distribución límite en el caso de una muestra y \\(t&gt;0\\), entonces se cumple que \\[\\lim_{m,n\\to \\infty} \\mathbb P \\left( \\left( \\dfrac{mn}{m+n}\\right)^{\\frac 12} D_{mn}\\leq t\\right) = H(t)\\] En este caso se rechaza la hipótesis nula si \\(\\left(\\dfrac{mn}{m+n}\\right)^{\\frac 12}D_{mn} \\geq H^{-1}(1-\\alpha_0)\\). Ejemplo Suponga que se tienen dos grupos de personas a las cuales a unas se les dio un tratamiento para la presión arterial y al otro se le dio un placebo. Al se midieron las diferencias entre las presiones arteriales al cabo de 12 semanas de tratamiento. Los resultados fueron estos Medicina &lt;- c(7, -4, 18, 17, -3, -5, 1, 10, 11, -2) Placebo &lt;- c(-1, 12, -1, -3, 3, -5, 5, 2, -11, -1, -3) La pregunta es si ambos conjuntos de datos vienen de la misma distribución. ks.test(Medicina, Placebo) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: Medicina and Placebo ## D = 0.40909, p-value = 0.3446 ## alternative hypothesis: two-sided En este caso rechazamos la hipótesis nula de que ambas distribuciones son iguales con un nivel de \\(\\alpha \\geq 0.346\\). df &lt;- rbind( data.frame(x = Medicina, Tratamiento = &quot;Medicina&quot;), data.frame(x = Placebo, Tratamiento = &quot;Placebo&quot;) ) ggplot(df) + stat_ecdf(aes(x, color = Tratamiento)) + theme_minimal() "],
["pruebas-no-paramétricas-pruebas-de-signo-y-rango.html", "Capítulo 16 Pruebas no-paramétricas: pruebas de signo y rango 16.1 Prueba de signo 16.2 Prueba de Wilconxon-Mann-Whitney", " Capítulo 16 Pruebas no-paramétricas: pruebas de signo y rango En este capítulo se explorará como hacer hipótesis sobre la distribución de un conjunto de datos cuando no se sabe con exactitud la distribución teórica a la cual pertenece. 16.1 Prueba de signo Sean \\(X_1,\\dots,X_n\\) una muestra aleatoria de una distribución desconocida continua. Recordemos que no toda distribución tiene media, por ejemplo la distribución Cauchy3. Sin embargo, toda distribución continua si tiene una mediana \\(\\mu\\)4 definida. La mediana es una popular medida de ubicación, que satisface \\[\\mathbb P(X_i\\leq \\mu)=0,5.\\] Suponga que queremos probar \\[\\begin{align*} H_0: &amp; \\mu\\leq \\mu_{0} \\\\ H_1: &amp; \\mu &gt; \\mu_{0} \\end{align*}\\] ¿Por qué esta prueba? Note que esta se basa en el hecho de que \\(\\mu\\leq \\mu_0\\) si y solo si e \\(\\mathbb P(X_i&lt;\\mu_0)\\geq 0,5\\) para \\(i = 1, \\dots, n\\). Es decir, que la mediana \\(\\mu\\) es menor que algún valor \\(\\mu_{0}\\) si y solo si, la probabilidad de que los valores \\(X_i\\) sean menores que \\(\\mu_{0}\\) sea mayor de 0.5. De forma más gráfica, si tiraramos una moneda, la mitad de la veces un valor de \\(X\\) caería por debajo de \\(\\mu_{0}\\) y la otra mitad por encima de \\(\\mu_{0}\\) Usando esta última observación, para \\(i=1,\\dots,n\\), sea \\(Y_i = 1\\) si \\(X_i\\leq \\mu_0\\) y \\(Y_i = 0\\) si no. Defina \\(p = \\mathbb P(Y_i = 1)\\). Entonces, probar que \\(\\mu\\leq \\mu_0\\) es equivalente a probar \\(p \\geq 0,5\\). Como \\(X_1,\\dots,X_n\\) son independientes, \\(Y_1,\\dots,Y_n\\) lo son, entonces \\[Y_1,\\dots,Y_n\\sim \\text{Binomial}(n,p).\\] Calculamos \\(W = Y_1+\\dots+Y_n\\) y decimos que rechazamos la hipótesis nula si \\(W\\) es pequeño. Para tener una prueba de tamaño \\(\\alpha_{0}\\), escoja \\(c\\) tal que \\[\\sum_{w=0}^c{n\\choose w}\\left( \\dfrac 12\\right)^n \\leq \\alpha_0 &lt; \\sum_{w=0}^{c+1}{n\\choose w}\\left( \\dfrac 12\\right)^n\\] Se rechaza \\(H_0\\) si \\(W\\leq c\\). La prueba descrita es llamada prueba de signo pues está basada en el número de observaciones en las cuales \\(X_i-\\mu_0\\) es negativo. Si se desea hacer una prueba de dos colas \\[\\begin{align*} H_0: &amp; \\mu \\neq \\mu_{0} (p = \\frac{1}{2}) \\\\ H_1: &amp; \\mu \\neq \\mu_{0} (p \\neq \\frac{1}{2}) \\end{align*}\\] Se rechaza \\(H_0\\) si \\(W\\leq c\\) o \\(W \\geq n-c\\) y para obtener un nivel de significacia del \\(\\alpha_{0}\\) seleccionamos \\(c\\) tal que \\[\\sum_{w=0}^c{n\\choose w}\\left( \\dfrac 12\\right)^n \\leq \\dfrac{\\alpha_0}{2} &lt; \\sum_{w=0}^{c+1}{n\\choose w}\\left( \\dfrac 12\\right)^n\\] La función de potencia es \\[\\mathbb P(W\\leq c) =\\sum_{w=0}^c{n\\choose w}(1-p)^{n-w}p^w \\] Ejemplo: En junio de 1986 la revista Consumer Reports reportó las calorías de 20 marcas de salchichas. Este fueros los datos que dieron: x &lt;- c( 186, 181, 176, 149, 184, 190, 158, 139, 175, 148, 152, 111, 141, 153, 190, 157, 131, 149, 135, 132 ) ggplot(as.data.frame(x), aes(y = x)) + geom_boxplot() + theme_minimal() Se quiere hacer el supuesto que la mediana es igual a 150 (\\(\\mu = 150\\)). Entonces se plantea la hipótesis \\[\\begin{align*} H_0: &amp; \\mu = 150 \\\\ H_1: &amp; \\mu \\neq 150 \\end{align*}\\] La prueba de signo cuenta cuantas veces \\(X_i - 150\\) es negativo. Observe que para estos datos tenemos que M &lt;- data.frame( diferencias = x - 150, signo_negativo = x - 150 &lt; 0) M ## diferencias signo_negativo ## 1 36 FALSE ## 2 31 FALSE ## 3 26 FALSE ## 4 -1 TRUE ## 5 34 FALSE ## 6 40 FALSE ## 7 8 FALSE ## 8 -11 TRUE ## 9 25 FALSE ## 10 -2 TRUE ## 11 2 FALSE ## 12 -39 TRUE ## 13 -9 TRUE ## 14 3 FALSE ## 15 40 FALSE ## 16 7 FALSE ## 17 -19 TRUE ## 18 -1 TRUE ## 19 -15 TRUE ## 20 -18 TRUE summary(M$signo_negativo) ## Mode FALSE TRUE ## logical 11 9 Y el valor \\(p\\) correspondiente es: 2 * pbinom(q = 9, size = 20, prob = 1 / 2) ## [1] 0.8238029 Rechazamos la hipótesis nula con un nivel \\(\\alpha_0\\geq 0.8238\\). Este mismo problema se puede resolver con la función binom.test de R (x es el número de signos negativos y n es el número total de datos). binom.test(x = 9, n = 20) ## ## Exact binomial test ## ## data: 9 and 20 ## number of successes = 9, number of trials = 20, p-value = 0.8238 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.2305779 0.6847219 ## sample estimates: ## probability of success ## 0.45 16.2 Prueba de Wilconxon-Mann-Whitney Dado dos conjuntos de variables \\(X_1,\\dots, X_m\\overset{i.i.d}{\\sim} F\\) y \\(Y_1, \\dots, Y_{n}\\overset{i.i.d}{\\sim} G\\), queremos hacer la hipótesis \\[\\begin{align*} H_0: &amp; F = G \\\\ H_1: &amp; F \\neq G \\end{align*}\\] Esta hipótesis se puede hacer con las pruebas de Kolmogorov-Smirnov o la prueba \\(t\\) (dependiendo de \\(F\\) y \\(G\\)). Otra forma de hacerlo es usando la prueba de Wilconxon-Mann-Whitney (prueba de rango) descubierta por F. Wilcoxon, H. B. Mann y D. R. Whitney en la década de 1940. La lógica de la prueba reside en que si unimos los dos conjuntos de valores y ambos tienen la misma distribución, entonces los datos se podrían ordenar de menor a mayor y todos estarían dispersos equitativamente. Sean \\(X_1,\\dots,X_n\\overset{i.i.d}{\\sim} F\\) y \\(X_1,\\dots,X_n\\overset{i.i.d}{\\sim} G\\) con \\(F,G\\) continuas. Considere la hipótesis \\(H_0: F = G\\) vs \\(H_1: F\\neq G\\). Asuma que \\(H_0\\) es cierto y unimos las dos muestras \\[(W_1,\\dots,W_n+m) = (X_1,\\dots,X_m,Y_1,\\dots,Y_n)\\] Esta muestra se puede ordernar según los estadísticos de orden \\[ (W_(1),\\dots,W_(n+m)) \\] Ejemplo: Supongamos que tenemos estos dos conjuntos de datos y queremos ver si tienen la misma distribución. x &lt;- c(2.183, 2.431, 2.556, 2.629, 2.641, 2.715, 2.805, 2.840) y &lt;- c(2.120, 2.153, 2.213, 2.240, 2.245, 2.266, 2.281, 2.336, 2.558, 2.587) dfx &lt;- data.frame(W = x, variable = &quot;x&quot;) dfy &lt;- data.frame(W = y, variable = &quot;y&quot;) Lo primero sería unirlos dfw &lt;- full_join(dfx, dfy) Y luego ordenarlos y calcular su rango dfw &lt;- dfw %&gt;% arrange(W) %&gt;% mutate(rango = 1:n()) dfw ## W variable rango ## 1 2.120 y 1 ## 2 2.153 y 2 ## 3 2.183 x 3 ## 4 2.213 y 4 ## 5 2.240 y 5 ## 6 2.245 y 6 ## 7 2.266 y 7 ## 8 2.281 y 8 ## 9 2.336 y 9 ## 10 2.431 x 10 ## 11 2.556 x 11 ## 12 2.558 y 12 ## 13 2.587 y 13 ## 14 2.629 x 14 ## 15 2.641 x 15 ## 16 2.715 x 16 ## 17 2.805 x 17 ## 18 2.840 x 18 ggplot(dfw, aes(y = W, fill = variable)) + geom_boxplot() + theme_minimal() \\(\\qed\\) Para una muestra como \\(X_1,\\dots,X_m\\) se tiene que las posiciones de cada uno los datos se puede escribir como variables aleatorias \\[X_1,\\dots,X_m \\to X_{I_1},\\dots,X_{I_m} \\to X_{(1)},\\dots,X_{(m)} .\\] Note que estas posiciones se pueden modelar como \\[(I_1,\\dots,I_m) \\sim \\text{Unif. Discreta}(1,\\dots,m)\\] Bajo \\(H_0\\), \\(W_1,\\dots,W_{n+m}\\) tiene índices de posición uniformemente distribuidos sobre los enteros \\(1,\\dots,m+n\\). Defina \\(S = \\sum_{i=1}^m I_i\\), es decir la suma de todos los índices hasta el valor \\(m\\). Se puede probar que \\(\\mathbb E[S] \\overset{H_0}{=} m\\left(\\dfrac{m+n+1}{2}\\right)\\). \\(\\text{Var}(S) \\overset{H_0}{=} mn\\left(\\dfrac{m+n+1}{12}\\right)\\). El resultado importante de Mann y Whitney en 1947 fue probar que si \\(m,n\\) son grandes, entonces \\[S\\underset{H_0}{\\sim}N\\left(\\dfrac{m(m+n+1)}{2},\\dfrac{mn(m+n+1)}{12}\\right).\\] Por lo tanto la prueba se convierte en una prueba de normalidad sobre los rangos de los datos. Rechazamos la hipótesis nula si \\(S\\) se desvía mucho del valor de la media \\(\\mathbb E[S]\\). En otra palabras rechazamos \\(H_0\\) si \\[ \\left|S- \\frac{m(m+n+1)}{2}\\right| \\geq (\\text{Var}(S))^\\frac{1}{2} \\Phi^{-1}(1-\\frac{\\alpha}{2}) \\] Ejemplo Continuando con nuestro ejemplo definamos el \\(m\\) y \\(n\\) de las muestras. OJO: Tomaremos siempre como \\(m\\) el conjunto de datos más pequeño (m &lt;- length(x)) ## [1] 8 (n &lt;- length(y)) ## [1] 10 Construirmos las medias y varianzas teóricas de los rangos (media_S &lt;- m * (m + n + 1) / 2) ## [1] 76 (var_S &lt;- m * n * (m + n + 1) / 12) ## [1] 126.6667 Tomamos la suma de todos los rangos de la muestra más pequeña. En este caso sería sobre los x S &lt;- dfw %&gt;% filter(variable == &quot;x&quot;) %&gt;% summarise (S = sum(rango)) (S &lt;- as.numeric(S)) ## [1] 104 La variable \\(S\\) sigue una distribución \\(N(76, \\sqrt{126.67})\\). Por lo tanto su \\(p\\)-valor es 2 * (1 - pnorm(q = (S - media_S) / sqrt(var_S))) ## [1] 0.01285124 Rechazamos \\(H_0\\) si el nivel de significacia \\(\\alpha_0&gt;0.0128\\) La función en R wilcox.test calcula la misma prueba, aunque esta hace algunos ajustes adicionales a los rangos, por eso los valores son ligeramente diferentes. Los detalles los pueden consultar en la ayuda de la función. wilcox.test(x, y) ## ## Wilcoxon rank sum exact test ## ## data: x and y ## W = 68, p-value = 0.01166 ## alternative hypothesis: true location shift is not equal to 0 La densidad de una distribución Cauchy se define como \\(\\displaystyle f(x;x_{0},\\gamma )={1 \\over \\pi \\gamma }\\left[{\\gamma ^{2} \\over (x-x_{0})^{2}+\\gamma ^{2}}\\right],\\), donde \\(x_0\\) y \\(\\gamma\\) son parámetros de localización y escala respectivamente. Esta distribución no tiene ningún momento definido y su mediana es \\(x_0\\).↩︎ Importante: Aunque normalmente se denota \\(\\mu\\) como la media, en este capítulo \\(\\mu\\) es la mediana.↩︎ "],
["ejercicios-varios.html", "Capítulo 17 Ejercicios varios 17.1 Capítulo 8", " Capítulo 17 Ejercicios varios 17.1 Capítulo 8 17.1.1 8.4.6 Sabemos que \\(U = (20)^{1/2} (\\bar{X}-\\mu)/\\sigma^\\prime\\) es una distribución t con 19 grados de libertad. (tquantile &lt;- qt(p = 0.95, df = 19)) ## [1] 1.729133 Y despejando la expresión tenemos que tquantile / sqrt(20) ## [1] 0.3866459 "]
]
