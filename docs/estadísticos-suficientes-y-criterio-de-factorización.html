<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 5 Estadísticos Suficientes y Criterio de Factorización | Notas Curso de Estadística (Parte I)</title>
  <meta name="description" content="Capítulo 5 Estadísticos Suficientes y Criterio de Factorización | Notas Curso de Estadística (Parte I)" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 5 Estadísticos Suficientes y Criterio de Factorización | Notas Curso de Estadística (Parte I)" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 5 Estadísticos Suficientes y Criterio de Factorización | Notas Curso de Estadística (Parte I)" />
  
  
  

<meta name="author" content="Maikol Solís" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="estimación-por-máxima-verosimilitud.html"/>
<link rel="next" href="distribución-muestral-de-un-estadístico.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso de Estadística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html"><i class="fa fa-check"></i><b>2</b> Inferencia estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#ejemplo"><i class="fa fa-check"></i><b>2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#modelo-estadístico"><i class="fa fa-check"></i><b>2.2</b> Modelo estadístico</a></li>
<li class="chapter" data-level="2.3" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#estadístico"><i class="fa fa-check"></i><b>2.3</b> Estadístico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><i class="fa fa-check"></i><b>3</b> Densidades previas conjugadas y estimadores de Bayes</a>
<ul>
<li class="chapter" data-level="3.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-previa-distribución-a-priori"><i class="fa fa-check"></i><b>3.1</b> Distribución previa (distribución a priori)</a></li>
<li class="chapter" data-level="3.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidad-posterior"><i class="fa fa-check"></i><b>3.2</b> Densidad posterior</a></li>
<li class="chapter" data-level="3.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#proceso-de-modelación-de-parámetros."><i class="fa fa-check"></i><b>3.3</b> Proceso de modelación de parámetros.</a></li>
<li class="chapter" data-level="3.4" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-verosimilitud"><i class="fa fa-check"></i><b>3.4</b> Función de verosimilitud</a></li>
<li class="chapter" data-level="3.5" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#familias-conjugadas"><i class="fa fa-check"></i><b>3.5</b> Familias conjugadas</a></li>
<li class="chapter" data-level="3.6" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidades-previas-impropias"><i class="fa fa-check"></i><b>3.6</b> Densidades previas impropias</a></li>
<li class="chapter" data-level="3.7" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7</b> Funciones de pérdida</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>3.7.1</b> Función de pérdida cuadrática</a></li>
<li class="chapter" data-level="3.7.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-absoluta"><i class="fa fa-check"></i><b>3.7.2</b> Función de pérdida absoluta</a></li>
<li class="chapter" data-level="3.7.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#otras-funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7.3</b> Otras funciones de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#efecto-de-muestras-grandes"><i class="fa fa-check"></i><b>3.8</b> Efecto de muestras grandes</a></li>
<li class="chapter" data-level="3.9" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#consistencia"><i class="fa fa-check"></i><b>3.9</b> Consistencia</a></li>
<li class="chapter" data-level="3.10" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#laboratorio"><i class="fa fa-check"></i><b>3.10</b> Laboratorio</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-previa"><i class="fa fa-check"></i><b>3.10.1</b> Distribución previa</a></li>
<li class="chapter" data-level="3.10.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-conjunta"><i class="fa fa-check"></i><b>3.10.2</b> Distribución conjunta</a></li>
<li class="chapter" data-level="3.10.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-posterior"><i class="fa fa-check"></i><b>3.10.3</b> Distribución posterior</a></li>
<li class="chapter" data-level="3.10.4" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#agregando-nuevos-datos"><i class="fa fa-check"></i><b>3.10.4</b> Agregando nuevos datos</a></li>
<li class="chapter" data-level="3.10.5" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#familias-conjugadas-normales"><i class="fa fa-check"></i><b>3.10.5</b> Familias conjugadas normales</a></li>
<li class="chapter" data-level="3.10.6" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#funciones-de-pérdida-1"><i class="fa fa-check"></i><b>3.10.6</b> Funciones de pérdida</a></li>
<li class="chapter" data-level="3.10.7" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#caso-concreto"><i class="fa fa-check"></i><b>3.10.7</b> Caso concreto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html"><i class="fa fa-check"></i><b>4</b> Estimación por máxima verosimilitud</a>
<ul>
<li class="chapter" data-level="4.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#propiedades-del-mle"><i class="fa fa-check"></i><b>4.1</b> Propiedades del MLE</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#propiedad-de-invarianza"><i class="fa fa-check"></i><b>4.1.1</b> Propiedad de invarianza</a></li>
<li class="chapter" data-level="4.1.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#consistencia"><i class="fa fa-check"></i><b>4.1.2</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#cálculo-numérico"><i class="fa fa-check"></i><b>4.2</b> Cálculo numérico</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#método-de-los-momentos"><i class="fa fa-check"></i><b>4.2.1</b> Método de los momentos</a></li>
<li class="chapter" data-level="4.2.2" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#método-delta"><i class="fa fa-check"></i><b>4.2.2</b> Método Delta</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#laboratorio"><i class="fa fa-check"></i><b>4.3</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html"><i class="fa fa-check"></i><b>5</b> Estadísticos Suficientes y Criterio de Factorización</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadísticos-suficientes"><i class="fa fa-check"></i><b>5.1</b> Estadísticos suficientes</a></li>
<li class="chapter" data-level="5.2" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#teorema-de-factorización-de-fisher"><i class="fa fa-check"></i><b>5.2</b> Teorema de Factorización de Fisher</a></li>
<li class="chapter" data-level="5.3" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadístico-suficiente-multivariado."><i class="fa fa-check"></i><b>5.3</b> Estadístico suficiente multivariado.</a></li>
<li class="chapter" data-level="5.4" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadísticos-minimales"><i class="fa fa-check"></i><b>5.4</b> Estadísticos minimales</a></li>
<li class="chapter" data-level="5.5" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#mejorando-estimadores"><i class="fa fa-check"></i><b>5.5</b> Mejorando estimadores</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html"><i class="fa fa-check"></i><b>6</b> Distribución muestral de un estadístico</a>
<ul>
<li class="chapter" data-level="6.1" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-muestral"><i class="fa fa-check"></i><b>6.1</b> Distribución muestral</a></li>
<li class="chapter" data-level="6.2" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-chi2"><i class="fa fa-check"></i><b>6.2</b> Distribución <span class="math inline">\(\chi^2\)</span></a></li>
<li class="chapter" data-level="6.3" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-t"><i class="fa fa-check"></i><b>6.3</b> Distribución <span class="math inline">\(t\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html"><i class="fa fa-check"></i><b>7</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="7.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-para-la-media-de-una-distribución-normal"><i class="fa fa-check"></i><b>7.1</b> Intervalos de confianza para la media de una distribución normal</a></li>
<li class="chapter" data-level="7.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-abiertos"><i class="fa fa-check"></i><b>7.2</b> Intervalos de confianza abiertos</a></li>
<li class="chapter" data-level="7.3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-en-otros-casos"><i class="fa fa-check"></i><b>7.3</b> Intervalos de confianza en otros casos</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-aproximados."><i class="fa fa-check"></i><b>7.3.1</b> Intervalos de confianza aproximados.</a></li>
<li class="chapter" data-level="7.3.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#transformaciones-estabilizadoras-de-la-varianza"><i class="fa fa-check"></i><b>7.3.2</b> Transformaciones estabilizadoras de la varianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html"><i class="fa fa-check"></i><b>8</b> Estimación Bayesiana bajo normalidad</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#precisión-de-una-distribución-normal"><i class="fa fa-check"></i><b>8.1</b> Precisión de una distribución normal</a></li>
<li class="chapter" data-level="8.2" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#distribución-marginal-de-mu"><i class="fa fa-check"></i><b>8.2</b> Distribución marginal de <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.3" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#efecto-de-previas-no-informativas"><i class="fa fa-check"></i><b>8.3</b> Efecto de previas no informativas</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html"><i class="fa fa-check"></i><b>9</b> Estimación insesgada</a>
<ul>
<li class="chapter" data-level="9.1" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimadores-insesgados"><i class="fa fa-check"></i><b>9.1</b> Estimadores insesgados</a></li>
<li class="chapter" data-level="9.2" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimador-insesgado-de-la-varianza"><i class="fa fa-check"></i><b>9.2</b> Estimador insesgado de la varianza</a></li>
<li class="chapter" data-level="9.3" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#información-de-fisher"><i class="fa fa-check"></i><b>9.3</b> Información de Fisher</a></li>
<li class="chapter" data-level="9.4" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#desigualdad-de-cramer-rao"><i class="fa fa-check"></i><b>9.4</b> Desigualdad de Cramer-Rao</a></li>
<li class="chapter" data-level="9.5" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimadores-eficientes"><i class="fa fa-check"></i><b>9.5</b> Estimadores eficientes</a></li>
<li class="chapter" data-level="9.6" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#comportamiento-asintótico-del-mle"><i class="fa fa-check"></i><b>9.6</b> Comportamiento asintótico del MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html"><i class="fa fa-check"></i><b>10</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-de-hipótesis-1"><i class="fa fa-check"></i><b>10.1</b> Pruebas de hipótesis</a></li>
<li class="chapter" data-level="10.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#regiones-críticas-y-estadísticas-de-prueba"><i class="fa fa-check"></i><b>10.2</b> Regiones críticas y estadísticas de prueba</a></li>
<li class="chapter" data-level="10.3" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#función-de-potencia-y-tipos-de-error"><i class="fa fa-check"></i><b>10.3</b> Función de potencia y tipos de error</a></li>
<li class="chapter" data-level="10.4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#valor-p"><i class="fa fa-check"></i><b>10.4</b> Valor <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="10.5" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#dualidad-entre-pruebas-de-hipótesis-y-regiones-de-confianza"><i class="fa fa-check"></i><b>10.5</b> Dualidad entre pruebas de hipótesis y regiones de confianza</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#dualidad-en-pruebas-unilaterales"><i class="fa fa-check"></i><b>10.5.1</b> Dualidad en pruebas unilaterales</a></li>
<li class="chapter" data-level="10.5.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-de-cociente-de-verosimilitud-lrt"><i class="fa fa-check"></i><b>10.5.2</b> Pruebas de cociente de verosimilitud (LRT)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html"><i class="fa fa-check"></i><b>11</b> Pruebas con hipótesis simples</a>
<ul>
<li class="chapter" data-level="11.1" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#hipótesis-simples"><i class="fa fa-check"></i><b>11.1</b> Hipótesis simples</a></li>
<li class="chapter" data-level="11.2" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#prueba-t"><i class="fa fa-check"></i><b>11.2</b> Prueba <span class="math inline">\(t\)</span></a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#propiedades-de-las-pruebas-t"><i class="fa fa-check"></i><b>11.2.1</b> Propiedades de las pruebas <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="11.2.2" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#prueba-t-pareada"><i class="fa fa-check"></i><b>11.2.2</b> Prueba <span class="math inline">\(t\)</span> pareada</a></li>
<li class="chapter" data-level="11.2.3" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#pruebas-t-de-dos-colas"><i class="fa fa-check"></i><b>11.2.3</b> Pruebas <span class="math inline">\(t\)</span> de dos colas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html"><i class="fa fa-check"></i><b>12</b> Prueba de comparación de medias en 2 poblaciones</a>
<ul>
<li class="chapter" data-level="12.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#comparación-de-medias-normales"><i class="fa fa-check"></i><b>12.1</b> Comparación de medias normales</a></li>
<li class="chapter" data-level="12.2" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-t-de-dos-muestras"><i class="fa fa-check"></i><b>12.2</b> Prueba <span class="math inline">\(t\)</span> de dos muestras</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-de-2-colas"><i class="fa fa-check"></i><b>12.2.1</b> Prueba de 2 colas</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-f"><i class="fa fa-check"></i><b>12.3</b> Prueba <span class="math inline">\(F\)</span></a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-de-2-colas-prueba-de-homocedasticidad"><i class="fa fa-check"></i><b>12.3.1</b> Prueba de 2 colas (prueba de homocedasticidad)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pruebas-de-hipótesis-bayesianas.html"><a href="pruebas-de-hipótesis-bayesianas.html"><i class="fa fa-check"></i><b>13</b> Pruebas de hipótesis bayesianas</a>
<ul>
<li class="chapter" data-level="13.1" data-path="pruebas-de-hipótesis-bayesianas.html"><a href="pruebas-de-hipótesis-bayesianas.html#pruebas-de-hipótesis-bayesianas-1"><i class="fa fa-check"></i><b>13.1</b> Pruebas de hipótesis bayesianas</a></li>
<li class="chapter" data-level="13.2" data-path="pruebas-de-hipótesis-bayesianas.html"><a href="pruebas-de-hipótesis-bayesianas.html#hipótesis-de-una-cola"><i class="fa fa-check"></i><b>13.2</b> Hipótesis de una cola</a></li>
<li class="chapter" data-level="13.3" data-path="pruebas-de-hipótesis-bayesianas.html"><a href="pruebas-de-hipótesis-bayesianas.html#hipótesis-de-2-colas"><i class="fa fa-check"></i><b>13.3</b> Hipótesis de 2 colas</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas Curso de Estadística (Parte I)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estadísticos-suficientes-y-criterio-de-factorización" class="section level1" number="5">
<h1><span class="header-section-number">Capítulo 5</span> Estadísticos Suficientes y Criterio de Factorización</h1>
<div id="estadísticos-suficientes" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Estadísticos suficientes</h2>
<p>Una función de verosimilitud se va a describir a través de un número. El
objetivo es buscar un estadístico <span class="math inline">\(T=r(X_1,\dots,X_n)\)</span> que resuma de manera
óptima la información de <span class="math inline">\(X_1,\dots,X_n\)</span></p>
<p><strong>Definición</strong>. Sea <span class="math inline">\(X_1,\dots,X_n\)</span> una muestra indexada por <span class="math inline">\(\theta\)</span>. Sea <span class="math inline">\(T\)</span>
un estadístico, suponga que para cada <span class="math inline">\(\theta \in \Omega\)</span> y para cada <span class="math inline">\(t\)</span> en la imagen
de <span class="math inline">\(T\)</span>, <span class="math inline">\(X_1\cdots X_n|T=t\)</span> depende solamente de <span class="math inline">\(t\)</span> y no de <span class="math inline">\(\theta\)</span>. Entonces <span class="math inline">\(T\)</span>
es suficiente.</p>
</div>
<div id="teorema-de-factorización-de-fisher" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Teorema de Factorización de Fisher</h2>
<p><strong>Teorema</strong>. Si <span class="math inline">\(X_1,\dots,X_n\)</span> es una muestra aleatoria de <span class="math inline">\(f(X|\theta)\)</span>, el
parámetro <span class="math inline">\(\theta\)</span> es desconocido. Un estadístico <span class="math inline">\(T=r(X_1,\dots,X_n)\)</span> es
suficiente si y solo si <span class="math display">\[f_n(x|\theta) = u(x)v(r(x),\theta)\;\forall x\in
\mathbb R, \; \forall \theta \in \mathbb R.\]</span></p>
<p><em>Prueba</em> (Discreta). <span class="math inline">\(f_n(x|\theta) = \mathbb P(X=x|\theta)\)</span></p>
<p>“<span class="math inline">\(\Leftarrow\)</span>” Sea <span class="math inline">\(A(t) = \{x\in \mathbb R| r(x) =t\}\)</span>. Para <span class="math inline">\(\theta \in \mathbb R\)</span>, <span class="math inline">\(x\in A(t)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
\mathbb P(X=x|T=t) 
&amp; = \dfrac{\mathbb P(X=x \cap T=t)}{\mathbb P (T=t)} \\ 
&amp;= \dfrac{f_n(x|\theta, T=t)}{\displaystyle\sum_{y \in A(t)}f_n(y|\theta)} \\
&amp; = \dfrac{u(x)v(r(x),\theta)}{\displaystyle\sum_{y \in A(t)} u(y)v(r(y),\theta)} \\ 
&amp; = \dfrac{u(x)v(t,\theta)}{\displaystyle v(t,\theta)\sum_{y \in A(t)} u(y)} \text{(Como \(y\in A(t)\) entonces \(r(y) = t\) que es constante.)}\\ 
&amp;= \dfrac{u(x)}{\displaystyle\sum_{y \in A(t)}u(y)}
\end{align*}\]</span></p>
<p>no depende de <span class="math inline">\(\theta\)</span>.</p>
<p>Si <span class="math inline">\(x\notin A(t) \implies \mathbb P(X=x|T=t) = 0\)</span> no depende de <span class="math inline">\(\theta\)</span>.</p>
<p>“<span class="math inline">\(\Rightarrow\)</span>” Si <span class="math inline">\(T\)</span> es un estadístico suficiente, <span class="math inline">\(u(x) = \mathbb P(X=x|T=t)\)</span> no depende de <span class="math inline">\(\theta\)</span>. Sea <span class="math inline">\(v(t,\theta) = \mathbb P_{\theta}(T=t)\)</span>. Entonces</p>
<p><span class="math display">\[ f_n(x|\theta) = \mathbb P (X=x|\theta) = \dfrac{\mathbb P(X=x|\theta)}{\mathbb P(T=t)}\mathbb P(T=t) = u(x)v(t,\theta).\]</span></p>
<p><strong>Consecuencia</strong>: <span class="math inline">\(f_n(x|\theta) \propto v(r(x),\theta)\)</span> (<span class="math inline">\(u(x)\)</span> es una constante con respecto a <span class="math inline">\(\theta\)</span>). Aplicando el teorema de Bayes,
<span class="math display">\[ \pi(\theta|x) \propto \pi(\theta)v(r(x),\theta).\]</span></p>
<p><strong>Corolario</strong>. Un estadístico <span class="math inline">\(r(x)\)</span> es suficiente si y solo si no importa cuál previa de <span class="math inline">\(\theta\)</span> se use, la posterior depende solamente de <span class="math inline">\(r(x)\)</span> a través de los datos.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim \text{Poi}(\lambda)\)</span>,</p>
<p><span class="math display">\[f_n(x|\theta) = \prod_{i=1}^n \dfrac{e^{-\lambda}}{x_i!} = \dfrac{e^{-\lambda n} \lambda ^{\sum x_i (\;= r(x))}}{\prod x_i!} = \underbrace{\dfrac{1}{\prod_{i=1}^n x_i!}}_{u(x)} \underbrace{e^{-\lambda n}\lambda^{r(x)}}_{v(r(x),\lambda)}\]</span></p>
<p>Si <span class="math inline">\(x_i &lt; 0\)</span> para al menos un <span class="math inline">\(i\)</span>, entonces <span class="math inline">\(f_n(x|\theta) = 0\)</span>. Tome <span class="math inline">\(u(x) = 0\)</span>. Por el teorema de factorización, <span class="math inline">\(r(x) = \sum x_i\)</span> es un estadístico suficiente para <span class="math inline">\(\lambda\)</span>.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim f(x|\theta)\)</span>
<span class="math display">\[ f(x|\theta) = \begin{cases}\theta x^{\theta-1} &amp; 0&lt;x&lt; 1\\ 0 &amp; \text{otro caso}\end{cases}\]</span></p>
<p>Verosimilitud: (<span class="math inline">\(0&lt;x_i&lt;1\)</span> <span class="math inline">\(\forall i\)</span>)</p>
<p><span class="math display">\[ f_n(x|\theta) = \theta^n\bigg[\underbrace{\prod(x_i)}_{r(x)}\bigg]^{\theta-1}  = \underbrace{\theta^n(r(x))^{\theta-1}}_{v(r(x),\theta)}\cdot \underbrace{1}_{u(x)}\]</span></p>
<p>Por el teorema de factorización <span class="math inline">\(r(x) = \prod x_i\)</span> es un estadístico suficiente.,</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim N(\mu, \sigma^2)\)</span> (<span class="math inline">\(\sigma^2\)</span> conocido).</p>
<p><span class="math display">\[\begin{align*} 
f_n(x|\theta) &amp; = (2\pi\sigma^2)^{-n/2} \exp\bigg[-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\mu)^2\bigg] \\
&amp; = (2\pi\sigma^2)^{-n/2} \exp\bigg[-\dfrac{1}{2\sigma^2}\underbrace{\sum_{i=1}^n X_i^2}_{r_2(x)}+ \dfrac{\mu}{\sigma^2}\underbrace{\sum_{i=1}^n X_i}_{r_1(x)} - \dfrac{\mu^2 n}{2\sigma^2} \bigg]
\end{align*}\]</span></p>
<p>Tome <span class="math display">\[u(x) = (2\pi\sigma^2)^{-n/2}\exp\bigg[-\dfrac{1}{2\sigma^2} \displaystyle\sum_{i=1}^n X_i^2\bigg],\]</span>
<span class="math display">\[ v(r_{1}(x),\mu) = \exp\bigg[\dfrac{\mu}{\sigma^2}r_{1}(x) - \dfrac{n\mu^2}{2\sigma^2}\bigg]. \]</span></p>
<p>Por teorema de factorización, <span class="math inline">\(r_{1}(x)=\sum X_i\)</span> es un estadístico suficiente para <span class="math inline">\(\mu\)</span>.</p>
<p>Con <span class="math inline">\(\sigma^2\)</span> desconocido, <span class="math inline">\(\theta = (\mu,\sigma^2)\)</span>, tome <span class="math inline">\(u(x) = 1\)</span>,
<span class="math display">\[ v(r_1(x),r_2(x),\theta) = (2\pi\sigma^2)^{-n/2}\exp\bigg[\dfrac{-r_2(x)}{2\sigma^2} + \dfrac{\mu r_1(x)}{\sigma^2}- \dfrac{n\mu^2}{2\sigma^2}\bigg] \]</span>
Entonces
<span class="math display">\[ (r_1(x),r_2(x)) = \left(\sum{x_i},\sum x_i ^2\right) \]</span>
es un estadístico suficiente para <span class="math inline">\((\mu, \sigma^2)\)</span>.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \stackrel{i.i.d}{\sim}\text{Unif}(0,\theta)\)</span>, <span class="math inline">\(\theta&gt;0\)</span>, <span class="math inline">\(f(x|\theta) = 1_{[0,\theta]}(x)\dfrac 1\theta\)</span>.
<span class="math display">\[f_n(x|\theta) = \prod_{i=1}^n 1_{[0,\theta]}(x_i)\left(\dfrac 1\theta \right) \]</span></p>
<p><em>Nota:</em> si al menos uno de los <span class="math inline">\(x_i&lt;0\)</span> o <span class="math inline">\(x_i&gt;\theta\)</span>, <span class="math inline">\(u(x) = 0\)</span> <span class="math inline">\((f(x|\theta) = 0)\)</span> (Trivial).</p>
<p>Si <span class="math inline">\(0&lt;x_i&lt;\theta\)</span> <span class="math inline">\(\forall i \implies f_n(x|\theta) = 1_{[0,\theta]}(\max\{x_i\})\left(\dfrac 1\theta \right)^n.\)</span></p>
<p>Si <span class="math inline">\(T = r(x) = X_{(n)} \implies f_n(x|\theta) = u(x)v(r(x),\theta)\)</span>, <span class="math inline">\(u(x) = 1\)</span>. Por teorema de factorización, <span class="math inline">\(r(x) = x_{(n)}\)</span> es un estadístico suficiente para <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="estadístico-suficiente-multivariado." class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Estadístico suficiente multivariado.</h2>
<p>Si <span class="math inline">\(\theta \in \mathbb R^k\)</span>, <span class="math inline">\(k\geq 1\)</span> se necesita al menos <span class="math inline">\(k\)</span> estadísticos
<span class="math inline">\((T_1,\dots,T_k)\)</span> para cada <span class="math inline">\(i=1,\dots,k\)</span>, <span class="math inline">\(T_i = r_i(X_1,\dots, X_n)\)</span>.</p>
<p><strong>Definición</strong>. Suponga que para cada <span class="math inline">\(\theta\in \Omega\)</span> y <span class="math inline">\((t_1,\dots, t_k) \in \mathbb R^k\)</span> valor del estadístico <span class="math inline">\((T_1,\dots,T_k)\)</span>, la distribución condicional de
<span class="math inline">\(X_1,\dots, X_n\)</span> dado <span class="math inline">\((T_1,\dots,T_k) = (t_1,\dots, t_k)\)</span> no depende de
<span class="math inline">\(\theta\)</span>, entonces <span class="math inline">\((T_1,\dots,T_k)\)</span> es un <strong>estadístico suficiente</strong> para
<span class="math inline">\(\theta\)</span>.</p>
<p><strong>Criterio de factorización</strong>:</p>
<p><span class="math display">\[ f_n(x|\theta) = u(x)v(r_1(x),\dots,r_k(x),\theta) \Leftrightarrow T = (r_1(x),\dots,r_k(x)) \text{ es
suficiente}\]</span></p>
<p>Si <span class="math inline">\((T_1,\dots,T_k)\)</span> es suficiente para <span class="math inline">\(\theta\)</span> y si <span class="math inline">\((T_1&#39;,\dots,T_k&#39;) = g(T_1,\dots,T_k)\)</span> donde <span class="math inline">\(g\)</span> es biyectiva, entonces <span class="math inline">\((T_1&#39;,\dots,T_k&#39;)\)</span> es
suficiente para <span class="math inline">\(\theta\)</span>. <span class="math display">\[ u(x)v(r(x)|\theta) = u(x)v(g^{-1}(g(r(x))),\theta).\]</span></p>
<p><strong>Ejemplo</strong>. Considere los siguiente</p>
<p><span class="math display">\[\begin{align*}
T_1 &amp;= \sum_{i=1}^{n} X_i \\
T_2 &amp;= \sum_{i=1}^{n} X_i^2 \\
T_1&#39; &amp; = \frac{1}{n} \sum_{i=1}^{n} X_i \\
T_2&#39; &amp;= \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X}_n) ^{2}
\end{align*}\]</span></p>
<p>Entonces defina la siguiente función
<span class="math display">\[
(T_1&#39;,T_2&#39;) = g(T_1,T_2) =
\left(\dfrac{1}{n}T_1,\dfrac{1}{n}T_2 - \dfrac{1}{n^2}T_1^2\right).
\]</span></p>
<p>De la primera entrada,
<span class="math display">\[ T_1&#39; = \dfrac 1n T_1 \implies T_1 = nT_1&#39;.\]</span>
De la segunda,
<span class="math display">\[\begin{align*}
T_2&#39; = \dfrac 1n T_2 - \dfrac 1{n^2} &amp; = \dfrac 1n \sum X_i^2 - \left(\dfrac 1n \sum X_i\right)^2\\
&amp; = \dfrac 1n \sum X_i^2 - 2X_i\bar X_n^2 + \bar X_n \\
&amp; = \dfrac 1n \sum(X_i-\bar X_n)^2 = \hat\sigma_n^2
\end{align*}\]</span>
Como <span class="math inline">\(g\)</span> es biyectiva entonces <span class="math inline">\((\bar X_n, \sigma_n^2)\)</span> es un estadístico suficiente para <span class="math inline">\((\mu,\sigma^2)\)</span>.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim \text{Unif}(a,b)\)</span>, <span class="math inline">\(a&lt;b\)</span>. Encuentre un estadístico suficiente.</p>
<ol style="list-style-type: decimal">
<li><p>Si <span class="math inline">\(x_i \leq a\)</span> o <span class="math inline">\(x_i&gt;b\)</span>, tome <span class="math inline">\(u(x) = 0\)</span>.</p></li>
<li><p>Si <span class="math inline">\(a&lt; x_i &lt;b\)</span> <span class="math inline">\(\forall i\)</span>,</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(x_i &gt; a\)</span> <span class="math inline">\(\forall i \Leftrightarrow x_{(1)}&gt;a\)</span>.</p></li>
<li><p><span class="math inline">\(x_i &lt; b\)</span> <span class="math inline">\(\forall i \Leftrightarrow x_{(n)}&lt;b\)</span>.</p></li>
</ol></li>
</ol>
<p>La verosimilitud es de la forma</p>
<p><span class="math display">\[f_n(x|(a,b)) = \prod_{i=1}^n1_{[a,b]}(x_i) = \underbrace{\dfrac 1{(b-a)^n} 1_{\{(z,w): z&gt;a, w&lt;b\}}(X_{(1)},X_{(n)})}_{v(r_1(x),r_2(x),(a,b))}\cdot \underbrace{1}_{u(x)}\]</span></p>
<p>Por teorema de factorización <span class="math inline">\((r_{1}(x), r_{2}(x)) = (X_{(1)},X_{(n)})\)</span> es un
estadístico suficiente para <span class="math inline">\((a,b)\)</span>.</p>
</div>
<div id="estadísticos-minimales" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Estadísticos minimales</h2>
<p><strong>Idea:</strong> un estadístico suficiente que garantice una partición de <span class="math inline">\(\mathcal X\)</span>
(espacio muestral) de la manera más simple posible.</p>
<p><strong>Definición (Estadístico de orden)</strong>. Sean <span class="math inline">\(X_1,\dots, X_n \stackrel{i.i.d}{\sim} f\)</span>. Al ordenar los datos</p>
<p><span class="math display">\[(Y_1,\dots,Y_n) = (X_{(1)},\dots,X_{(n)}) \text { tal
que } Y_1&lt;\dots&lt;Y_n\]</span></p>
<p><strong>Nota</strong>: <span class="math inline">\((X_{(1)},\dots,X_{(n)})\)</span> es un estadístico suficiente de <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim \text{Cauchy}(\alpha)\)</span>.
<span class="math display">\[ f(x) = \dfrac1\pi[1+(x-\alpha)^2]^{-1}, x\in\mathbb R\]</span></p>
<p>Busque un estimador suficiente para <span class="math inline">\(\alpha \in \mathbb R\)</span>.</p>
<p><span class="math display">\[ f_n(x|\alpha) = \prod(x|\alpha) = \dfrac 1\pi [1+(x_i-\alpha)^2]^{-1} = \underbrace{\dfrac 1{\pi^n}}_{u(x)}\underbrace{\prod_{i=1}^n[1+(x_i-\alpha)^2]^{-1} }_{v(y,\alpha)} \]</span>
donde <span class="math inline">\(y = (X_{(1)},\dots,X_{(n)})\)</span> es suficiente para <span class="math inline">\(\alpha\)</span>.</p>
<p><strong>Ejercicio</strong>: estime <span class="math inline">\(\alpha\)</span> usando R o usando método de momentos.</p>
<p><strong>Definición</strong>. Un estadístico <span class="math inline">\(T\)</span> es <strong>suficiente minimal</strong> si <span class="math inline">\(T\)</span> es
suficiente y es función de cualquier otro estadístico suficiente.</p>
<p><strong>Teorema</strong>. Si <span class="math inline">\(T = r(X_1,\dots, X_n)\)</span> es un estadístico suficiente para
<span class="math inline">\(\theta\)</span>, entonces el MLE <span class="math inline">\(\hat\theta\)</span> de <span class="math inline">\(\theta\)</span> depende de <span class="math inline">\(X_1,\dots, X_n\)</span>
solamente a través de <span class="math inline">\(T\)</span>. Además, si <span class="math inline">\(\hat \theta\)</span> es suficiente entonces <span class="math inline">\(\hat \theta\)</span> es minimal.</p>
<p><em>Prueba</em>. Por teorema de factorización, <span class="math inline">\(f_n(x|\theta) = u(x)v(r(x),\theta)\)</span> de <span class="math inline">\(T =r(x)\)</span>
es suficiente y
<span class="math display">\[\hat\theta = \operatorname*{argmax}_\theta f_n(x|\theta) = \operatorname*{argmax}_\theta v(r(x),\theta), \quad (\Delta)\]</span></p>
<p>Como <span class="math inline">\(\hat\theta = g(T)\)</span> para cualquier <span class="math inline">\(T\)</span> estadístico suficiente, entonces <span class="math inline">\(\hat\theta\)</span> es minimal.</p>
<p><strong>Teorema</strong>. Si <span class="math inline">\(T = r(X_1,\dots, X_n)\)</span> es un estadístico suficiente para
<span class="math inline">\(\theta\)</span> entonces el estimador bayesiano (bajo una escogencia de <span class="math inline">\(L\)</span>) depende de
<span class="math inline">\(X_1,\dots, X_n\)</span> solamente a través de <span class="math inline">\(T\)</span> (el estimador bayesiano es minimal).</p>
<p><em>Prueba</em>. Sustituya <span class="math inline">\((\Delta)\)</span> por <span class="math inline">\(\pi(\theta|x) \propto v(r(x),\theta)\cdot\pi(\theta)\)</span>. Como cualquier
estimador bayesiano depende de <span class="math inline">\(\pi(\theta|x)\)</span>, cualquier estimador bayesiano depende
e los datos a través de <span class="math inline">\(r(x)\)</span>.</p>
</div>
<div id="mejorando-estimadores" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Mejorando estimadores</h2>
<p><strong>Idea:</strong> ¿Será posible mejorar un estimar que no es suficiente?</p>
<p>¿Existirá otra medida de comparación entre estimadores?</p>
<p>Considere una <strong>función de riesgo o pérdida</strong>
<span class="math display">\[ R(\theta,\delta) = \mathbb E[(\delta(x)-\theta) ^2]\]</span>
Si <span class="math inline">\(\delta(x)\)</span> estima una característica de <span class="math inline">\(F\)</span>:
<span class="math display">\[ R(\theta,\delta) = \mathbb E[(\delta(x)-h(\theta))^2]\quad (\Delta\Delta)\]</span>
donde <span class="math inline">\(h\)</span> es la característica.</p>
<p><strong>Nota:</strong> la función de riesgo puede ser calculada con una posterior <span class="math inline">\(\pi(\theta|X)\)</span>.</p>
<p><strong>Definición</strong>.</p>
<ul>
<li><p>Decimos que <span class="math inline">\(\delta\)</span> es <strong>inadmisible</strong> si <span class="math inline">\(\exists \delta_0\)</span> (otro estimador) tal que
<span class="math inline">\(R(\theta, \delta_{0}) \leq R(\theta,\delta)\)</span> <span class="math inline">\(\forall \theta \in \Omega\)</span>.
deltadelta</p></li>
<li><p>Decimos que <span class="math inline">\(\delta_0\)</span> <strong>“domina”</strong> a <span class="math inline">\(\delta\)</span> en el caso anterior.</p></li>
<li><p>Decimos que <span class="math inline">\(\delta_0\)</span> es admisible si no existe otro estimador que domine a
<span class="math inline">\(\delta_0\)</span>.</p></li>
<li><p>A <span class="math inline">\((\Delta \Delta)\)</span> se le llama <strong>MSE</strong> o <strong>error cuadrático medio</strong>.</p></li>
</ul>
<p><strong>Teorema (Rao-Blackwell)</strong>. Sea <span class="math inline">\(\delta(X)\)</span> un estimador y <span class="math inline">\(T\)</span> un estadístico
suficiente para <span class="math inline">\(\theta\)</span> y sea <span class="math inline">\(\delta_0 = \mathbb E[\delta(X)|T]\)</span>. Entonces <span class="math display">\[ R(\theta,\delta_0) \leq
R(\theta,\delta) \; \forall \theta \in \Omega\]</span></p>
<p><em>Prueba</em>. Por la desigualdad de Jensen,
<span class="math display">\[ \mathbb E_\theta[(\delta(x)-\theta)^2] \geq (E_\theta[(\delta(x)-\theta)])^2. \]</span>
También,
<span class="math display">\[\mathbb E[(\delta(x)-\theta)^2|T] \geq (E[(\delta(x)|T)]-\theta)^2 = (\delta_0(T)-\theta)^2.\]</span></p>
<p>Entonces,
<span class="math display">\[ \mathbb E[(\delta(x)-\theta)^2] \leq \mathbb E[\mathbb E[(\delta(x)-\theta)^2|T]] = \mathbb E[(\delta(x)-\theta)^2] = R(\theta,\delta).\]</span></p>
<p><strong>Nota</strong>. Si cambiamos a <span class="math inline">\(R(\theta,\delta) = \mathbb E[|\delta(x)-\theta|]\)</span> (error medio absoluto), el resultado anterior es cierto.</p>
<p><strong>Ejemplo</strong>. Sean <span class="math inline">\(X_1,\dots, X_n \stackrel{i.i.d}{\sim} \text{Poisson}(\theta)\)</span> donde <span class="math inline">\(\theta\)</span> es la tasa de “visitas” de clientes por hora.</p>
<p>Numericamente podemos hacer el ejemplo con <span class="math inline">\(\theta = 2\)</span> y una muestra de <span class="math inline">\(n = 10000\)</span>,</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb98-1"></a>X &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dt">n =</span> <span class="dv">10000</span>, <span class="dt">lambda =</span> <span class="dv">2</span>)</span>
<span id="cb98-2"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb98-2"></a><span class="kw">head</span>(X, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>##  [1] 1 2 4 2 0 0 3 1 3 1 6 2 2 2 1 0 1 1 3 0</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb100-1"></a><span class="kw">hist</span>(X)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/unnamed-chunk-36-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>A partir de la verosimilitud,
<span class="math display">\[f_n(X|\theta) = \dfrac{e^{-\theta n} \theta^{\sum X_i}}{\prod X_i!} \]</span>
se tiene que <span class="math inline">\(T=\sum X_i\)</span> es un estadístico suficiente para <span class="math inline">\(\theta\)</span>.</p>
<p>Sea <span class="math inline">\(Y_i = \begin{cases} 1 &amp; \text{si } X_i = 1\\ 0 &amp; \text{si } X_i \ne 1\end{cases}\)</span>.</p>
<p>Esta <span class="math inline">\(Y\)</span> se calcula de la forma</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb101-1"></a>Y &lt;-<span class="st"> </span>X <span class="op">==</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb101-2"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb101-2"></a><span class="kw">head</span>(Y, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE</code></pre>
<p>El objetivo es estimar <span class="math inline">\(p\)</span> donde <span class="math inline">\(p\)</span> es la probabilidad de que <span class="math inline">\(X_i =1\)</span> (solo llegue un cliente por hora). Un estimador de <span class="math inline">\(p\)</span> (MLE) es
<span class="math display">\[\delta(x) = \dfrac{\sum Y_i}{n}\]</span></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb103-1"></a>(delta &lt;-<span class="st"> </span><span class="kw">mean</span>(Y))</span></code></pre></div>
<pre><code>## [1] 0.2734</code></pre>
<p>¿Es el óptimo?</p>
<p>Calculamos
<span class="math display">\[\mathbb E[\delta(x)|T] = \dfrac 1n \sum_{i=1}^n \mathbb E (Y_i|T)\]</span>
Vea que
<span class="math display">\[\begin{align*}
\mathbb E[Y_i|T = t] = \mathbb P(X_i = 1 | T = t) &amp; = \dfrac{\mathbb P(X_i = 1, T=t)}{\mathbb P(T=t)}\\
&amp; = \dfrac{\mathbb P(X_i = 1, \sum_{j\ne i} X_j = t-1)}{\mathbb P(T=t)}\\
&amp; = \dfrac{\mathbb P(X_i = 1) \mathbb P(\sum_{j\ne i} X_j = t-1)}{\mathbb P(T=t)} = \Delta
\end{align*}\]</span></p>
<ul>
<li><p><span class="math inline">\(\mathbb P(X_i = 1) = \theta e^{-\theta}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb P(\sum_{j\ne i}X_j = t-1) = e^{-(n-1)\theta}\dfrac{((n-1)\theta)^{t-1}}{(t-1)!}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb P(T=t) = e^{-n\theta}\dfrac{(n\theta)^t}{t!}\)</span></p></li>
</ul>
<p>Entonces,
<span class="math display">\[\begin{align*}
\Delta = \dfrac{\theta e^{-n\theta}\dfrac{((n-1)\theta)^{t-1}}{(t-1)!}}{e^{-n\theta}\dfrac{(n\theta)^t}{t!}} = \dfrac tn \left(1-\dfrac 1n\right)^{t-1} = G\left(\dfrac tn\right)
\end{align*}\]</span></p>
<p>es el estadístico con MSE mínimo.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb105-1"></a>T &lt;-<span class="st"> </span><span class="kw">sum</span>(X)</span>
<span id="cb105-2"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb105-2"></a>n &lt;-<span class="st"> </span><span class="kw">length</span>(X)</span>
<span id="cb105-3"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#cb105-3"></a>(delta_<span class="dv">0</span> &lt;-<span class="st"> </span>(T<span class="op">/</span>n) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n)<span class="op">^</span>(T <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.2686675</code></pre>
<p>En este caso <span class="math inline">\(\delta_0\)</span> es mejor que <span class="math inline">\(\delta\)</span> bajo una pérdida cuadrática.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimación-por-máxima-verosimilitud.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distribución-muestral-de-un-estadístico.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/edit/master/04-estadisticos-suficientes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/blob/master/04-estadisticos-suficientes.Rmd",
"text": null
},
"download": ["Notas-Curso-Estadistica.pdf"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 5
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
