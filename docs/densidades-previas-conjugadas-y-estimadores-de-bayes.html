<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Densidades previas conjugadas y estimadores de Bayes | Notas Curso de Estadística (Parte I)</title>
  <meta name="description" content="Capítulo 3 Densidades previas conjugadas y estimadores de Bayes | Notas Curso de Estadística (Parte I)" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Densidades previas conjugadas y estimadores de Bayes | Notas Curso de Estadística (Parte I)" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Densidades previas conjugadas y estimadores de Bayes | Notas Curso de Estadística (Parte I)" />
  
  
  

<meta name="author" content="Maikol Solís" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inferencia-estadística.html"/>
<link rel="next" href="estimación-por-máxima-verosimilitud.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Curso de Estadística</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html"><i class="fa fa-check"></i><b>2</b> Inferencia estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#ejemplo"><i class="fa fa-check"></i><b>2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="2.2" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#modelo-estadístico"><i class="fa fa-check"></i><b>2.2</b> Modelo estadístico</a></li>
<li class="chapter" data-level="2.3" data-path="inferencia-estadística.html"><a href="inferencia-estadística.html#estadístico"><i class="fa fa-check"></i><b>2.3</b> Estadístico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><i class="fa fa-check"></i><b>3</b> Densidades previas conjugadas y estimadores de Bayes</a>
<ul>
<li class="chapter" data-level="3.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-previa-distribución-a-priori"><i class="fa fa-check"></i><b>3.1</b> Distribución previa (distribución a priori)</a></li>
<li class="chapter" data-level="3.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidad-posterior"><i class="fa fa-check"></i><b>3.2</b> Densidad posterior</a></li>
<li class="chapter" data-level="3.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#proceso-de-modelación-de-parámetros."><i class="fa fa-check"></i><b>3.3</b> Proceso de modelación de parámetros.</a></li>
<li class="chapter" data-level="3.4" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-verosimilitud"><i class="fa fa-check"></i><b>3.4</b> Función de verosimilitud</a></li>
<li class="chapter" data-level="3.5" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#familias-conjugadas"><i class="fa fa-check"></i><b>3.5</b> Familias conjugadas</a></li>
<li class="chapter" data-level="3.6" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#densidades-previas-impropias"><i class="fa fa-check"></i><b>3.6</b> Densidades previas impropias</a></li>
<li class="chapter" data-level="3.7" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7</b> Funciones de pérdida</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-cuadrática"><i class="fa fa-check"></i><b>3.7.1</b> Función de pérdida cuadrática</a></li>
<li class="chapter" data-level="3.7.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#función-de-pérdida-absoluta"><i class="fa fa-check"></i><b>3.7.2</b> Función de pérdida absoluta</a></li>
<li class="chapter" data-level="3.7.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#otras-funciones-de-pérdida"><i class="fa fa-check"></i><b>3.7.3</b> Otras funciones de pérdida</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#efecto-de-muestras-grandes"><i class="fa fa-check"></i><b>3.8</b> Efecto de muestras grandes</a></li>
<li class="chapter" data-level="3.9" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#consistencia"><i class="fa fa-check"></i><b>3.9</b> Consistencia</a></li>
<li class="chapter" data-level="3.10" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#laboratorio"><i class="fa fa-check"></i><b>3.10</b> Laboratorio</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-previa"><i class="fa fa-check"></i><b>3.10.1</b> Distribución previa</a></li>
<li class="chapter" data-level="3.10.2" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-conjunta"><i class="fa fa-check"></i><b>3.10.2</b> Distribución conjunta</a></li>
<li class="chapter" data-level="3.10.3" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#distribución-posterior"><i class="fa fa-check"></i><b>3.10.3</b> Distribución posterior</a></li>
<li class="chapter" data-level="3.10.4" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#agregando-nuevos-datos"><i class="fa fa-check"></i><b>3.10.4</b> Agregando nuevos datos</a></li>
<li class="chapter" data-level="3.10.5" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#familias-conjugadas-normales"><i class="fa fa-check"></i><b>3.10.5</b> Familias conjugadas normales</a></li>
<li class="chapter" data-level="3.10.6" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#funciones-de-pérdida-1"><i class="fa fa-check"></i><b>3.10.6</b> Funciones de pérdida</a></li>
<li class="chapter" data-level="3.10.7" data-path="densidades-previas-conjugadas-y-estimadores-de-bayes.html"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#caso-concreto"><i class="fa fa-check"></i><b>3.10.7</b> Caso concreto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html"><i class="fa fa-check"></i><b>4</b> Estimación por máxima verosimilitud</a>
<ul>
<li class="chapter" data-level="4.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#propiedades-del-mle"><i class="fa fa-check"></i><b>4.1</b> Propiedades del MLE</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#propiedad-de-invarianza"><i class="fa fa-check"></i><b>4.1.1</b> Propiedad de invarianza</a></li>
<li class="chapter" data-level="4.1.2" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#consistencia-1"><i class="fa fa-check"></i><b>4.1.2</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#cálculo-numérico"><i class="fa fa-check"></i><b>4.2</b> Cálculo numérico</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#método-de-los-momentos"><i class="fa fa-check"></i><b>4.2.1</b> Método de los momentos</a></li>
<li class="chapter" data-level="4.2.2" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#método-delta"><i class="fa fa-check"></i><b>4.2.2</b> Método Delta</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="estimación-por-máxima-verosimilitud.html"><a href="estimación-por-máxima-verosimilitud.html#laboratorio-1"><i class="fa fa-check"></i><b>4.3</b> Laboratorio</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html"><i class="fa fa-check"></i><b>5</b> Estadísticos Suficientes y Criterio de Factorización</a>
<ul>
<li class="chapter" data-level="5.1" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadísticos-suficientes"><i class="fa fa-check"></i><b>5.1</b> Estadísticos suficientes</a></li>
<li class="chapter" data-level="5.2" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#teorema-de-factorización-de-fisher"><i class="fa fa-check"></i><b>5.2</b> Teorema de Factorización de Fisher</a></li>
<li class="chapter" data-level="5.3" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadístico-suficiente-multivariado."><i class="fa fa-check"></i><b>5.3</b> Estadístico suficiente multivariado.</a></li>
<li class="chapter" data-level="5.4" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#estadísticos-minimales"><i class="fa fa-check"></i><b>5.4</b> Estadísticos minimales</a></li>
<li class="chapter" data-level="5.5" data-path="estadísticos-suficientes-y-criterio-de-factorización.html"><a href="estadísticos-suficientes-y-criterio-de-factorización.html#mejorando-estimadores"><i class="fa fa-check"></i><b>5.5</b> Mejorando estimadores</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html"><i class="fa fa-check"></i><b>6</b> Distribución muestral de un estadístico</a>
<ul>
<li class="chapter" data-level="6.1" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-muestral"><i class="fa fa-check"></i><b>6.1</b> Distribución muestral</a></li>
<li class="chapter" data-level="6.2" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-chi2"><i class="fa fa-check"></i><b>6.2</b> Distribución <span class="math inline">\(\chi^2\)</span></a></li>
<li class="chapter" data-level="6.3" data-path="distribución-muestral-de-un-estadístico.html"><a href="distribución-muestral-de-un-estadístico.html#distribución-t"><i class="fa fa-check"></i><b>6.3</b> Distribución <span class="math inline">\(t\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html"><i class="fa fa-check"></i><b>7</b> Intervalos de confianza</a>
<ul>
<li class="chapter" data-level="7.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-para-la-media-de-una-distribución-normal"><i class="fa fa-check"></i><b>7.1</b> Intervalos de confianza para la media de una distribución normal</a></li>
<li class="chapter" data-level="7.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#caso-normal."><i class="fa fa-check"></i><b>7.2</b> Caso normal.</a></li>
<li class="chapter" data-level="7.3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-abiertos"><i class="fa fa-check"></i><b>7.3</b> Intervalos de confianza abiertos</a></li>
<li class="chapter" data-level="7.4" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-en-otros-casos"><i class="fa fa-check"></i><b>7.4</b> Intervalos de confianza en otros casos</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#intervalos-de-confianza-aproximados."><i class="fa fa-check"></i><b>7.4.1</b> Intervalos de confianza aproximados.</a></li>
<li class="chapter" data-level="7.4.2" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html#transformaciones-estabilizadoras-de-la-varianza"><i class="fa fa-check"></i><b>7.4.2</b> Transformaciones estabilizadoras de la varianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html"><i class="fa fa-check"></i><b>8</b> Estimación Bayesiana bajo normalidad</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#precisión-de-una-distribución-normal"><i class="fa fa-check"></i><b>8.1</b> Precisión de una distribución normal</a></li>
<li class="chapter" data-level="8.2" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#distribución-marginal-de-mu"><i class="fa fa-check"></i><b>8.2</b> Distribución marginal de <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.3" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#intervalos-de-credibilidad."><i class="fa fa-check"></i><b>8.3</b> Intervalos de credibilidad.</a></li>
<li class="chapter" data-level="8.4" data-path="estimación-bayesiana-bajo-normalidad.html"><a href="estimación-bayesiana-bajo-normalidad.html#efecto-de-previas-no-informativas-opcional"><i class="fa fa-check"></i><b>8.4</b> Efecto de previas no informativas (Opcional)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html"><i class="fa fa-check"></i><b>9</b> Estimación insesgada</a>
<ul>
<li class="chapter" data-level="9.1" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimadores-insesgados"><i class="fa fa-check"></i><b>9.1</b> Estimadores insesgados</a></li>
<li class="chapter" data-level="9.2" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimador-insesgado-de-la-varianza"><i class="fa fa-check"></i><b>9.2</b> Estimador insesgado de la varianza</a></li>
<li class="chapter" data-level="9.3" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#información-de-fisher"><i class="fa fa-check"></i><b>9.3</b> Información de Fisher</a></li>
<li class="chapter" data-level="9.4" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#desigualdad-de-cramer-rao"><i class="fa fa-check"></i><b>9.4</b> Desigualdad de Cramer-Rao</a></li>
<li class="chapter" data-level="9.5" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#estimadores-eficientes"><i class="fa fa-check"></i><b>9.5</b> Estimadores eficientes</a></li>
<li class="chapter" data-level="9.6" data-path="estimación-insesgada.html"><a href="estimación-insesgada.html#comportamiento-asintótico-del-mle"><i class="fa fa-check"></i><b>9.6</b> Comportamiento asintótico del MLE</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html"><i class="fa fa-check"></i><b>10</b> Pruebas de hipótesis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-de-hipótesis-1"><i class="fa fa-check"></i><b>10.1</b> Pruebas de hipótesis</a></li>
<li class="chapter" data-level="10.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#regiones-críticas-y-estadísticas-de-prueba"><i class="fa fa-check"></i><b>10.2</b> Regiones críticas y estadísticas de prueba</a></li>
<li class="chapter" data-level="10.3" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#función-de-potencia-y-tipos-de-error"><i class="fa fa-check"></i><b>10.3</b> Función de potencia y tipos de error</a></li>
<li class="chapter" data-level="10.4" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#valor-p"><i class="fa fa-check"></i><b>10.4</b> Valor <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="10.5" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#dualidad-entre-pruebas-de-hipótesis-y-regiones-de-confianza"><i class="fa fa-check"></i><b>10.5</b> Dualidad entre pruebas de hipótesis y regiones de confianza</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#dualidad-en-pruebas-unilaterales"><i class="fa fa-check"></i><b>10.5.1</b> Dualidad en pruebas unilaterales</a></li>
<li class="chapter" data-level="10.5.2" data-path="pruebas-de-hipótesis.html"><a href="pruebas-de-hipótesis.html#pruebas-de-cociente-de-verosimilitud-lrt"><i class="fa fa-check"></i><b>10.5.2</b> Pruebas de cociente de verosimilitud (LRT)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html"><i class="fa fa-check"></i><b>11</b> Pruebas con hipótesis simples</a>
<ul>
<li class="chapter" data-level="11.1" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#hipótesis-simples"><i class="fa fa-check"></i><b>11.1</b> Hipótesis simples</a></li>
<li class="chapter" data-level="11.2" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#criterio-de-neyman-pearson"><i class="fa fa-check"></i><b>11.2</b> Criterio de Neyman-Pearson</a></li>
<li class="chapter" data-level="11.3" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#pruebas-insesgadas"><i class="fa fa-check"></i><b>11.3</b> Pruebas insesgadas</a></li>
<li class="chapter" data-level="11.4" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#prueba-t"><i class="fa fa-check"></i><b>11.4</b> Prueba <span class="math inline">\(t\)</span></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#propiedades-de-las-pruebas-t"><i class="fa fa-check"></i><b>11.4.1</b> Propiedades de las pruebas <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="11.4.2" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#prueba-t-pareada"><i class="fa fa-check"></i><b>11.4.2</b> Prueba <span class="math inline">\(t\)</span> pareada</a></li>
<li class="chapter" data-level="11.4.3" data-path="pruebas-con-hipótesis-simples.html"><a href="pruebas-con-hipótesis-simples.html#pruebas-t-de-dos-colas"><i class="fa fa-check"></i><b>11.4.3</b> Pruebas <span class="math inline">\(t\)</span> de dos colas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html"><i class="fa fa-check"></i><b>12</b> Prueba de comparación de medias en 2 poblaciones</a>
<ul>
<li class="chapter" data-level="12.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#comparación-de-medias-normales"><i class="fa fa-check"></i><b>12.1</b> Comparación de medias normales</a></li>
<li class="chapter" data-level="12.2" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-t-de-dos-muestras"><i class="fa fa-check"></i><b>12.2</b> Prueba <span class="math inline">\(t\)</span> de dos muestras</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-de-2-colas"><i class="fa fa-check"></i><b>12.2.1</b> Prueba de 2 colas</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-f"><i class="fa fa-check"></i><b>12.3</b> Prueba <span class="math inline">\(F\)</span></a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="prueba-de-comparación-de-medias-en-2-poblaciones.html"><a href="prueba-de-comparación-de-medias-en-2-poblaciones.html#prueba-de-2-colas-prueba-de-homocedasticidad"><i class="fa fa-check"></i><b>12.3.1</b> Prueba de 2 colas (prueba de homocedasticidad)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bondad-de-ajuste.html"><a href="bondad-de-ajuste.html"><i class="fa fa-check"></i><b>13</b> Bondad de ajuste</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bondad-de-ajuste.html"><a href="bondad-de-ajuste.html#prueba-chi2"><i class="fa fa-check"></i><b>13.1</b> Prueba <span class="math inline">\(\chi^2\)</span></a></li>
<li class="chapter" data-level="13.2" data-path="bondad-de-ajuste.html"><a href="bondad-de-ajuste.html#pruebas-chi2-con-hipótesis-parametrizadas"><i class="fa fa-check"></i><b>13.2</b> Pruebas <span class="math inline">\(\chi^2\)</span> con hipótesis parametrizadas</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html"><i class="fa fa-check"></i><b>14</b> Tablas de contingencia</a>
<ul>
<li class="chapter" data-level="14.1" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#prueba-de-independencia"><i class="fa fa-check"></i><b>14.1</b> Prueba de independencia</a></li>
<li class="chapter" data-level="14.2" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#prueba-de-homogeneidad"><i class="fa fa-check"></i><b>14.2</b> Prueba de homogeneidad</a></li>
<li class="chapter" data-level="14.3" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#similitudes-entre-las-pruebas-de-independecia-y-homogeneidad"><i class="fa fa-check"></i><b>14.3</b> Similitudes entre las pruebas de independecia y homogeneidad</a></li>
<li class="chapter" data-level="14.4" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#comparación-de-dos-o-más-proporciones"><i class="fa fa-check"></i><b>14.4</b> Comparación de dos o más proporciones</a></li>
<li class="chapter" data-level="14.5" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#paradoja-de-simpson"><i class="fa fa-check"></i><b>14.5</b> Paradoja de Simpson</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="tablas-de-contingencia.html"><a href="tablas-de-contingencia.html#cómo-evitamos-esta-paradoja"><i class="fa fa-check"></i><b>14.5.1</b> ¿Cómo evitamos esta paradoja?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pruebas-de-kolmogorov-smirnov.html"><a href="pruebas-de-kolmogorov-smirnov.html"><i class="fa fa-check"></i><b>15</b> Pruebas de Kolmogorov-Smirnov</a>
<ul>
<li class="chapter" data-level="15.1" data-path="pruebas-de-kolmogorov-smirnov.html"><a href="pruebas-de-kolmogorov-smirnov.html#prueba-de-kolmogorov-smirnov-para-una-muestra"><i class="fa fa-check"></i><b>15.1</b> Prueba de Kolmogorov-Smirnov para una muestra</a></li>
<li class="chapter" data-level="15.2" data-path="pruebas-de-kolmogorov-smirnov.html"><a href="pruebas-de-kolmogorov-smirnov.html#prueba-de-2-muestras"><i class="fa fa-check"></i><b>15.2</b> Prueba de 2 muestras</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html"><a href="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html"><i class="fa fa-check"></i><b>16</b> Pruebas no-paramétricas: pruebas de signo y rango</a>
<ul>
<li class="chapter" data-level="16.1" data-path="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html"><a href="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html#prueba-de-signo"><i class="fa fa-check"></i><b>16.1</b> Prueba de signo</a></li>
<li class="chapter" data-level="16.2" data-path="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html"><a href="pruebas-no-paramétricas-pruebas-de-signo-y-rango.html#prueba-de-wilconxon-mann-whitney"><i class="fa fa-check"></i><b>16.2</b> Prueba de Wilconxon-Mann-Whitney</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ejercicios-varios.html"><a href="ejercicios-varios.html"><i class="fa fa-check"></i><b>17</b> Ejercicios varios</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ejercicios-varios.html"><a href="ejercicios-varios.html#capítulo-8"><i class="fa fa-check"></i><b>17.1</b> Capítulo 8</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="ejercicios-varios.html"><a href="ejercicios-varios.html#section"><i class="fa fa-check"></i><b>17.1.1</b> 8.4.6</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notas Curso de Estadística (Parte I)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="densidades-previas-conjugadas-y-estimadores-de-bayes" class="section level1" number="3">
<h1><span class="header-section-number">Capítulo 3</span> Densidades previas conjugadas y estimadores de Bayes</h1>
<div id="distribución-previa-distribución-a-priori" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Distribución previa (distribución a priori)</h2>
<p>Suponga que tenemos un modelo estadístico con parámetro <span class="math inline">\(\theta\)</span>. Su <span class="math inline">\(\theta\)</span> es aleatorio entonces su densidad (antes de observar cualquier muestra) se llama <strong>densidad previa</strong>: <span class="math inline">\(\pi\)</span>.</p>
<p><strong>Ejemplo</strong>: <span class="math inline">\(X_1,\dots, X_n \sim \text{Exp}(\theta)\)</span> y <span class="math inline">\(\theta\)</span> es aleatorio tal que <span class="math inline">\(\theta \sim \Gamma(\stackrel{\alpha}{1},\stackrel{\beta}{2})\)</span> entonces</p>
<p><span class="math display">\[ \pi(\theta) = \dfrac{1}{\Gamma(\alpha)}\beta^\alpha\theta^{\alpha-1}e^{\beta\theta} = 2e^{-2\theta}, \quad \theta &gt; 0\]</span></p>
<p><strong>Ejemplo</strong>: Sea <span class="math inline">\(\theta\)</span> la probabilidad de obtener cara al tirar una moneda.</p>
<p>En este caso antes de modelar exactamente el <span class="math inline">\(\theta\)</span>, lo importante es
modelar el tipo de moneda. Es decir, supongamos que tenemos dos opciones</p>
<ul>
<li><p><em>Moneda justa:</em> <span class="math inline">\(\theta = \dfrac{1}{2}\)</span> con probabilidad previa <span class="math inline">\(0.8\)</span> (<span class="math inline">\(\pi(\frac{1}{2}) = 0.8\)</span>).</p></li>
<li><p><em>Moneda con solo una cara:</em> <span class="math inline">\(\theta = 1\)</span> con probabilidad previa <span class="math inline">\(0.2\)</span> (<span class="math inline">\(\pi(1) = 0.2\)</span>).</p></li>
</ul>
<p>En este ejemplo si tuviéramos 100 monedas con probabilidad previa <span class="math inline">\(\pi\)</span>
entonces 20 tendrían solo una cara y 80 serían monedas normales.</p>
<p><strong>Notas</strong>:</p>
<ul>
<li><p><span class="math inline">\(\pi\)</span> está definida en <span class="math inline">\(\Omega\)</span> (espacio paramétrico).</p></li>
<li><p><span class="math inline">\(\pi\)</span> es definida antes de obtener la muestra.</p></li>
</ul>
<p><strong>Ejemplo</strong> (Componentes eléctricos) Supoga que se quiere conocer el tiempo de
vida de cierto componente eléctrico. Sabemos que este tiempo se puede modelar
con una distribución exponencial con parámetro <span class="math inline">\(\theta\)</span> desconocido.
Este parámetro asumimos que tiene una distribución previa Gamma.</p>
<p>Un experto en componentes eléctricos conoce mucho de su área y sabe
que el parámetro <span class="math inline">\(\theta\)</span> tiene las siguientes características:</p>
<p><span class="math display">\[\begin{equation*}
\mathbb{E}[\theta] = 0.0002, \quad \sqrt{\text{Var}(\theta)} = 0.0001.
\end{equation*}\]</span></p>
<p>Como sabemos que la previa <span class="math inline">\(\pi\)</span> es Gamma, podemos deducir lo siguiente:</p>
<p><span class="math display">\[\begin{equation*}
 \mathbb{E}[\theta] = \dfrac{\alpha}{\beta}, \text{Var}(\theta) = \dfrac{\alpha}{\beta^2}
\end{equation*}\]</span></p>
<p><span class="math display">\[\implies \begin{cases}\dfrac{\alpha}{\beta} = 2\times 10^{-4}\\\sqrt{\dfrac{\alpha}{\beta^2}} = 1 \times 10^{-4}\end{cases} \implies \beta = 20000, \alpha = 4\]</span></p>
<p><strong>Notación</strong>:</p>
<ul>
<li><p><span class="math inline">\(X = (X_1,\dots, X_n)\)</span>: vector que contiene la muestra aleatoria.</p></li>
<li><p>Densidad conjunta de <span class="math inline">\(X\)</span>: <span class="math inline">\(f_\theta(x)\)</span>.</p></li>
<li><p>Densidad de <span class="math inline">\(X\)</span> condicional en <span class="math inline">\(\theta\)</span>: <span class="math inline">\(f_n(x|\theta)\)</span>.</p></li>
</ul>
<p><strong>Supuesto</strong>: <span class="math inline">\(X\)</span> viene de una muestra aleatoria si y solo si <span class="math inline">\(X\)</span> es condicionalmente independiente dado <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Consecuencia</strong>: <span class="math display">\[f_n(X|\theta) = f(X_1|\theta)\cdot f(X_2|\theta)\cdots f(X_n|\theta)\]</span></p>
<p><strong>Ejemplo</strong></p>
<p>Si <span class="math inline">\(X = (X_1,\dots, X_n)\)</span> es una muestra tal que <span class="math inline">\(X_i\sim \text{Exp}(\theta)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
f_n(X|\theta) &amp;= \begin{cases}\prod_{i=1}^n \theta e^{-\theta X_i} &amp; \text{si } X_i&gt;0\\
0 &amp; \text{si no}
\end{cases}  \\
&amp;= \begin{cases}\theta^n e^{-\theta\sum_{i=1}^n X_i} &amp; X_i &gt; 0  \\ 0 &amp; \text{si no}\end{cases}
\end{align*}\]</span></p>
</div>
<div id="densidad-posterior" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Densidad posterior</h2>
<p><strong>Definición</strong>. Considere un modelo estadístico con parámetro <span class="math inline">\(\theta\)</span> y muestra
aleatoria <span class="math inline">\(X_1,\dots, X_n\)</span>. La densidad condicional de <span class="math inline">\(\theta\)</span> dado
<span class="math inline">\(X_1,\dots,X_n\)</span> se llama <em>densidad posterior</em>: <span class="math inline">\(\pi(\theta|X)\)</span></p>
<p><strong>Teorema</strong>. Bajo las condiciones anteriores:</p>
<p><span class="math display">\[\begin{equation*}
\pi(\theta|X) =
\dfrac{f(X_1|\theta)\cdots f(X_n|\theta)\pi(\theta)}{g_n(X)} 
\end{equation*}\]</span></p>
<p>para <span class="math inline">\(\theta \in \Omega\)</span>, donde <span class="math inline">\(g_n\)</span> es una constante de
normalización.</p>
<p><em>Prueba</em>:
<span class="math display">\[\begin{align*}
\pi(\theta|X) &amp; = \dfrac{\pi(\theta,X)}{\text{marginal de X}} = \dfrac{\pi(\theta,X)}{\int \pi(\theta,X)\;d\theta}= \dfrac{P(X|\theta)\cdot \pi(\theta)}{\int \pi(\theta,X)\;d\theta}\\
&amp; \dfrac{f_n(X|\theta)\cdot \pi(\theta)}{g_n(X)} = \dfrac{f(X_1|\theta)\cdots f(X_n|\theta)\pi(\theta)}{g_n(X)}
\end{align*}\]</span></p>
<p>Del ejemplo anterior,</p>
<p><span class="math display">\[f_n(X|\theta) = \theta^n e^{-\theta y}, y = \sum{X_i} \text{ (estadístico})\]</span>
Numerador:</p>
<p><span class="math display">\[f_n(X|\theta)\pi(\theta) = \underbrace{\theta^n e^{-\theta y}}_{f_n(X|\theta)} \cdot \underbrace{\dfrac{200000^4}{3!}\theta^3e^{-20000\cdot\theta}}_{\pi(\theta)} = \dfrac{20000^4}{3!}\theta^{n+3}e^{(20000+y)\theta}\]</span></p>
<p>Denominador:</p>
<p><span class="math display">\[g_n(x) = \int_{0}^{+\infty}\theta^{n+3}e^{-(20000+y)\theta}\;d\theta = \dfrac{\Gamma(n+4)}{(20000+y)^{n+4}}\]</span></p>
<p>Entonces la posterior corresponde a
<span class="math display">\[\pi(\theta|X) = \dfrac{\theta^{n+3}e^{-(20000+y)\theta}}{\Gamma(n+4)} (20000+y)^{n+4}\]</span>
que es una <span class="math inline">\(\Gamma(n+4,20000+y)\)</span>.</p>
<p>Con 5 observaciones (horas): 2911, 3403, 3237, 3509, 3118.
<span class="math display">\[y = \sum_{i=1}^{5}X_i = 16478, \quad n= 5\]</span>
por lo que <span class="math inline">\(\theta|X \sim \Gamma(9,36178)\)</span></p>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-1-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>Es sensible al tamaño de la muestra (una muestra grande implica un efecto de la previa menor).</p>
<p><strong>Hiperparámetros</strong>: parámetros de la previa o posterior.</p>
</div>
<div id="proceso-de-modelación-de-parámetros." class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Proceso de modelación de parámetros.</h2>
<p>De ahora en adelante vamos a entender un modelo como el conjunto de los datos
<span class="math inline">\(X_1, \ldots, X_n\)</span>, la función de densidad <span class="math inline">\(f\)</span> y el parámetro de la densidad
<span class="math inline">\(\theta\)</span>. Estos dos últimos resumen el comportamiento de los datos.</p>
<p>Ahora para identificar este modelo se hace por partes,</p>
<ol style="list-style-type: decimal">
<li>La información previa <span class="math inline">\(\pi(\theta)\)</span> es la información extra o basado en la
experiencia que tengo del mdoelo.</li>
<li>Los datos es la información observada. La función de densidad <span class="math inline">\(f\)</span> filtra y
mejora la información de la previa.</li>
<li>La densidad posterior es la “mezcla” entre la información y los datos
observados. Es una versión más informada de la distribución del parámetro.</li>
</ol>
</div>
<div id="función-de-verosimilitud" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Función de verosimilitud</h2>
<p>Bajo el modelo estadístico anterior a <span class="math inline">\(f_n(X|\theta)\)</span> se le llama <strong>verosimilitud</strong> o <strong>función de verosimilitud</strong>.</p>
<p><strong>Observación</strong>. En el caso de una función de verosimilitud, el argumento es <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Ejemplo</strong>.</p>
<p>Sea <span class="math inline">\(\theta\)</span> la proporción de aparatos defectuosos, con <span class="math inline">\(\theta \in [0,1]\)</span>
<span class="math display">\[ X_i = \begin{cases}
 0 &amp; \text{falló} \\
 1 &amp; \text{no falló}
 \end{cases}\]</span></p>
<p><span class="math inline">\(\{X_i\}_{i=1}^n\)</span> es una muestra aleatoria y <span class="math inline">\(X_i \sim Ber(\theta)\)</span>.</p>
<ul>
<li><strong>Verosimilitud</strong></li>
</ul>
<p><span class="math display">\[ f_n(X|\theta) = \prod_{i=1}^n f(X_i|\theta) = \begin{cases}\theta^{\sum X_i}(1-\theta)^{n-\sum X_i} &amp; X_i = 0,1\; \forall i\\ 0 &amp; \text{si no}\end{cases}\]</span></p>
<ul>
<li><p><strong>Previa</strong>:
<span class="math display">\[\pi(\theta) = 1_{\{0\leq\theta\leq 1\}}\]</span></p></li>
<li><p><strong>Posterior</strong>:</p></li>
</ul>
<p>Por el teorema de Bayes,
<span class="math display">\[\begin{align*}
\pi(\theta|X) \propto \theta^y (1-\theta)^{n-y}\cdot 1  \\
&amp;= \theta^{\overbrace{y+1}^{\alpha}-1}(1-\theta)^{\overbrace{n-y+1}^{\beta}-1}
&amp;\implies \theta|X \sim \text{Beta}(y+1,n-y+1)
\end{align*}\]</span></p>
<ul>
<li><strong>Predicción</strong>.</li>
</ul>
<p><em>Supuesto</em>: los datos son secuenciales. Calculamos la distribución posterior
secuencialmente:</p>
<p><span class="math display">\[\begin{align*}
\pi(\theta|X_1) &amp; \propto \pi(\theta) f(X_1|\theta)\\
\pi(\theta|X_1,X_2) &amp;\propto \pi(\theta) f(X_1,X_2|\theta) \\
&amp;= \pi(\theta) f(X_1|\theta) f(X_2|\theta) \text{ (por independencia condicional)}
\\ &amp; = \pi(\theta|X_1)f(X_2|\theta)\\
\vdots &amp;  \\
\pi(\theta|X_1,\dots,X_n) &amp; \propto f(X_n|\theta)\pi(\theta|X_1,\dots, X_{n-1})
\end{align*}\]</span></p>
<p>Bajo independencia condicional no hay diferencia en la posterior si los datos
son secuenciales.</p>
<p>Luego,</p>
<p><span class="math display">\[\begin{align*} 
g_n(X) &amp; = \int_{\Omega} f(X_n|\theta) \pi(\theta|X_1,\dots, X_{n-1})\;d\theta\\
&amp; = P(X_n|X_1,\dots,X_{n-1}) \text{ (Predicción para }X_n)
\end{align*}\]</span></p>
<p>Continuando con el ejemplo de los artefactos, <span class="math inline">\(P(X_6&gt;3000|X_1,X_2,X_3,X_4,X_5)\)</span>.
Se necesita calcular <span class="math inline">\(f(X_6|X)\)</span>. Dado que <span class="math display">\[ \pi(\theta|X) = 2.6\times
10^{36}\theta^8 e^{-36178\theta}\]</span></p>
<p>se tiene</p>
<p><span class="math display">\[ f(X_6|X) = 2.6\times 10^{36} \int_{0}^1 \underbrace{\theta e^{-\theta
X_6}}_{\text{Densidad de } X_6}\theta^8 e^{-36178\theta}\;d\theta = \dfrac{9.55
\times 10^{41}}{(X_6+36178)^{10}}\]</span></p>
<p>Entonces,</p>
<p><span class="math display">\[ P(X_6&gt;3000) =
\int_{3000}^{\infty} \dfrac{9.55\times10^{41}}{(X_6+36178)^{10}}\; dX_6 =
0.4882\]</span></p>
<p>La vida media se calcula como <span class="math inline">\(\dfrac{1}{2} = P(X_6&gt;u|X)\)</span>.</p>
</div>
<div id="familias-conjugadas" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Familias conjugadas</h2>
<p><strong>Definición</strong>. Sea <span class="math inline">\(X_1,\dots, X_n\)</span> i.i.d. condicional dado <span class="math inline">\(\theta\)</span> con
densidad <span class="math inline">\(f(X|\theta)\)</span>. Sea <span class="math inline">\(\psi\)</span> la familia de posibles densidades previas
sobre <span class="math inline">\(\Omega\)</span>. Si, sin importar los datos, la posterior pertenece a <span class="math inline">\(\psi\)</span>,
entonces decimos que <span class="math inline">\(\psi\)</span> es una familia conjugada de previas.</p>
<p><strong>Ejemplos</strong>:</p>
<ul>
<li><p>La familia Beta es familia conjugada para muestras según una Bernoulli.</p></li>
<li><p>La familia Gama es familia conjugada para muestras exponenciales.</p></li>
<li><p>Para el caso Poisson, si <span class="math inline">\(X_1,\dots,X_n\sim Poi(\lambda)\)</span>,entonces la familia
Gamma es familia conjugada.</p></li>
</ul>
<p>La función de densidad de una Poisson es <span class="math inline">\(P(X_i = k) = e^{-\lambda}\dfrac{\lambda^k}{k!}\)</span>. La verosimilitud corresponde a
<span class="math display">\[ f_n(X|\lambda) = \prod_{i=1}^{n}e^{-\lambda}\dfrac{\lambda^X_i}{X_i!} = \dfrac{e^{-n\lambda^y}}{\prod_{i=1}^n X_i}.\]</span>
La previa de <span class="math inline">\(\lambda\)</span> está definida por <span class="math inline">\(\pi(\lambda)\propto\lambda^{\alpha-1}e^{-\beta\lambda}\)</span>. Por lo tanto, la posterior es
<span class="math display">\[ \pi(\lambda|X) \propto \lambda^{y+\alpha-1}e^{-(\beta+n)\lambda} \implies
 \lambda|X \sim \Gamma(y+\alpha,\beta+n)\]</span></p>
<ul>
<li>En el caso normal, si <span class="math inline">\(X_1,\dots,X_n\sim N(\theta,\sigma^2)\)</span>,entonces la familia normal es conjugada si <span class="math inline">\(\sigma^2\)</span> es conocido.</li>
</ul>
<p>Si <span class="math inline">\(\theta \sim N(\mu_0,V_0^2) \implies \theta|X \sim N(\mu_1, V_1^2)\)</span> donde,
<span class="math display">\[\mu_1 = \dfrac{\sigma^2\mu_0 + nV_0^2 \bar X_n}{\sigma^2 + nV_0^2}  = \dfrac{\sigma^2}{\sigma^2 + nV_0^2}\mu_0 + \dfrac{nV_0^2}{\sigma^2 + nV_0^2}\bar X_n\]</span></p>
<p>Combina de manera ponderada la previa y la de los datos.</p>
<p><strong>Ejemplo</strong></p>
<p>Considere una verosimilitud Poisson(<span class="math inline">\(\lambda\)</span>) y una previa
<span class="math display">\[ \pi(\lambda) = \begin{cases}2e^{-2\lambda} &amp; \lambda&gt; 0 \\ 0 &amp; \lambda \geq 0\end{cases} \quad \lambda \sim \Gamma(1,2)\]</span></p>
<p>Supongamos que es una muestra aleatoria de tamaño <span class="math inline">\(n\)</span>. ¿Cuál es el número de observciones para reducir la varianza, a lo sumo, a 0.01?</p>
<p>Por teorema de Bayes, la posterior <span class="math inline">\(\lambda|x \sim \Gamma(y+1,n+2)\)</span>. Luego, la varianza de la Gamma es
<span class="math display">\[\dfrac{\alpha}{\beta^2} = \dfrac{\sum x_i + 1}{(n+2)^2} \leq 0.01 \implies \dfrac{1}{(n+2)^2} \leq \dfrac{\sum x_i + 1}{(n+2)^2} \leq 0.01 \implies 100 \leq (n+2)^2 \implies n\geq 8\]</span>
<strong>Teorema</strong>. Si <span class="math inline">\(X_1,\dots,X_n \sim N(\theta, \sigma^2)\)</span> con <span class="math inline">\(\sigma^2\)</span> conocido y la previa es <span class="math inline">\(\theta \sim N(\mu_0,V_0^2)\)</span>, entonces <span class="math inline">\(\theta|X\sim N(\mu_1,V_1^2)\)</span> donde
<span class="math display">\[ \mu_1 =  \dfrac{\sigma^2\mu_0 + nV_0^2 \bar X_n}{\sigma^2 + nV_0^2}, \quad V_1^2 = \dfrac{\sigma^2V_0^2}{\sigma^2 + nV_0^2}\]</span></p>
<p><em>Prueba</em>:</p>
<ul>
<li><strong>Verosimilitud</strong>:</li>
</ul>
<p><span class="math display">\[ f_n(X|\theta) \propto \exp\left[- \dfrac{1}{2\sigma^2} \sum_{i=1}^{n}(X_i\theta)^2\right]\]</span>
Luego,
<span class="math display">\[\begin{align*}
\sum_{i=1}^n (X_i-\theta)^2 &amp; = \sum_{i=1}^n (X_i-\bar X + \bar X - \theta)^2 \\
&amp; = n(\bar X + \theta)^2 + \sum_{i=1}^n (X_i-\bar X)^2 + \underbrace{2 \sum_{i=1}^n (X_i-\bar X)(\bar X - \theta)}_{= 0 \text{ pues } \sum Xi = n\bar X)}
\end{align*}\]</span>
Entonces
<span class="math display">\[ f_n(X|\theta) \propto \exp\left[-\dfrac{n}{2\sigma ^2}(\bar X - \theta )^2\right].\]</span></p>
<ul>
<li><strong>Previa</strong>:</li>
</ul>
<p><span class="math display">\[ \pi(\theta) \propto \exp\left[-\dfrac{1}{2V_0^2}(\theta - \mu_0)^2\right].\]</span></p>
<ul>
<li><strong>Posterior</strong>:</li>
</ul>
<p><span class="math display">\[ \pi(\theta|X) \propto \exp\left[-\dfrac{n}{2\sigma ^2}(\bar X - \theta )^2-\dfrac{1}{2V_0^2}(\theta - \mu_0)^2\right].\]</span></p>
<p>Con <span class="math inline">\(\mu_1\)</span> y <span class="math inline">\(V_1^2\)</span> definidos anteriormente, se puede comprobar la siguiente identidad:</p>
<p><span class="math display">\[-\dfrac{n}{\sigma ^2}(\bar X - \theta )^2-\dfrac{1}{V_0^2}(\theta - \mu_0)^2= \dfrac{1}{V_1^2}(\theta-\mu_1)^2 + \underbrace{\dfrac{n}{\sigma^2 + nV_0^2}(\bar X_n- \mu_0)^2}_{\text{Constante con respecto a }\theta}\]</span>
Por lo tanto, <span class="math display">\[\pi(\theta|X) \propto \exp\left[-\dfrac{n}{2V_1^2}(\theta -\mu_1)^2\right]\]</span></p>
<p><em>Media posterior</em>:</p>
<p><span class="math display">\[\mu_1 = \underbrace{\dfrac{\sigma^2}{\sigma^2 + nV_0^2}}_{W_1}\mu_0 + \underbrace{\dfrac{nV_0^2}{\sigma^2 + nV_0^2}}_{W_2}
\bar X_n \]</span></p>
<p><strong>Afirmaciones</strong>:</p>
<ol style="list-style-type: decimal">
<li><p>Si <span class="math inline">\(V_0^2\)</span> y <span class="math inline">\(\sigma^2\)</span> son fijos, entonces <span class="math inline">\(W_1 \xrightarrow[n\to \infty]{}0\)</span> (la importancia de la media empírica crece conforme aumenta <span class="math inline">\(n\)</span>).</p></li>
<li><p>Si <span class="math inline">\(V_0^2\)</span> y <span class="math inline">\(n\)</span> son fijos, entonces <span class="math inline">\(W_2 \xrightarrow[\sigma^2\to \infty]{}0\)</span> (la importancia de la media empírica decrece conforme la muestra es menos precisa).</p></li>
<li><p>Si <span class="math inline">\(\sigma^2\)</span> y <span class="math inline">\(n\)</span> son fijos, entonces <span class="math inline">\(W_2 \xrightarrow[V_0^2\to \infty]{}1\)</span> (la importancia de la media empírica crece conforma la previa es menos precisa).</p></li>
</ol>
<p><strong>Ejemplo (determinación de n)</strong></p>
<p>Sean <span class="math inline">\(X_1,\dots, X_n \sim N(\theta,1)\)</span> y <span class="math inline">\(\theta\sim N(\mu_0,4)\)</span>. Sabemos que <span class="math display">\[V_1^2 = \dfrac{\sigma^2V_0^2}{\sigma^2 + nV_0^2}. \]</span>
Buscamos que <span class="math inline">\(V_1\leq 0.01\)</span>, entonces
<span class="math display">\[ \dfrac{4}{4n+1}\leq 0.01 \implies n\geq 99.75 \text{ (al menos 100 observaciones)}\]</span></p>
</div>
<div id="densidades-previas-impropias" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Densidades previas impropias</h2>
<p><strong>Definición</strong>. Sea <span class="math inline">\(\pi\)</span> una función positiva cuyo dominio está en <span class="math inline">\(\Omega\)</span>. Suponga que <span class="math inline">\(\int\pi(\theta)\;d\theta = \infty\)</span>. Entonces decimos que <span class="math inline">\(\pi\)</span> es una <strong>densidad impropia</strong>.</p>
<p><strong>Ejemplo</strong>: <span class="math inline">\(\theta \sim \text{Unif}(\mathbb{R})\)</span>, <span class="math inline">\(\lambda \sim \text{Unif}(0,\infty)\)</span>.</p>
<p>Una técnica para seleccionar distribuciones impropia es sustituir los hiperparámetros previos por 0.</p>
<p><strong>Ejemplo</strong>:</p>
<p>Se presenta el número de soldados prusianos muertos por una patada de caballo (280 conteros, unidades de combate en 20 años).</p>
<table>
<thead>
<tr class="header">
<th>Unidades</th>
<th>Ocurrencias</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>144</td>
<td>0</td>
</tr>
<tr class="even">
<td>91</td>
<td>1</td>
</tr>
<tr class="odd">
<td>32</td>
<td>2</td>
</tr>
<tr class="even">
<td>11</td>
<td>3</td>
</tr>
<tr class="odd">
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Muestra de Poisson: <span class="math inline">\(X_1 = 0, X_2 = 1, X_3 = 1,\dots, X_{280} = 0 \sim \text{Poi}(\lambda)\)</span>.</p></li>
<li><p>Previa: <span class="math inline">\(\lambda \sim \Gamma(\alpha, \beta)\)</span>.</p></li>
<li><p>Posterior: <span class="math inline">\(\lambda|X \sim \Gamma(y+\alpha, n+\beta) = \Gamma(196 + \alpha, 280 + \beta)\)</span>.</p></li>
</ul>
<p>Sustituyendo, <span class="math inline">\(\alpha=\beta = 0\)</span>
<span class="math display">\[\begin{align*}
\pi(\lambda) &amp;= \dfrac{1}{\Gamma(\alpha)}\beta^\alpha\lambda^{\alpha-1}e^{\beta\lambda}  \\
\\ \propto \lambda^{\alpha-1}e^{-\lambda\beta} \\
&amp;=\dfrac{1}{\lambda}
\end{align*}\]</span>
donde <span class="math inline">\(\displaystyle\int_{0}^{\infty}\dfrac{1}{\lambda} d\lambda = \infty\)</span>.</p>
<p>Por teorema de Bayes, <span class="math display">\[\theta|X \sim \Gamma(196,280)\]</span></p>
</div>
<div id="funciones-de-pérdida" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Funciones de pérdida</h2>
<p><strong>Definición</strong>. Sean <span class="math inline">\(X_1,\dots, X_n\)</span> datos observables cuyo modelo está indexado por <span class="math inline">\(\theta\in\Omega\)</span>. Un estimador de <span class="math inline">\(\theta\)</span> es cualquier estadístico <span class="math inline">\(\delta(X_1,\dots, X_n)\)</span>.</p>
<p><strong>Notación</strong>:</p>
<ul>
<li>Estimador <span class="math inline">\(\to \delta(X_1,\dots,X_n)\)</span>.</li>
<li>Estimación o estimado: <span class="math inline">\(\delta(X_1,\dots,X_n)(\omega) = \delta(\overbrace{x_1,\dots,x_n}^{datos})\)</span></li>
</ul>
<p><strong>Definición</strong>. Una <strong>función de pérdida</strong> es una función de dos variables:
<span class="math display">\[ L(\theta,a), \quad \theta \in\Omega\]</span>
con <span class="math inline">\(a\)</span> un número real.</p>
<p><strong>Interpretación</strong>: es lo que pierde un analista cuando el parámetro es <span class="math inline">\(\theta\)</span> y el estimador es <span class="math inline">\(a\)</span>.</p>
<p>Asuma que <span class="math inline">\(\theta\)</span> tiene una previa. La pérdida esperada es
<span class="math display">\[ \mathbb{E}[L(\theta,a)] = \int_{\Omega}L(\theta, a) \pi(\theta)\;d\theta\]</span>
la cual es una función de <span class="math inline">\(a\)</span>, que a su vez es función de <span class="math inline">\(X_1,\dots,X_n\)</span>. Asuma que <span class="math inline">\(a\)</span> se selecciona el minimizar esta esperanza. A ese estimador <span class="math inline">\(a = \delta^*(X_1,\dots, X_n)\)</span> se le llama <strong>estimador bayesiano</strong>, si ponderamos los parámetros con respecto a la posterior.</p>
<p><span class="math display">\[\mathbb{E}[L(\theta, \delta^*)|X] = \int_{\Omega}L(\theta, a) \pi(\theta)\;d\theta = \min_a \mathbb{E}[L(\theta|a)X]. \]</span></p>
<div id="función-de-pérdida-cuadrática" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Función de pérdida cuadrática</h3>
<p><span class="math display">\[ L(\theta, a) = (\theta-a)^2\]</span></p>
<p>En el caso en que <span class="math inline">\(\theta\)</span> es real y <span class="math inline">\(\mathbb{E}[\theta|X]\)</span> es finita, entonces
<span class="math display">\[ \delta^*(X_1,\dots, X_n) = \mathbb{E}[\theta|X] \text{ cuando } L(\theta,a) = (\theta-a)^2. \]</span></p>
<p><strong>Ejemplo</strong>: <span class="math inline">\(X_1,\dots, X_n \sim \text{Ber}(\theta)\)</span>, <span class="math inline">\(\theta \sim \text{Beta}(\alpha,\beta) \implies \theta|X \sim \text{Beta}(\alpha+y,\beta+n-y)\)</span>.</p>
<p>El estimador de <span class="math inline">\(\theta\)</span> es
<span class="math display">\[ \delta^*(X_1,\dots, X_n) = \dfrac{\alpha+y}{\alpha + \beta + n} = \overbrace{\dfrac{\alpha}{\alpha + \beta} }^{\text{Esperanza previa}}\cdot \dfrac{\alpha +\beta}{\alpha +\beta + n} + \overbrace{\dfrac{y}{n}}^{\bar X}\cdot \dfrac{n}{\alpha +\beta + n}.  \]</span></p>
</div>
<div id="función-de-pérdida-absoluta" class="section level3" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Función de pérdida absoluta</h3>
<p><span class="math display">\[ L(\theta,a) = |\theta-a|\]</span></p>
<p>La pérdida esperada es
<span class="math display">\[ f(a) = \mathbb{E}[L(\theta,a)|X] = \int_{-\infty}^{+\infty}|\theta-a|\pi(\theta|X)\;d\theta = \int_{a}^{+\infty}(\theta-a)\pi(\theta|X)\;d\theta + \int_{-\infty}^{a}(a-\theta)\pi(\theta|X)\;d\theta \]</span></p>
<p>Usando el teorema fundamental del cálculo,
<span class="math display">\[F_{\pi}(a|X) = \int_{-\infty}^{\hat a}\pi(\theta|X)\;d\theta = \dfrac12 \Leftrightarrow \hat a= \operatorname*{argmin}_a f(a)\]</span></p>
<p>La <strong>mediana</strong> es el punto de <span class="math inline">\(X_{0.5}\)</span> tal que <span class="math inline">\(F(X_{0.5}) = \dfrac{1}{2}\)</span>.</p>
<p><strong>Corolario</strong>. Bajo la función de pérdida absoluta, el estimador bayesiano es la mediana posterior.</p>
<p><strong>Ejemplo</strong>: Bernoulli.
<span class="math display">\[ \dfrac{1}{\text{Beta}(\alpha+y, \beta+n-y)}\int_{-\infty}^{X_{0.5}}\theta^{\alpha+y-1} (1-\theta)^{\beta+n-y-1}\;d\theta = \dfrac12\]</span>
Resuelva para <span class="math inline">\(X_{0.5}\)</span>.</p>
</div>
<div id="otras-funciones-de-pérdida" class="section level3" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Otras funciones de pérdida</h3>
<ul>
<li><p><span class="math inline">\(L(\theta,a) = |\theta-a|^k\)</span>, <span class="math inline">\(k\ne 1,2\)</span>, <span class="math inline">\(0&lt;k&lt;1\)</span>.</p></li>
<li><p><span class="math inline">\(L(\theta,a) = \lambda(\theta)|\theta-a|^2\)</span> (<span class="math inline">\(\lambda(\theta)\)</span> penaliza la magnitud del parámetro).</p></li>
<li><p><span class="math inline">\(L(\theta,a)=\begin{cases}3(\theta-a)^2 &amp; \theta\leq a \text{ (sobreestima)}\\ (\theta-a)^2&amp;\theta\geq a \text{ (subestima)} \end{cases}\)</span></p></li>
</ul>
</div>
</div>
<div id="efecto-de-muestras-grandes" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Efecto de muestras grandes</h2>
<p><strong>Ejemplo</strong>: ítemes malos (proporción: <span class="math inline">\(\theta\)</span>), <span class="math inline">\(\theta \in [0,1]\)</span>. Función de pérdida cuadrática. El tamaño de muestra son <span class="math inline">\(n=100\)</span> ítemes, de los cuales <span class="math inline">\(y=10\)</span> están malos.</p>
<p><span class="math display">\[ X_1,\dots,X_n\sim \text{Ber}(\theta)\]</span></p>
<ul>
<li>Primer previa. <span class="math inline">\(\alpha = \beta = 1\)</span> (Beta). El estimador bayesiano corresponde a</li>
</ul>
<p><span class="math display">\[ \mathbb{E}[\theta|X] = \dfrac{\alpha+y}{\alpha+\beta+n} = \dfrac{1+10}{2+100} = 0.108\]</span></p>
<ul>
<li>Segunda previa. <span class="math inline">\(\alpha =1, \beta=2 \implies \pi(\theta) = 2e^{-2\theta}, \theta &gt;0\)</span>.</li>
</ul>
<p><span class="math display">\[ \mathbb{E}[\theta|X] = \dfrac{1+10}{1+2+100} = \dfrac{11}{103}=0.107\]</span></p>
<p>La media es <span class="math inline">\(\bar X_n = \dfrac{10}{100} = 0.1\)</span>.</p>
</div>
<div id="consistencia" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Consistencia</h2>
<p><strong>Definición</strong>. Un estimador de <span class="math inline">\(\theta\)</span> <span class="math inline">\(\delta(X_1,\dots, X_n)\)</span> es consistente si <span class="math display">\[\delta(X_1,\dots, X_n)\xrightarrow[n\to \infty]{\mathbb{P}}\theta.\]</span></p>
<p>Bajo pérdida cuadrática, <span class="math inline">\(\mathbb{E}[\theta|X] = W_1\mathbb{E}[\theta] + X_2\bar X_n = \delta^*\)</span>. Sabemos, por ley de grandes números, que <span class="math inline">\(\bar X_n \xrightarrow[n\to \infty]{\mathbb{P}}\theta\)</span>. Además, <span class="math inline">\(W_1\xrightarrow[n\to \infty]{}0\)</span> y <span class="math inline">\(W_2\xrightarrow[n\to \infty]{}1\)</span>.</p>
<p>En los ejemplos que hemos analizado
<span class="math display">\[\delta^* \xrightarrow[n\to \infty]{\mathbb{P}}\theta \]</span>
<strong>Teorema</strong>. Bajo condiciones generales, los estimadores bayesianos son consistentes.</p>
<p><strong>Estimador</strong>. Si <span class="math inline">\(X_1,\dots, X_n\)</span> es una muestra en un modelo indexado por <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta \in \Omega\)</span> (<span class="math inline">\(k\)</span>-dimensiones), sea</p>
<p><span class="math display">\[h:\Omega \to H \subset \mathbb{R}^d.\]</span>
Sea <span class="math inline">\(\psi = h(\theta)\)</span>. Un <strong>estimador</strong> de <span class="math inline">\(\psi\)</span> es un estadístico <span class="math inline">\(\delta^*(X_1,\dots, X_n) \in H\)</span>. A <span class="math inline">\(\delta^*(X_1,\dots, X_n)\)</span> estimador de <span class="math inline">\(\psi\)</span> se puede evaluar y construir estimadores nuevos.</p>
<p><strong>Ejemplo</strong>. <span class="math inline">\(X_1,\dots, X_n \sim \text{Exp}(\theta)\)</span>, <span class="math inline">\(\theta|X \sim \Gamma(\alpha,\beta) = \Gamma (4,8.6)\)</span>. La característica de interés es <span class="math inline">\(\psi = \dfrac{1}\theta\)</span>, el valor esperado del tiempo de fallo.</p>
<p>Es estimador se calcula de la siguiente manera:</p>
<p><span class="math display">\[\begin{align*}
\delta^*(x) = \mathbb{E}[\psi|x] &amp; = \int_{0}^\infty \dfrac{1}\theta\pi(\theta|x)\;d\theta\\
&amp; = \int_{0}^\infty \dfrac{1}\theta \dfrac{8.6^4}{\Gamma(4)} \theta^3e^{-8.6\theta}\;d\theta\\
&amp;=\dfrac{8.6^4}{6} \underbrace{\int_{0}^\infty \theta^2 e^{-8.6\theta}\;d\theta}_{\frac{\Gamma(3)}{8.6^3}}\\
&amp; = \dfrac{8.6^4}{6}\dfrac{2}{8.6^3} = 2.867 \text{ unidades de tiempo.}
\end{align*}\]</span></p>
<p>Por otro lado, vea que <span class="math inline">\(\mathbb{E}(\theta|X) = \dfrac{4}{8.6}\)</span>. El estimador <em>plug-in</em> correspondería a
<span class="math display">\[\dfrac{1}{\mathbb{E}(\theta|X)} = \dfrac{8.6}{4} = 2.15.\]</span></p>
</div>
<div id="laboratorio" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Laboratorio</h2>
<p>Lo primero es cargar los paquetes necesarios que usaremos en todo el curso</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb1-1"></a><span class="kw">library</span>(tidyverse)</span></code></pre></div>
<div id="distribución-previa" class="section level3" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Distribución previa</h3>
<p>En nuestro ejemplo se tenía que <span class="math inline">\(\mathbb E [\theta] = 0.0002\)</span> y <span class="math inline">\(\mathrm{Var}(\theta) = 0.001\)</span>. Suponiendo que <span class="math inline">\(\theta\)</span> es gamma se puede resolver el sistema de ecuaciones obtenemos que <span class="math inline">\(\beta=20000\)</span> y <span class="math inline">\(\alpha=4\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-1"></a>alpha_previa &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb2-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-2"></a>beta_previa &lt;-<span class="st"> </span><span class="dv">20000</span></span>
<span id="cb2-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-3"></a></span>
<span id="cb2-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-4"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e6</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb2-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-5"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> alpha_previa, <span class="dt">scale =</span> beta_previa)) <span class="op">+</span></span>
<span id="cb2-6"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-6"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb2-7"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-7"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb2-8"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb2-8"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-3-1.svg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="distribución-conjunta" class="section level3" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Distribución conjunta</h3>
<p>Asumiendo que tenemos algunos datos <span class="math inline">\(X_1, ..., X_n\)</span>, asumimos que estos son exponencial recordando que <span class="math inline">\(\mathbb E [X] = 1/\theta\)</span>, entonces una aproximación de esta densidad es</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-1"></a>x  &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2911</span>, <span class="dv">3403</span>, <span class="dv">3237</span>, <span class="dv">3509</span>, <span class="dv">3118</span>)</span>
<span id="cb3-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-2"></a></span>
<span id="cb3-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-3"></a>theta &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">mean</span>(x)</span>
<span id="cb3-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-4"></a></span>
<span id="cb3-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-5"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e5</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb3-6"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-6"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dexp, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">rate =</span> theta)) <span class="op">+</span></span>
<span id="cb3-7"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-7"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>) <span class="op">+</span></span>
<span id="cb3-8"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-8"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb3-9"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb3-9"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-4-1.svg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="distribución-posterior" class="section level3" number="3.10.3">
<h3><span class="header-section-number">3.10.3</span> Distribución posterior</h3>
<p>Según los contenidos del curso, se puede estimar los parámetros de la densidad posterior de la forma</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb4-1"></a>(y  &lt;-<span class="st"> </span><span class="kw">sum</span>(x))</span></code></pre></div>
<pre><code>## [1] 16178</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb6-1"></a>(n &lt;-<span class="st"> </span><span class="kw">length</span>(x))</span></code></pre></div>
<pre><code>## [1] 5</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb8-1"></a>(alpha_posterior &lt;-<span class="st"> </span>n <span class="op">+</span><span class="st"> </span>alpha_previa)</span></code></pre></div>
<pre><code>## [1] 9</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb10-1"></a>(beta_posterior &lt;-<span class="st"> </span>beta_previa <span class="op">+</span><span class="st"> </span>y)</span></code></pre></div>
<pre><code>## [1] 36178</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">7.5e5</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb12-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> alpha_previa, <span class="dt">scale =</span> beta_previa), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa&quot;</span>)) <span class="op">+</span></span>
<span id="cb12-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-3"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> alpha_posterior, <span class="dt">scale =</span> beta_posterior), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Posterior&quot;</span>)) <span class="op">+</span></span>
<span id="cb12-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dexp, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">rate =</span> theta), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Verosimilitud&quot;</span>)) <span class="op">+</span></span>
<span id="cb12-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-5"></a><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="fl">1.5e-5</span>) <span class="op">+</span></span>
<span id="cb12-6"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb12-6"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-6-1.svg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="agregando-nuevos-datos" class="section level3" number="3.10.4">
<h3><span class="header-section-number">3.10.4</span> Agregando nuevos datos</h3>
<p>Si tenemos un 6to dato, y queremos ver cual es su distribución posterior. Lo primero es estimar la densidad posterior de este 6to dato, pero asumiendo que la previa es la densidad que obtuvimos en el caso anterior.</p>
<p>Suponga que <span class="math inline">\(X_6 = 3000\)</span></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb13-1"></a>(alpha_previa &lt;-<span class="st"> </span>alpha_posterior)</span></code></pre></div>
<pre><code>## [1] 9</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb15-1"></a>(beta_previa &lt;-<span class="st"> </span>beta_posterior)</span></code></pre></div>
<pre><code>## [1] 36178</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb17-1"></a>(alpha_posterior &lt;-<span class="st"> </span>alpha_previa <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb19-1"></a>(beta_posterior &lt;-<span class="st"> </span>beta_previa <span class="op">+</span><span class="st"> </span><span class="dv">3000</span>)</span></code></pre></div>
<pre><code>## [1] 39178</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">1e6</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb21-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> <span class="dv">4</span>, <span class="dt">scale =</span> <span class="dv">20000</span>), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa #1&quot;</span>)) <span class="op">+</span></span>
<span id="cb21-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-3"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> alpha_previa, <span class="dt">scale =</span> beta_previa), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa #2&quot;</span>)) <span class="op">+</span></span>
<span id="cb21-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dgamma, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">shape =</span> alpha_posterior, <span class="dt">scale =</span> beta_posterior), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Posterior&quot;</span>)) <span class="op">+</span></span>
<span id="cb21-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-5"></a><span class="st">  </span><span class="kw">ylim</span>(<span class="dv">0</span>, <span class="fl">1.5e-5</span>) <span class="op">+</span></span>
<span id="cb21-6"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb21-6"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-7-1.svg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="familias-conjugadas-normales" class="section level3" number="3.10.5">
<h3><span class="header-section-number">3.10.5</span> Familias conjugadas normales</h3>
<p>Si tenemos pocos datos, la información previa es la que “prevalece”.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb22-1"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">3</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span>
<span id="cb22-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb22-2"></a></span>
<span id="cb22-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb22-3"></a>(mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x))</span></code></pre></div>
<pre><code>## [1] 9.952353</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb24-1"></a>(sigma &lt;-<span class="st"> </span><span class="kw">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 1.506354</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb26-1"></a>(n &lt;-<span class="st"> </span><span class="kw">length</span>(x))</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb28-1"></a>(mu_previa &lt;-<span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb30-1"></a>(sigma_previa &lt;-<span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb32-1"></a>(mu_posterior &lt;-<span class="st"> </span>((sigma<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu_previa <span class="op">+</span><span class="st"> </span>((n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu)</span></code></pre></div>
<pre><code>## [1] 5.66644</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb34-1"></a>(sigma2_posterior &lt;-<span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.4306432</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb36-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb36-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb36-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_previa, <span class="dt">sd =</span> sigma_previa), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa&quot;</span>)) <span class="op">+</span></span>
<span id="cb36-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb36-3"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_posterior, <span class="dt">sd =</span> <span class="kw">sqrt</span>(sigma2_posterior)), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Posterior&quot;</span>)) <span class="op">+</span></span>
<span id="cb36-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb36-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Verosimilitud&quot;</span>)) <span class="op">+</span></span>
<span id="cb36-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb36-5"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-8-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>Con más datos, la distribución se ajusta a esto y le quita importancia a la información previa.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb37-1"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span>
<span id="cb37-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb37-2"></a></span>
<span id="cb37-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb37-3"></a>(mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x))</span></code></pre></div>
<pre><code>## [1] 10.10844</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb39-1"></a>(sigma &lt;-<span class="st"> </span><span class="kw">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 1.00343</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb41-1"></a>(n &lt;-<span class="st"> </span><span class="kw">length</span>(x))</span></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb43-1"></a>(mu_previa &lt;-<span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb45-1"></a>(sigma_previa &lt;-<span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb47-1"></a>(mu_posterior &lt;-<span class="st"> </span>((sigma<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu_previa <span class="op">+</span><span class="st"> </span>((n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu)</span></code></pre></div>
<pre><code>## [1] 10.00768</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb49-1"></a>(sigma2_posterior &lt;-<span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.009968347</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb51-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb51-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb51-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_previa, <span class="dt">sd =</span> sigma_previa), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa&quot;</span>)) <span class="op">+</span></span>
<span id="cb51-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb51-3"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_posterior, <span class="dt">sd =</span> <span class="kw">sqrt</span>(sigma2_posterior)), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Posterior&quot;</span>)) <span class="op">+</span></span>
<span id="cb51-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb51-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Verosimilitud&quot;</span>)) <span class="op">+</span></span>
<span id="cb51-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb51-5"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-9-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>Si los datos por si solo son muy variable, la posterior tiende a parecerse a la
distribución previa en lugar que a la verosimilitud.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb52-1"></a>x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">mean =</span> <span class="dv">10</span>, <span class="dt">sd =</span> <span class="dv">5</span>)</span>
<span id="cb52-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb52-2"></a></span>
<span id="cb52-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb52-3"></a>(mu &lt;-<span class="st"> </span><span class="kw">mean</span>(x))</span></code></pre></div>
<pre><code>## [1] 6.366894</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb54-1"></a>(sigma &lt;-<span class="st"> </span><span class="kw">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 6.209978</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb56-1"></a>(n &lt;-<span class="st"> </span><span class="kw">length</span>(x))</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb58-1"></a>(mu_previa &lt;-<span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb60-1"></a>(sigma_previa &lt;-<span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb62-1"></a>(mu_posterior &lt;-<span class="st"> </span>((sigma<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu_previa <span class="op">+</span><span class="st"> </span>((n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span>mu)</span></code></pre></div>
<pre><code>## [1] 1.311036</code></pre>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb64-1"></a>(sigma2_posterior &lt;-<span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(sigma<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>sigma_previa<span class="op">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.7940854</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb66-1"></a><span class="kw">ggplot</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">15</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span></span>
<span id="cb66-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb66-2"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_previa, <span class="dt">sd =</span> sigma_previa), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Previa&quot;</span>)) <span class="op">+</span></span>
<span id="cb66-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb66-3"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu_posterior, <span class="dt">sd =</span> <span class="kw">sqrt</span>(sigma2_posterior)), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Posterior&quot;</span>)) <span class="op">+</span></span>
<span id="cb66-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb66-4"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma), <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;Verosimilitud&quot;</span>)) <span class="op">+</span></span>
<span id="cb66-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb66-5"></a><span class="st">  </span><span class="kw">theme_minimal</span>()</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-10-1.svg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="funciones-de-pérdida-1" class="section level3" number="3.10.6">
<h3><span class="header-section-number">3.10.6</span> Funciones de pérdida</h3>
<p>Lo más importante acá es que dependiendo de la función de pérdida podemos construir una estimador para <span class="math inline">\(\theta\)</span>. En el caso de los componentes electrónicos recordemos que la posterior nos daba</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb67-1"></a>alpha &lt;-<span class="st"> </span><span class="dv">9</span></span>
<span id="cb67-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb67-2"></a>beta &lt;-<span class="st"> </span><span class="dv">36178</span></span></code></pre></div>
<ul>
<li><strong>Pérdida cuadrática:</strong> Recoremos que la media de una gamma es <span class="math inline">\(\alpha/\beta\)</span> entonces</li>
</ul>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb68-1"></a>(theta &lt;-<span class="st"> </span>alpha <span class="op">/</span><span class="st"> </span>beta)</span></code></pre></div>
<pre><code>## [1] 0.00024877</code></pre>
<p>Y por lo tanto el tiempo promedio del componente electrónico es <span class="math inline">\(1/\theta\)</span>=4019.7777778.</p>
<ul>
<li><strong>Pérdidad absoluta:</strong> La distribución Gamma no tiene una forma cerrada para la mediana, por que se puede aproximar así,</li>
</ul>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb70-1"></a>m &lt;-<span class="st"> </span><span class="kw">rgamma</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">scale =</span> beta, <span class="dt">shape =</span> alpha)</span>
<span id="cb70-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb70-2"></a>(theta &lt;-<span class="st"> </span><span class="kw">median</span> (m))</span></code></pre></div>
<pre><code>## [1] 311778.2</code></pre>
<p>Y por lo tanto el tiempo promedio del componente electrónico es <span class="math inline">\(1/\theta\)</span>=3.2074086^{-6}.</p>
<p><strong>OJO: En este caso la pérdida cuadrática ajusta mejor ya que la distribución que la pérdida absoluta ya que la distribución NO es simétrica. En el caso simétrico los resultados serían muy similares.</strong></p>
</div>
<div id="caso-concreto" class="section level3" number="3.10.7">
<h3><span class="header-section-number">3.10.7</span> Caso concreto</h3>
<p>Suponga que se que quiere averiguar si los estudiantes de cierto colegio duermen más de 8 horas o menos de 8 horas.</p>
<p>Para esto primero cargaremos el siguiente paquete,</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb72-1"></a><span class="kw">library</span>(LearnBayes)</span></code></pre></div>
<p>Suponga que se hace una encuesta a 27 estudiantes y se encuentra que 11 dicen que duermen más de 8 horas diarias y el resto no. Nuestro objetivo es encontrar inferencias sobre la proporción <span class="math inline">\(p\)</span> de estudiantes que duermen al menos 8 horas diarias. El modelo más adecuado es</p>
<p><span class="math display">\[
f(x \vert p) \propto p^s (1-p)^f
\]</span></p>
<p>donde <span class="math inline">\(s\)</span> es la cantidad de estudiantes que duermen más de 8 horas y <span class="math inline">\(f\)</span> los que duermen menos de 8 horas.</p>
<p>Una primera aproximación para la previa es usar una distribución discreta. En este caso, el investigador asigna una probabilidad a cierta cantidad de horas de sueño, según su experiencia. Así, por ejemplo:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb73-1"></a>p &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="dt">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb73-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb73-2"></a>prior &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">5.2</span>, <span class="dv">8</span>, <span class="fl">7.2</span>, <span class="fl">4.6</span>, <span class="fl">2.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb73-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb73-3"></a>prior &lt;-<span class="st"> </span>prior <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(prior)</span>
<span id="cb73-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb73-4"></a><span class="kw">plot</span>(p, prior, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Probabilidad Previa&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-15-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>El paquete <code>LearnBayes</code> tiene la función <code>pdisc</code> que estima la distribución posterior para una previa discreta binomial. Recuerde que el valor 11 representa la cantidad de estudiantes con más de 8 horas de sueño y 16 lo que no duermen esa cantidad.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb74-1"></a>data &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">11</span>, <span class="dv">16</span>)</span>
<span id="cb74-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb74-2"></a>post &lt;-<span class="st"> </span><span class="kw">pdisc</span>(p, prior, data)</span>
<span id="cb74-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb74-3"></a><span class="kw">round</span>(<span class="kw">cbind</span>(p, prior, post), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##          p prior post
##  [1,] 0.05  0.03 0.00
##  [2,] 0.15  0.18 0.00
##  [3,] 0.25  0.28 0.13
##  [4,] 0.35  0.25 0.48
##  [5,] 0.45  0.16 0.33
##  [6,] 0.55  0.07 0.06
##  [7,] 0.65  0.02 0.00
##  [8,] 0.75  0.00 0.00
##  [9,] 0.85  0.00 0.00
## [10,] 0.95  0.00 0.00</code></pre>
<p>Y podemos ver la diferencia entre la previa (negro) y la posterior (roja),</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb76-1"></a><span class="kw">plot</span>(p, post, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb76-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb76-2"></a><span class="kw">lines</span>(p <span class="op">+</span><span class="st"> </span><span class="fl">0.01</span>, prior, <span class="dt">type =</span> <span class="st">&quot;h&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-17-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>¿Qué se puede deducir de estos resultados?</p>
<p><strong>Ejercicio:</strong> Suponga que se tiene la base de datos <code>studentdata</code>. Realice los cálculos anteriores con esos datos,</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb77-1"></a><span class="kw">data</span>(<span class="st">&quot;studentdata&quot;</span>)</span>
<span id="cb77-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb77-2"></a>horas_sueno &lt;-<span class="st"> </span>studentdata<span class="op">$</span>WakeUp <span class="op">-</span><span class="st"> </span>studentdata<span class="op">$</span>ToSleep</span>
<span id="cb77-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb77-3"></a>horas_sueno &lt;-<span class="st"> </span><span class="kw">na.omit</span>(horas_sueno)</span>
<span id="cb77-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb77-4"></a><span class="kw">summary</span>(horas_sueno)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   2.500   6.500   7.500   7.385   8.500  12.500</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb79-1"></a><span class="kw">hist</span>(horas_sueno, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-18-1.svg" width="100%" style="display: block; margin: auto;" /></p>
<p>Ahora supongamos que se tiene quiere ajustar una previa continua a este modelo. Para esto usaremos una distribución Beta con parámetros <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\beta\)</span>, de la forma</p>
<p><span class="math display">\[
pi(p\vert \alpha, \beta) \propto p^{1-\alpha} (1-p)^{1-\beta}.
\]</span></p>
<p>El ajuste de los paramétros de la Beta depende mucho de la información previa que se tenga del modelo. Una forma fácil de estimarlo es a través de cuantiles con los cuales se puede reescribir estos parámetros. Para una explicación detallada revisar <a href="https://stats.stackexchange.com/a/237849" class="uri">https://stats.stackexchange.com/a/237849</a></p>
<p>En particular, suponga que se cree que el <span class="math inline">\(50\%\)</span> de las observaciones la proporción será menor que 0.3 y que el <span class="math inline">\(90\%\)</span> será menor que 0.5.</p>
<p>Para esto ajustaremos los siguientes parámetros</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb80-1"></a>quantile2 &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">p =</span> <span class="fl">.9</span>, <span class="dt">x =</span> <span class="fl">.5</span>)</span>
<span id="cb80-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb80-2"></a>quantile1 &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">p =</span> <span class="fl">.5</span>, <span class="dt">x =</span> <span class="fl">.3</span>)</span>
<span id="cb80-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb80-3"></a>(ab &lt;-<span class="st"> </span><span class="kw">beta.select</span>(quantile1, quantile2))</span></code></pre></div>
<pre><code>## [1] 3.26 7.19</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb82-1"></a>a &lt;-<span class="st"> </span>ab[<span class="dv">1</span>]</span>
<span id="cb82-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb82-2"></a>b &lt;-<span class="st"> </span>ab[<span class="dv">2</span>]</span>
<span id="cb82-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb82-3"></a>s &lt;-<span class="st"> </span><span class="dv">11</span></span>
<span id="cb82-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb82-4"></a>f &lt;-<span class="st"> </span><span class="dv">16</span></span></code></pre></div>
<p>En este caso se obtendra la distribución posterior Beta con paramétros <span class="math inline">\(\alpha + s\)</span> y <span class="math inline">\(\beta + f\)</span>,</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-1"></a><span class="kw">curve</span>(<span class="kw">dbeta</span>(x, a <span class="op">+</span><span class="st"> </span>s, b <span class="op">+</span><span class="st"> </span>f), <span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>,</span>
<span id="cb83-2"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-2"></a>  <span class="dt">xlab =</span> <span class="st">&quot;p&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Densidad&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb83-3"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-3"></a><span class="kw">curve</span>(<span class="kw">dbeta</span>(x, s <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, f <span class="op">+</span><span class="st"> </span><span class="dv">1</span>), <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb83-4"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-4"></a><span class="kw">curve</span>(<span class="kw">dbeta</span>(x, a, b), <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">3</span>, <span class="dt">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb83-5"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-5"></a><span class="kw">legend</span>(.<span class="dv">7</span>, <span class="dv">4</span>, <span class="kw">c</span>(<span class="st">&quot;Previa&quot;</span>, <span class="st">&quot;Verosimilitud&quot;</span>, <span class="st">&quot;Posterior&quot;</span>),</span>
<span id="cb83-6"><a href="densidades-previas-conjugadas-y-estimadores-de-bayes.html#cb83-6"></a>  <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">lwd =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span></code></pre></div>
<p><img src="Notas-Curso-Estadistica_files/figure-html/02-distribuciones-previas-posteriores-20-1.svg" width="100%" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferencia-estadística.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="estimación-por-máxima-verosimilitud.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/edit/master/02-distribuciones-previas-posteriores.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/maikol-solis/notas-curso-estadistica-parte-1/blob/master/02-distribuciones-previas-posteriores.Rmd",
"text": null
},
"download": ["Notas-Curso-Estadistica.pdf"],
"toc": {
"collapse": "subsection"
},
"toc_depth": 5
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
